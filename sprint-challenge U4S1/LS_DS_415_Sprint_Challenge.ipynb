{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Sprint Challenge\n",
    "## *Data Science Unit 4 Sprint 1*\n",
    "\n",
    "After a week of Natural Language Processing, you've learned some cool new stuff: how to process text, how turn text into vectors, and how to model topics from documents. Apply your newly acquired skills to one of the most famous NLP datasets out there: [Yelp](https://www.yelp.com/dataset/challenge). As part of the job selection process, some of my friends have been asked to create analysis of this dataset, so I want to empower you to have a head start.  \n",
    "\n",
    "The real dataset is massive (almost 8 gigs uncompressed). I've sampled the data for you to something more managable for the Sprint Challenge. You can analyze the full dataset as a stretch goal or after the sprint challenge. As you work on the challenge, I suggest adding notes about your findings and things you want to analyze in the future.\n",
    "\n",
    "## Challenge Objectives\n",
    "*Successfully complete these all these objectives to earn a 2. There are more details on each objective further down in the notebook.*\n",
    "* <a href=\"#p1\">Part 1</a>: Write a function to tokenize the yelp reviews\n",
    "* <a href=\"#p2\">Part 2</a>: Create a vector representation of those tokens\n",
    "* <a href=\"#p3\">Part 3</a>: Use your tokens in a classification model on yelp rating\n",
    "* <a href=\"#p4\">Part 4</a>: Estimate & Interpret a topic model of the Yelp reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "yelp = pd.read_json('./data/review_sample.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nDuEqIyRc8YKS1q1fX0CZg</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-03-31 16:50:30</td>\n",
       "      <td>0</td>\n",
       "      <td>eZs2tpEJtXPwawvHnHZIgQ</td>\n",
       "      <td>1</td>\n",
       "      <td>BEWARE!!! FAKE, FAKE, FAKE....We also own a sm...</td>\n",
       "      <td>10</td>\n",
       "      <td>n1LM36qNg4rqGXIcvVXv8w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eMYeEapscbKNqUDCx705hg</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-12-16 05:31:03</td>\n",
       "      <td>0</td>\n",
       "      <td>DoQDWJsNbU0KL1O29l_Xug</td>\n",
       "      <td>4</td>\n",
       "      <td>Came here for lunch Togo. Service was quick. S...</td>\n",
       "      <td>0</td>\n",
       "      <td>5CgjjDAic2-FAvCtiHpytA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6Q7-wkCPc1KF75jZLOTcMw</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-06-20 19:14:48</td>\n",
       "      <td>1</td>\n",
       "      <td>DDOdGU7zh56yQHmUnL1idQ</td>\n",
       "      <td>3</td>\n",
       "      <td>I've been to Vegas dozens of times and had nev...</td>\n",
       "      <td>2</td>\n",
       "      <td>BdV-cf3LScmb8kZ7iiBcMA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>k3zrItO4l9hwfLRwHBDc9w</td>\n",
       "      <td>3</td>\n",
       "      <td>2010-07-13 00:33:45</td>\n",
       "      <td>4</td>\n",
       "      <td>LfTMUWnfGFMOfOIyJcwLVA</td>\n",
       "      <td>1</td>\n",
       "      <td>We went here on a night where they closed off ...</td>\n",
       "      <td>5</td>\n",
       "      <td>cZZnBqh4gAEy4CdNvJailQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6hpfRwGlOzbNv7k5eP9rsQ</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-06-30 02:30:01</td>\n",
       "      <td>0</td>\n",
       "      <td>zJSUdI7bJ8PNJAg4lnl_Gg</td>\n",
       "      <td>4</td>\n",
       "      <td>3.5 to 4 stars\\n\\nNot bad for the price, $12.9...</td>\n",
       "      <td>5</td>\n",
       "      <td>n9QO4ClYAS7h9fpQwa5bhA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  cool                date  funny  \\\n",
       "0  nDuEqIyRc8YKS1q1fX0CZg     1 2015-03-31 16:50:30      0   \n",
       "1  eMYeEapscbKNqUDCx705hg     0 2015-12-16 05:31:03      0   \n",
       "2  6Q7-wkCPc1KF75jZLOTcMw     1 2010-06-20 19:14:48      1   \n",
       "3  k3zrItO4l9hwfLRwHBDc9w     3 2010-07-13 00:33:45      4   \n",
       "4  6hpfRwGlOzbNv7k5eP9rsQ     1 2018-06-30 02:30:01      0   \n",
       "\n",
       "                review_id  stars  \\\n",
       "0  eZs2tpEJtXPwawvHnHZIgQ      1   \n",
       "1  DoQDWJsNbU0KL1O29l_Xug      4   \n",
       "2  DDOdGU7zh56yQHmUnL1idQ      3   \n",
       "3  LfTMUWnfGFMOfOIyJcwLVA      1   \n",
       "4  zJSUdI7bJ8PNJAg4lnl_Gg      4   \n",
       "\n",
       "                                                text  useful  \\\n",
       "0  BEWARE!!! FAKE, FAKE, FAKE....We also own a sm...      10   \n",
       "1  Came here for lunch Togo. Service was quick. S...       0   \n",
       "2  I've been to Vegas dozens of times and had nev...       2   \n",
       "3  We went here on a night where they closed off ...       5   \n",
       "4  3.5 to 4 stars\\n\\nNot bad for the price, $12.9...       5   \n",
       "\n",
       "                  user_id  \n",
       "0  n1LM36qNg4rqGXIcvVXv8w  \n",
       "1  5CgjjDAic2-FAvCtiHpytA  \n",
       "2  BdV-cf3LScmb8kZ7iiBcMA  \n",
       "3  cZZnBqh4gAEy4CdNvJailQ  \n",
       "4  n9QO4ClYAS7h9fpQwa5bhA  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Tokenize Function\n",
    "<a id=\"#p1\"></a>\n",
    "\n",
    "Complete the function `tokenize`. Your function should\n",
    "- accept one document at a time\n",
    "- return a list of tokens\n",
    "\n",
    "You are free to use any method you have learned this week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "df = yelp[['stars', 'text']]\n",
    "\n",
    "def tokenize(document):\n",
    "    words = [\"\"]\n",
    "    \n",
    "    STOP_WORDS = nlp.Defaults.stop_words.union(words)\n",
    "    \n",
    "    doc = nlp(document)\n",
    "\n",
    "    lemmas = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if (token.text.lower() not in STOP_WORDS) and ((token.is_stop == False) and (token.is_punct == False)) and (token.pos_!= 'PRON'):\n",
    "            lemmas.append(token.lemma_)\n",
    "\n",
    "    return lemmas\n",
    "\n",
    "df['tokens'] = df['text'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>BEWARE!!! FAKE, FAKE, FAKE....We also own a sm...</td>\n",
       "      <td>[beware, fake, fake, fake, small, business, Lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Came here for lunch Togo. Service was quick. S...</td>\n",
       "      <td>[come, lunch, Togo, service, quick, staff, fri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>I've been to Vegas dozens of times and had nev...</td>\n",
       "      <td>[Vegas, dozen, time, step, foot, Circus, Circu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>We went here on a night where they closed off ...</td>\n",
       "      <td>[go, night, close, street, party, good, actual...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3.5 to 4 stars\\n\\nNot bad for the price, $12.9...</td>\n",
       "      <td>[3.5, 4, star, \\n\\n, bad, price, $, 12.99, lun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stars                                               text  \\\n",
       "0      1  BEWARE!!! FAKE, FAKE, FAKE....We also own a sm...   \n",
       "1      4  Came here for lunch Togo. Service was quick. S...   \n",
       "2      3  I've been to Vegas dozens of times and had nev...   \n",
       "3      1  We went here on a night where they closed off ...   \n",
       "4      4  3.5 to 4 stars\\n\\nNot bad for the price, $12.9...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [beware, fake, fake, fake, small, business, Lo...  \n",
       "1  [come, lunch, Togo, service, quick, staff, fri...  \n",
       "2  [Vegas, dozen, time, step, foot, Circus, Circu...  \n",
       "3  [go, night, close, street, party, good, actual...  \n",
       "4  [3.5, 4, star, \\n\\n, bad, price, $, 12.99, lun...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Vector Representation\n",
    "<a id=\"#p2\"></a>\n",
    "1. Create a vector representation of the reviews\n",
    "2. Write a fake review and query for the 10 most similiar reviews, print the text of the reviews. Do you notice any patterns?\n",
    "    - Given the size of the dataset, it will probably be best to use a `NearestNeighbors` model for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_text_tokens = []\n",
    "for set_of_tokens in df['tokens'].values:\n",
    "    working_set = \"\"\n",
    "    for token in set_of_tokens:\n",
    "        working_set += token + ' '\n",
    "    review_text_tokens.append(working_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'beware fak'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_text_tokens[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>$</th>\n",
       "      <th>1</th>\n",
       "      <th>...</th>\n",
       "      <th>à</th>\n",
       "      <th>à ce</th>\n",
       "      <th>à essayer</th>\n",
       "      <th>à la</th>\n",
       "      <th>ça</th>\n",
       "      <th>équipe</th>\n",
       "      <th>était</th>\n",
       "      <th>été</th>\n",
       "      <th>être</th>\n",
       "      <th>‍</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33564 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   \\n\\n\\n\\n   \\n\\n\\n   \\n\\n   \\n\\n \\n\\n   \\n\\n \\n   \\n\\n    \\n\\n  \\n\\n   \\\n",
       "0          0        0      0           0         0       0            0   \n",
       "1          0        0      0           0         0       0            0   \n",
       "2          0        0      1           0         0       0            0   \n",
       "3          0        0      0           0         0       0            0   \n",
       "4          0        0      4           0         0       0            0   \n",
       "\n",
       "   \\n\\n     \\n\\n  $  \\n\\n  1  ...  à  à ce  à essayer  à la  ça  équipe  \\\n",
       "0        0        0        0  ...  0     0          0     0   0       0   \n",
       "1        0        0        0  ...  0     0          0     0   0       0   \n",
       "2        0        0        0  ...  0     0          0     0   0       0   \n",
       "3        0        0        0  ...  0     0          0     0   0       0   \n",
       "4        0        0        0  ...  0     0          0     0   0       0   \n",
       "\n",
       "   était  été  être  ‍  \n",
       "0      0    0     0  0  \n",
       "1      0    0     0  0  \n",
       "2      0    0     0  0  \n",
       "3      0    0     0  0  \n",
       "4      0    0     0  0  \n",
       "\n",
       "[5 rows x 33564 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Instantiate vectorizer object\n",
    "count_vect = CountVectorizer(tokenizer=tokenize, max_df=.98, min_df=3, ngram_range=(1,2))\n",
    "count_vect.fit(review_text_tokens)\n",
    "dtm_count = count_vect.transform(review_text_tokens)\n",
    "dtm_count = pd.DataFrame(dtm_count.todense(), columns=count_vect.get_feature_names())\n",
    "dtm_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>$</th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>15</th>\n",
       "      <th>2</th>\n",
       "      <th>...</th>\n",
       "      <th>week</th>\n",
       "      <th>weekend</th>\n",
       "      <th>wife</th>\n",
       "      <th>wine</th>\n",
       "      <th>wish</th>\n",
       "      <th>wonderful</th>\n",
       "      <th>work</th>\n",
       "      <th>worth</th>\n",
       "      <th>wrong</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.401464</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.145458</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.068850</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.130594</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.189266</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.108949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.362818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.172046</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.124671</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.187231</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 321 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      \\n\\n   \\n    \\n\\n                         $    1   10        15    2  \\\n",
       "0  0.000000  0.0     0.0  0.401464  0.0  0.145458  0.0  0.0  0.000000  0.0   \n",
       "1  0.000000  0.0     0.0  0.000000  0.0  0.000000  0.0  0.0  0.000000  0.0   \n",
       "2  0.068850  0.0     0.0  0.130594  0.0  0.189266  0.0  0.0  0.000000  0.0   \n",
       "3  0.000000  0.0     0.0  0.000000  0.0  0.000000  0.0  0.0  0.000000  0.0   \n",
       "4  0.362818  0.0     0.0  0.172046  0.0  0.124671  0.0  0.0  0.187231  0.0   \n",
       "\n",
       "   ...  week  weekend  wife  wine  wish  wonderful  work  worth  wrong  \\\n",
       "0  ...   0.0      0.0   0.0   0.0   0.0        0.0   0.0    0.0    0.0   \n",
       "1  ...   0.0      0.0   0.0   0.0   0.0        0.0   0.0    0.0    0.0   \n",
       "2  ...   0.0      0.0   0.0   0.0   0.0        0.0   0.0    0.0    0.0   \n",
       "3  ...   0.0      0.0   0.0   0.0   0.0        0.0   0.0    0.0    0.0   \n",
       "4  ...   0.0      0.0   0.0   0.0   0.0        0.0   0.0    0.0    0.0   \n",
       "\n",
       "       year  \n",
       "0  0.000000  \n",
       "1  0.000000  \n",
       "2  0.108949  \n",
       "3  0.000000  \n",
       "4  0.000000  \n",
       "\n",
       "[5 rows x 321 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize, min_df=0.025, max_df=.98, ngram_range=(1,2))\n",
    "dtm_tfidf = tfidf.fit_transform(review_text_tokens)\n",
    "dtm_tfidf = pd.DataFrame(dtm_tfidf.todense(), columns=tfidf.get_feature_names())\n",
    "dtm_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(algorithm='kd_tree', leaf_size=30, metric='minkowski',\n",
       "         metric_params=None, n_jobs=None, n_neighbors=5, p=2, radius=1.0)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "nn_count = NearestNeighbors(n_neighbors=5, algorithm='kd_tree')\n",
    "nn_tfidf = NearestNeighbors(n_neighbors=5, algorithm='kd_tree')\n",
    "\n",
    "nn_count.fit(dtm_count)\n",
    "nn_tfidf.fit(dtm_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_yelp_review = [\"Love this place!  The set up makes for a very romantic date spot with it's warm and cozy feel.  I enjoyed all of the vintage photos on the walls.  We will be making Royal Oak one of our regular stops.  Don't forget to check out the second floor!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_count = count_vect.transform(sample_yelp_review)\n",
    "new_tfidf = tfidf.transform(sample_yelp_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[5.38516481, 5.47722558, 5.47722558, 5.56776436, 5.65685425]]),\n",
       " array([[6311, 5129, 6875, 3693, 2515]], dtype=int64))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_count.kneighbors(new_count.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Best place everrrrr!!!'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text[5129]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.        , 1.        , 1.        , 1.11958655, 1.12431662]]),\n",
       " array([[6311, 4839, 8413, 8006,  778]], dtype=int64))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_tfidf.kneighbors(new_tfidf.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Bomb low key spot...my bf and I loved it!! It's like my momma was here making this! #posoleshawwty\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text[8006]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Classification\n",
    "<a id=\"#p3\"></a>\n",
    "Your goal in this section will be to predict `stars` from the review dataset. \n",
    "\n",
    "1. Create a piepline object with a sklearn `CountVectorizer` or `TfidfVector` and any sklearn classifier. Use that pipeline to estimate a model to predict `stars`. Use the Pipeline to predict a star rating for your fake review from Part 2. \n",
    "2. Tune the entire pipeline with a GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize(document):\n",
    "#     doc = nlp(document)\n",
    "#     return [token.lemma_.strip() for token in doc if (token.is_stop != True) and (token.is_punct != True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "clf = XGBClassifier()\n",
    "\n",
    "svd = TruncatedSVD(n_components=100, \n",
    "                   algorithm='randomized',\n",
    "                   n_iter=10)\n",
    "\n",
    "# count_vect = CountVectorizer(tokenizer=tokenize, max_df=.98, min_df=3, ngram_range=(1,2), error_score='raise')\n",
    "\n",
    "# tfidf = TfidfVectorizer(tokenizer=tokenize, min_df=0.025, max_df=.98, ngram_range=(1,2))\n",
    "\n",
    "\n",
    "\n",
    "lsi_count = Pipeline([\n",
    "                 #Vectorizer\n",
    "                 ('vect', count_vect), \n",
    "                 # Classifier\n",
    "                 #('svd', svd)\n",
    "                ])\n",
    "lsi_tfidf = Pipeline([\n",
    "                 #Vectorizer\n",
    "                 ('vect', tfidf), \n",
    "                 # Classifier\n",
    "                 ('svd', svd)\n",
    "                ])\n",
    "\n",
    "pipe_count = Pipeline([('lsi', lsi_count), ('clf', clf)])\n",
    "pipe_tfidf = Pipeline([('lsi', lsi_tfidf), ('clf', clf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "vect = TfidfVectorizer(stop_words='english', ngram_range=(1,2))\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "pipe = Pipeline([('vect', vect),\n",
    "                 ('clf', clf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 32 candidates, totalling 64 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   18.1s\n",
      "[Parallel(n_jobs=-1)]: Done  64 out of  64 | elapsed:   29.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "...obs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'vect__max_df': (0.75, 1.0), 'vect__min_df': (0.02, 0.05), 'vect__max_features': (500, 1000), 'clf__n_estimators': (5, 10), 'clf__max_depth': (15, 20)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\n",
    "    'vect__max_df': ( 0.75, 1.0),\n",
    "    'vect__min_df': (.02, .05),\n",
    "    'vect__max_features': (500,1000),\n",
    "    'clf__n_estimators':(5, 10,),\n",
    "    'clf__max_depth':(15,20)\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipe,parameters, cv=2, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(df.text, df.stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5385"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5], dtype=int64)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.predict(sample_yelp_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 32 candidates, totalling 64 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   14.4s\n",
      "[Parallel(n_jobs=-1)]: Done  64 out of  64 | elapsed:   23.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "...obs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'vect__max_df': (0.75, 1.0), 'vect__min_df': (0.02, 0.05), 'vect__max_features': (500, 1000), 'clf__n_estimators': (5, 10), 'clf__max_depth': (15, 20)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(review_text_tokens, df.stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5368"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   6 out of   6 | elapsed:   17.1s remaining:    0.0s\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_model.vectors'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py\", line 418, in _process_worker\n    r = call_item()\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py\", line 272, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\", line 567, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\", line 225, in __call__\n    for func, args, kwargs in self.items]\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\", line 225, in <listcomp>\n    for func, args, kwargs in self.items]\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 528, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\sklearn\\pipeline.py\", line 265, in fit\n    Xt, fit_params = self._fit(X, y, **fit_params)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\sklearn\\pipeline.py\", line 230, in _fit\n    **fit_params_steps[name])\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\sklearn\\externals\\joblib\\memory.py\", line 342, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\sklearn\\pipeline.py\", line 614, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\sklearn\\pipeline.py\", line 300, in fit_transform\n    return last_step.fit_transform(Xt, y, **fit_params)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1032, in fit_transform\n    self.fixed_vocabulary_)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 942, in _count_vocab\n    for feature in analyze(doc):\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 328, in <lambda>\n    tokenize(preprocess(self.decode(doc))), stop_words)\n  File \"<ipython-input-3-359c6c1676dd>\", line 14, in tokenize\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\spacy\\language.py\", line 402, in __call__\n    doc = proc(doc, **component_cfg.get(name, {}))\n  File \"pipes.pyx\", line 392, in spacy.pipeline.pipes.Tagger.__call__\n  File \"pipes.pyx\", line 411, in spacy.pipeline.pipes.Tagger.predict\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\thinc\\neural\\_classes\\model.py\", line 169, in __call__\n    return self.predict(x)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\thinc\\neural\\_classes\\feed_forward.py\", line 40, in predict\n    X = layer(X)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\thinc\\neural\\_classes\\model.py\", line 169, in __call__\n    return self.predict(x)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\thinc\\api.py\", line 310, in predict\n    X = layer(layer.ops.flatten(seqs_in, pad=pad))\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\thinc\\neural\\_classes\\model.py\", line 169, in __call__\n    return self.predict(x)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\thinc\\neural\\_classes\\feed_forward.py\", line 40, in predict\n    X = layer(X)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\thinc\\neural\\_classes\\model.py\", line 169, in __call__\n    return self.predict(x)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\thinc\\neural\\_classes\\model.py\", line 133, in predict\n    y, _ = self.begin_update(X, drop=None)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\thinc\\api.py\", line 379, in uniqued_fwd\n    Y_uniq, bp_Y_uniq = layer.begin_update(X_uniq, drop=drop)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\thinc\\neural\\_classes\\feed_forward.py\", line 46, in begin_update\n    X, inc_layer_grad = layer.begin_update(X, drop=drop)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\thinc\\api.py\", line 163, in begin_update\n    values = [fwd(X, *a, **k) for fwd in forward]\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\thinc\\api.py\", line 163, in <listcomp>\n    values = [fwd(X, *a, **k) for fwd in forward]\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\thinc\\api.py\", line 256, in wrap\n    output = func(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\thinc\\api.py\", line 163, in begin_update\n    values = [fwd(X, *a, **k) for fwd in forward]\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\thinc\\api.py\", line 163, in <listcomp>\n    values = [fwd(X, *a, **k) for fwd in forward]\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\thinc\\api.py\", line 256, in wrap\n    output = func(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\thinc\\api.py\", line 163, in begin_update\n    values = [fwd(X, *a, **k) for fwd in forward]\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\thinc\\api.py\", line 163, in <listcomp>\n    values = [fwd(X, *a, **k) for fwd in forward]\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\thinc\\api.py\", line 256, in wrap\n    output = func(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\thinc\\api.py\", line 163, in begin_update\n    values = [fwd(X, *a, **k) for fwd in forward]\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\thinc\\api.py\", line 163, in <listcomp>\n    values = [fwd(X, *a, **k) for fwd in forward]\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\thinc\\api.py\", line 256, in wrap\n    output = func(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\thinc\\neural\\_classes\\static_vectors.py\", line 60, in begin_update\n    vector_table = self.get_vectors()\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\thinc\\neural\\_classes\\static_vectors.py\", line 55, in get_vectors\n    return get_vectors(self.ops, self.lang)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\thinc\\extra\\load_nlp.py\", line 26, in get_vectors\n    nlp = get_spacy(lang)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\thinc\\extra\\load_nlp.py\", line 14, in get_spacy\n    SPACY_MODELS[lang] = spacy.load(lang, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\spacy\\__init__.py\", line 27, in load\n    return util.load_model(name, **overrides)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\spacy\\util.py\", line 139, in load_model\n    raise IOError(Errors.E050.format(name=name))\nOSError: [E050] Can't find model 'en_model.vectors'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-aca40db38df2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mgrid_search_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipe_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mgrid_search_count\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreview_text_tokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;31m#grid_search_count.best_score_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    720\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1189\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1191\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    709\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 711\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    712\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    929\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 930\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    931\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    831\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    832\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 833\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    834\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    520\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    433\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_model.vectors'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    #'lsi__svd__n_components': [250],\n",
    "    'lsi__vect__max_df': [.95, .98],\n",
    "    #'clf__max_depth':[5, 10],\n",
    "    #'clf__tree_method': ['gpu_hist'],\n",
    "    #'clf__num_leaves': (6),\n",
    "    #'clf__min_child_samples': (100),\n",
    "    #'clf__min_child_weight': [1e-5, 1]\n",
    "}\n",
    "\n",
    "grid_search_count = GridSearchCV(pipe_count,parameters, cv=3, n_jobs=4, verbose=1)\n",
    "grid_search_count.fit(review_text_tokens, df.stars)\n",
    "#grid_search_count.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_count.predict(sample_yelp_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_model.vectors'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py\", line 418, in _process_worker\n    r = call_item()\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py\", line 272, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\", line 567, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\", line 225, in __call__\n    for func, args, kwargs in self.items]\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\", line 225, in <listcomp>\n    for func, args, kwargs in self.items]\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 528, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\sklearn\\pipeline.py\", line 265, in fit\n    Xt, fit_params = self._fit(X, y, **fit_params)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\sklearn\\pipeline.py\", line 230, in _fit\n    **fit_params_steps[name])\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\sklearn\\externals\\joblib\\memory.py\", line 342, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\sklearn\\pipeline.py\", line 614, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\sklearn\\pipeline.py\", line 298, in fit_transform\n    Xt, fit_params = self._fit(X, y, **fit_params)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\sklearn\\pipeline.py\", line 230, in _fit\n    **fit_params_steps[name])\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\sklearn\\externals\\joblib\\memory.py\", line 342, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\sklearn\\pipeline.py\", line 614, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1603, in fit_transform\n    X = super(TfidfVectorizer, self).fit_transform(raw_documents)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1032, in fit_transform\n    self.fixed_vocabulary_)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 942, in _count_vocab\n    for feature in analyze(doc):\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 328, in <lambda>\n    tokenize(preprocess(self.decode(doc))), stop_words)\n  File \"<ipython-input-3-359c6c1676dd>\", line 14, in tokenize\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\spacy\\language.py\", line 402, in __call__\n    doc = proc(doc, **component_cfg.get(name, {}))\n  File \"pipes.pyx\", line 392, in spacy.pipeline.pipes.Tagger.__call__\n  File \"pipes.pyx\", line 411, in spacy.pipeline.pipes.Tagger.predict\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\thinc\\neural\\_classes\\model.py\", line 169, in __call__\n    return self.predict(x)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\thinc\\neural\\_classes\\feed_forward.py\", line 40, in predict\n    X = layer(X)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\thinc\\neural\\_classes\\model.py\", line 169, in __call__\n    return self.predict(x)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\thinc\\api.py\", line 310, in predict\n    X = layer(layer.ops.flatten(seqs_in, pad=pad))\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\thinc\\neural\\_classes\\model.py\", line 169, in __call__\n    return self.predict(x)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\thinc\\neural\\_classes\\feed_forward.py\", line 40, in predict\n    X = layer(X)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\thinc\\neural\\_classes\\model.py\", line 169, in __call__\n    return self.predict(x)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\thinc\\neural\\_classes\\model.py\", line 133, in predict\n    y, _ = self.begin_update(X, drop=None)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\thinc\\api.py\", line 379, in uniqued_fwd\n    Y_uniq, bp_Y_uniq = layer.begin_update(X_uniq, drop=drop)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\thinc\\neural\\_classes\\feed_forward.py\", line 46, in begin_update\n    X, inc_layer_grad = layer.begin_update(X, drop=drop)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\thinc\\api.py\", line 163, in begin_update\n    values = [fwd(X, *a, **k) for fwd in forward]\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\thinc\\api.py\", line 163, in <listcomp>\n    values = [fwd(X, *a, **k) for fwd in forward]\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\thinc\\api.py\", line 256, in wrap\n    output = func(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\thinc\\api.py\", line 163, in begin_update\n    values = [fwd(X, *a, **k) for fwd in forward]\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\thinc\\api.py\", line 163, in <listcomp>\n    values = [fwd(X, *a, **k) for fwd in forward]\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\thinc\\api.py\", line 256, in wrap\n    output = func(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\thinc\\api.py\", line 163, in begin_update\n    values = [fwd(X, *a, **k) for fwd in forward]\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\thinc\\api.py\", line 163, in <listcomp>\n    values = [fwd(X, *a, **k) for fwd in forward]\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\thinc\\api.py\", line 256, in wrap\n    output = func(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\thinc\\api.py\", line 163, in begin_update\n    values = [fwd(X, *a, **k) for fwd in forward]\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\thinc\\api.py\", line 163, in <listcomp>\n    values = [fwd(X, *a, **k) for fwd in forward]\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\thinc\\api.py\", line 256, in wrap\n    output = func(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\thinc\\neural\\_classes\\static_vectors.py\", line 60, in begin_update\n    vector_table = self.get_vectors()\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\thinc\\neural\\_classes\\static_vectors.py\", line 55, in get_vectors\n    return get_vectors(self.ops, self.lang)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\thinc\\extra\\load_nlp.py\", line 26, in get_vectors\n    nlp = get_spacy(lang)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\thinc\\extra\\load_nlp.py\", line 14, in get_spacy\n    SPACY_MODELS[lang] = spacy.load(lang, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\spacy\\__init__.py\", line 27, in load\n    return util.load_model(name, **overrides)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\spacy\\util.py\", line 139, in load_model\n    raise IOError(Errors.E050.format(name=name))\nOSError: [E050] Can't find model 'en_model.vectors'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-de5883dfd5f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mgrid_search_tfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipe_tfidf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mgrid_search_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mgrid_search_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    720\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1189\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1191\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    709\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 711\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    712\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    929\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 930\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    931\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    831\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    832\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 833\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    834\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    520\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    433\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_model.vectors'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    'lsi__svd__n_components': [250],\n",
    "    'lsi__vect__max_df': [.95, .98],\n",
    "    #'clf__max_depth':[5, 10],\n",
    "    #'clf__tree_method': ['gpu_hist'],\n",
    "    #'clf__num_leaves': (6),\n",
    "    #'clf__min_child_samples': (100),\n",
    "    #'clf__min_child_weight': [1e-5, 1]\n",
    "}\n",
    "\n",
    "grid_search_tfidf = GridSearchCV(pipe_tfidf,parameters, cv=3, n_jobs=8, verbose=1)\n",
    "grid_search_tfidf.fit(review_text_tokens, df.stars)\n",
    "grid_search_tfidf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_tfidf.predict(sample_yelp_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Topic Modeling\n",
    "\n",
    "Let's find out what those yelp reviews are saying! :D\n",
    "\n",
    "1. Estimate a LDA topic model of the review text\n",
    "    - Keep the `iterations` parameter at or below 5 to reduce run time\n",
    "    - The `workers` parameter should match the number of physical cores on your machine.\n",
    "2. Create 1-2 visualizations of the results\n",
    "    - You can use the most important 3 words of a topic in relevant visualizations. Refer to yesterday's notebook to extract. \n",
    "3. In markdown, write 1-2 paragraphs of analysis on the results of your topic model\n",
    "\n",
    "__*Note*__: You can pass the DataFrame column of text reviews to gensim. You do not have to use a generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import LdaMulticore\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn the vocubalary of the yelp data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(df.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a bag of words representation of the entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "corpus = [id2word.doc2bow(text) for text in df.tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your LDA model should be ready for estimation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "lda = LdaMulticore(corpus=corpus,\n",
    "                   id2word=id2word,\n",
    "                   iterations=5,\n",
    "                   workers=8,\n",
    "                   num_topics = 10 # You can change this parameter\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28661"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id2word.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word.filter_extremes(no_below=20, no_above=0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2546"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id2word.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 1-2 visualizations of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\pyLDAvis\\_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el1412422472182341843397485484\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el1412422472182341843397485484_data = {\"mdsDat\": {\"x\": [0.0037780497774304173, 0.0034795238044419145, -0.0003716681621172915, 0.0024187614749686193, 0.0020047024600932063, -0.0013178247425217997, -0.00018509003179102522, -0.001759836498322635, -0.003202174217061872, -0.004844443865119534], \"y\": [0.0007002366539157187, -0.0007645673917292807, 0.004705003303728435, -0.0011570081609501895, -0.0027627706332903055, 0.0020899459385420797, -0.0018305125817761247, 0.0015652918408880503, 0.0017892820707314987, -0.004334901040059889], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [10.733778953552246, 10.495766639709473, 10.40690803527832, 10.275527000427246, 9.915989875793457, 9.796504974365234, 9.787773132324219, 9.771530151367188, 9.532938003540039, 9.283280372619629]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"Freq\": [13764.0, 4687.0, 3756.0, 2792.0, 8150.0, 3291.0, 3053.0, 1623.0, 3634.0, 5070.0, 3449.0, 2165.0, 4123.0, 1486.0, 1540.0, 6134.0, 4126.0, 1615.0, 1370.0, 1690.0, 1411.0, 1140.0, 1630.0, 2964.0, 977.0, 1071.0, 1162.0, 1395.0, 1751.0, 1490.0, 1.3727500438690186, 0.46363767981529236, 0.4600980579853058, 1.0628489255905151, 0.4517894983291626, 0.844386100769043, 0.8072388768196106, 0.41567540168762207, 0.7692041993141174, 0.7597466111183167, 0.39500805735588074, 0.389731764793396, 0.8663314580917358, 0.7403176426887512, 0.3842688202857971, 0.732438862323761, 0.5351439714431763, 0.7272746562957764, 0.8722038865089417, 0.3743998110294342, 0.37094876170158386, 1.9760501384735107, 0.6938996315002441, 1.317787766456604, 0.6820024251937866, 0.3491383492946625, 0.6646813154220581, 0.6617181897163391, 0.34515005350112915, 0.6556252241134644, 1.0989004373550415, 9.459578514099121, 1222.378173828125, 216.54478454589844, 81.47544860839844, 493.0860290527344, 23.193527221679688, 11.245682716369629, 1769.8701171875, 31.274656295776367, 62.13573455810547, 134.68067932128906, 49.75517272949219, 154.00741577148438, 96.83421325683594, 289.6063537597656, 137.1197967529297, 512.1077270507812, 186.3345947265625, 111.70820617675781, 124.54998016357422, 215.06317138671875, 166.68505859375, 63.253074645996094, 679.1123657226562, 565.3711547851562, 87.94967651367188, 75.47442626953125, 78.11748504638672, 94.08946228027344, 516.4841918945312, 257.5346984863281, 152.87637329101562, 284.6900329589844, 181.62269592285156, 114.0969009399414, 428.32208251953125, 390.68878173828125, 376.28973388671875, 179.4518280029297, 198.16387939453125, 128.83969116210938, 169.69674682617188, 290.10552978515625, 164.96087646484375, 185.64694213867188, 184.8197479248047, 279.3226318359375, 164.990478515625, 209.9522705078125, 213.49163818359375, 160.10751342773438, 0.4715350568294525, 0.8989777565002441, 0.6473541855812073, 0.8492580652236938, 0.43437132239341736, 0.4294790029525757, 0.8094630241394043, 1.2727670669555664, 0.7749682068824768, 0.40212196111679077, 0.7543008923530579, 0.7413715720176697, 0.38528308272361755, 0.733741819858551, 0.7198799252510071, 1.216047763824463, 0.36861947178840637, 0.3652186393737793, 0.7945599555969238, 0.8251637816429138, 0.679433286190033, 0.7866934537887573, 0.6759737133979797, 1.5661436319351196, 0.34223395586013794, 0.6518424153327942, 0.7891953587532043, 0.33518919348716736, 0.6368921995162964, 0.332541823387146, 2.148634195327759, 1.0426278114318848, 1.3395066261291504, 20.061260223388672, 1847.0438232421875, 58.11732864379883, 101.75975799560547, 80.26471710205078, 393.982666015625, 66.90921020507812, 198.2080841064453, 188.60281372070312, 25.124252319335938, 628.8247680664062, 79.01502227783203, 151.1837158203125, 422.91656494140625, 72.07494354248047, 115.81629943847656, 64.98143005371094, 473.5755920410156, 175.2987823486328, 188.82284545898438, 184.84130859375, 64.26033020019531, 846.2109985351562, 343.8985900878906, 119.5870361328125, 62.147216796875, 114.12648010253906, 202.15139770507812, 128.0706787109375, 195.1541748046875, 204.66693115234375, 172.5277557373047, 219.71192932128906, 190.25900268554688, 572.1044921875, 319.8018798828125, 238.8180694580078, 237.89730834960938, 259.5863342285156, 166.70155334472656, 157.63270568847656, 338.5711975097656, 364.33782958984375, 182.59967041015625, 275.56494140625, 321.86053466796875, 279.49591064453125, 181.80313110351562, 1.3335986137390137, 0.5919082760810852, 1.9238799810409546, 0.4516589343547821, 0.4415678083896637, 0.8430507183074951, 0.8324563503265381, 1.533231496810913, 0.8244447112083435, 0.42496612668037415, 0.4173804223537445, 1.4371963739395142, 1.643557071685791, 0.7909675240516663, 0.7904114127159119, 0.39290690422058105, 0.7443169951438904, 0.3861525058746338, 1.0386408567428589, 0.7226925492286682, 1.680870771408081, 0.36947140097618103, 1.1948963403701782, 0.7039072513580322, 1.7467182874679565, 0.6899526715278625, 0.35765814781188965, 0.6845299601554871, 0.6841065883636475, 0.6445065140724182, 21.97567367553711, 4.699220657348633, 733.713134765625, 1.5380991697311401, 43.82596969604492, 38.487483978271484, 728.6705322265625, 163.9900665283203, 177.2350311279297, 199.58863830566406, 82.82672882080078, 50.5930290222168, 25.775718688964844, 72.1114273071289, 98.96632385253906, 186.84136962890625, 453.1799011230469, 136.44732666015625, 149.83047485351562, 89.4542465209961, 74.80316162109375, 463.1001892089844, 70.39674377441406, 344.6383361816406, 89.5037841796875, 124.29669952392578, 1268.148681640625, 164.79473876953125, 179.14974975585938, 137.33482360839844, 207.6483917236328, 326.664306640625, 177.27523803710938, 725.1925659179688, 412.4330139160156, 560.5712890625, 146.73495483398438, 160.18084716796875, 195.85757446289062, 149.89170837402344, 274.912841796875, 330.0849304199219, 243.3456268310547, 215.09242248535156, 296.2987365722656, 180.81402587890625, 175.4739990234375, 190.7339324951172, 254.07986450195312, 174.60728454589844, 191.5350799560547, 1.0778859853744507, 0.9874534606933594, 0.9085137248039246, 0.47524702548980713, 0.45715442299842834, 0.870144784450531, 0.8365861773490906, 0.6077114939689636, 1.1659071445465088, 0.4196069538593292, 0.7914291620254517, 0.4133588969707489, 0.786293625831604, 0.7857456803321838, 0.9178494215011597, 0.7158759832382202, 0.7164646983146667, 2.9243996143341064, 0.5046079754829407, 1.1565251350402832, 0.3573542535305023, 0.50193852186203, 0.9672453999519348, 2.5135176181793213, 1.1285545825958252, 1.5629595518112183, 0.6553760170936584, 3.191234588623047, 0.7821514010429382, 0.3331456184387207, 2.5696170330047607, 20.320823669433594, 1718.2259521484375, 112.03230285644531, 295.115966796875, 89.28093719482422, 71.06401062011719, 147.9102020263672, 711.6375122070312, 137.87359619140625, 217.6006317138672, 53.41808319091797, 103.0878677368164, 89.92206573486328, 193.88990783691406, 185.275146484375, 183.7998809814453, 30.968503952026367, 111.45238494873047, 79.53130340576172, 399.7940368652344, 112.67444610595703, 328.489501953125, 73.0668716430664, 129.62510681152344, 103.08377838134766, 162.77088928222656, 495.40826416015625, 123.71835327148438, 54.11460494995117, 728.0580444335938, 337.0253601074219, 406.8572692871094, 260.001708984375, 194.6954803466797, 304.469482421875, 165.251953125, 386.33782958984375, 305.34661865234375, 175.63058471679688, 258.291259765625, 194.27651977539062, 161.92091369628906, 292.0500183105469, 305.4920349121094, 165.16773986816406, 143.30902099609375, 146.03237915039062, 156.06431579589844, 0.6018447279930115, 1.04304838180542, 1.0153878927230835, 0.48777759075164795, 0.4872775077819824, 0.9809152483940125, 0.8084853291511536, 0.804436445236206, 0.7764486074447632, 0.7676578164100647, 0.39533624053001404, 0.743891179561615, 0.7296944260597229, 1.3892685174942017, 0.7235993146896362, 0.7239632606506348, 0.7160511612892151, 1.2069495916366577, 0.7037131786346436, 0.6967037916183472, 1.5723375082015991, 0.35975661873817444, 0.3562784492969513, 0.6640559434890747, 0.3431527316570282, 0.6534162759780884, 0.647193431854248, 0.33184704184532166, 0.324893057346344, 0.7535200715065002, 1.3123642206192017, 6.42465877532959, 28.451091766357422, 162.8805389404297, 244.3343048095703, 31.503053665161133, 469.5688781738281, 763.4829711914062, 81.95986938476562, 184.94009399414062, 1438.185546875, 183.6898956298828, 68.39558410644531, 119.6778793334961, 130.07485961914062, 450.7382507324219, 55.03739547729492, 373.8877258300781, 44.63619613647461, 201.86935424804688, 217.4235076904297, 765.671630859375, 171.53152465820312, 106.83698272705078, 162.53970336914062, 116.5318832397461, 108.09907531738281, 168.2877655029297, 154.17071533203125, 388.1666259765625, 149.20265197753906, 287.9825439453125, 157.9820098876953, 287.6859130859375, 420.4367370605469, 296.7622985839844, 210.4436798095703, 155.8917694091797, 176.5366973876953, 157.98870849609375, 326.9903869628906, 278.8099670410156, 230.53173828125, 202.05316162109375, 157.58523559570312, 0.6034353971481323, 0.888297438621521, 0.6495012640953064, 0.8204220533370972, 0.991068422794342, 1.1516822576522827, 0.7281332612037659, 7.053107261657715, 0.7081897854804993, 0.8552716374397278, 1.7465707063674927, 0.6778444051742554, 0.6745232343673706, 0.6701282858848572, 0.9528464078903198, 1.2650545835494995, 0.653538167476654, 1.6627744436264038, 0.4717063903808594, 0.33482539653778076, 0.33432307839393616, 0.4708174169063568, 0.46585556864738464, 0.4627511501312256, 0.32838544249534607, 0.6246041655540466, 0.6112115383148193, 0.6080434322357178, 4.488348484039307, 0.6061149835586548, 0.7044176459312439, 2.9509739875793457, 2.789057731628418, 1.1360293626785278, 33.47685623168945, 224.7268524169922, 32.530269622802734, 70.34244537353516, 474.0267028808594, 57.48556137084961, 3.703131914138794, 558.8939819335938, 93.30973815917969, 52.73094940185547, 478.3421325683594, 1430.6773681640625, 472.9895324707031, 117.56287384033203, 552.2967529296875, 67.57466125488281, 95.62567138671875, 69.31182861328125, 115.75383758544922, 54.85063171386719, 310.3633728027344, 119.70748138427734, 133.65420532226562, 129.17027282714844, 165.56234741210938, 354.5283203125, 171.4236297607422, 106.61387634277344, 554.2327880859375, 155.17031860351562, 130.08255004882812, 289.70025634765625, 651.2813110351562, 168.13214111328125, 109.31981658935547, 327.8995361328125, 158.1997833251953, 280.46380615234375, 144.13182067871094, 222.39866638183594, 127.18849182128906, 246.43154907226562, 156.7711639404297, 161.03770446777344, 166.9313507080078, 145.10594177246094, 157.5824737548828, 144.3293914794922, 135.32510375976562, 136.38059997558594, 135.4316864013672, 1.052492618560791, 1.0482275485992432, 0.4353339672088623, 0.5591481328010559, 0.7504770755767822, 0.5373729467391968, 0.5125380158424377, 0.6791626214981079, 0.6777504086494446, 1.6137841939926147, 0.8185858726501465, 0.346819669008255, 1.6906100511550903, 0.9243219494819641, 0.644592821598053, 0.3355160653591156, 0.33322325348854065, 1.1809872388839722, 0.6350748538970947, 0.6320503950119019, 1.0549848079681396, 0.456748366355896, 1.4765651226043701, 0.4537392258644104, 0.32065078616142273, 0.4453423321247101, 0.6004128456115723, 0.599009096622467, 0.8539037108421326, 0.4394838809967041, 0.5985234379768372, 6.92730188369751, 5.169022560119629, 254.19921875, 0.8457725644111633, 91.54157257080078, 55.39092254638672, 33.04515838623047, 26.12031364440918, 51.532562255859375, 112.18385314941406, 187.78807067871094, 245.73110961914062, 125.29058837890625, 62.08305358886719, 152.38243103027344, 133.62786865234375, 97.80937957763672, 132.5267791748047, 68.01270294189453, 861.6190795898438, 1371.4847412109375, 168.22142028808594, 105.366455078125, 25.827251434326172, 127.1969223022461, 374.9222717285156, 135.37098693847656, 132.8709259033203, 70.40702819824219, 381.5528564453125, 99.81941223144531, 76.32333374023438, 463.3056945800781, 161.75804138183594, 574.8562622070312, 168.0750274658203, 297.97772216796875, 220.78082275390625, 373.56939697265625, 246.21665954589844, 219.7894744873047, 337.7965087890625, 414.1844787597656, 286.3714599609375, 162.0828857421875, 313.3758239746094, 250.82742309570312, 153.1257781982422, 195.18362426757812, 146.00047302246094, 149.29136657714844, 144.33956909179688, 140.19000244140625, 0.6076298356056213, 0.5763028264045715, 1.0335034132003784, 1.0784785747528076, 0.8725491166114807, 1.0378895998001099, 1.0099542140960693, 0.8043658137321472, 0.41013264656066895, 0.7561694383621216, 0.39287614822387695, 0.8821849226951599, 0.7171260714530945, 1.0210288763046265, 1.1647684574127197, 0.828460693359375, 0.6781367659568787, 0.9686737060546875, 1.1291338205337524, 6.468005180358887, 1.122393012046814, 0.6614559888839722, 0.34456565976142883, 0.6553621292114258, 0.6537678837776184, 1.51542067527771, 0.645474374294281, 0.6430618762969971, 0.33331337571144104, 0.32956409454345703, 0.6301453709602356, 11.067943572998047, 1.8583046197891235, 420.64849853515625, 61.126426696777344, 34.637874603271484, 56.251991271972656, 568.8197631835938, 313.3622741699219, 73.59051513671875, 209.626708984375, 206.79237365722656, 65.57562255859375, 45.407676696777344, 11.650167465209961, 451.42095947265625, 414.17828369140625, 90.44841766357422, 124.80681610107422, 627.239013671875, 327.88330078125, 131.2642059326172, 76.78648376464844, 82.42516326904297, 347.9674987792969, 730.1474609375, 131.93348693847656, 1124.9195556640625, 359.8757629394531, 162.30003356933594, 470.2812194824219, 108.78927612304688, 227.7366485595703, 123.57537841796875, 109.9967269897461, 150.7755889892578, 183.05038452148438, 311.0387878417969, 103.16709899902344, 330.3091735839844, 170.95188903808594, 261.3445129394531, 152.0710906982422, 196.0977020263672, 153.4735565185547, 169.74447631835938, 146.31614685058594, 139.3831329345703, 148.05056762695312, 137.03282165527344, 1.2406059503555298, 2.147637367248535, 0.8865112662315369, 0.8584693670272827, 0.7929688096046448, 0.7859323024749756, 0.7337526082992554, 0.7635506987571716, 0.39581298828125, 1.4121067523956299, 0.37881502509117126, 0.7206375598907471, 1.011116862297058, 0.3649955987930298, 0.35907429456710815, 0.9790827035903931, 0.35516542196273804, 0.48697108030319214, 0.6510767340660095, 0.6413912177085876, 0.6407598257064819, 0.6366412043571472, 0.6314500570297241, 0.3255020081996918, 0.3220045566558838, 0.4531826972961426, 0.6113329529762268, 0.4480434060096741, 0.6063162684440613, 0.5928375720977783, 1.1144665479660034, 155.5113983154297, 12.240768432617188, 172.61004638671875, 513.6115112304688, 216.23033142089844, 243.51971435546875, 69.3685531616211, 132.08981323242188, 425.50250244140625, 31.315021514892578, 46.22787857055664, 7.557754993438721, 38.61830139160156, 907.0877075195312, 100.7367935180664, 131.0786590576172, 44.65265655517578, 318.6828308105469, 112.64591217041016, 142.27804565429688, 320.6255187988281, 270.95880126953125, 419.40460205078125, 78.8279800415039, 53.65476989746094, 459.02081298828125, 49.10872268676758, 166.94705200195312, 206.40452575683594, 225.38967895507812, 332.5480041503906, 219.73114013671875, 99.96788024902344, 990.6214599609375, 496.9978332519531, 155.77871704101562, 154.67588806152344, 105.38381958007812, 143.12767028808594, 326.5486755371094, 377.35162353515625, 157.8557586669922, 225.0006866455078, 243.20571899414062, 144.5379638671875, 151.02297973632812, 1.1715483665466309, 0.9329474568367004, 1.5574809312820435, 0.8352749943733215, 0.814791738986969, 0.8149129152297974, 0.9768388271331787, 0.4157724380493164, 1.2899309396743774, 0.7445603609085083, 0.7152758240699768, 0.37072673439979553, 0.34851527214050293, 0.6611693501472473, 2.2938432693481445, 0.34201884269714355, 0.7372905015945435, 0.3302183151245117, 1.4404780864715576, 0.6307830214500427, 0.3203880190849304, 0.3199811577796936, 2.630349636077881, 0.3173976242542267, 0.6076780557632446, 0.31715595722198486, 0.3164098262786865, 0.3149220049381256, 0.3135778605937958, 1.0114848613739014, 1.259575605392456, 1.7876733541488647, 1.3804800510406494, 1.0015547275543213, 459.869140625, 52.250953674316406, 27.196552276611328, 221.7921600341797, 52.578792572021484, 54.85951614379883, 276.1609191894531, 66.6904296875, 189.193603515625, 137.77513122558594, 479.0677185058594, 398.8919677734375, 170.7306365966797, 407.7888488769531, 136.5013427734375, 91.75421905517578, 169.6971435546875, 110.92866516113281, 593.9471435546875, 137.72637939453125, 166.312255859375, 399.7397155761719, 328.6184387207031, 713.0971069335938, 125.88287353515625, 158.35972595214844, 149.29205322265625, 418.16827392578125, 66.30863952636719, 323.1282653808594, 257.3341979980469, 78.71217346191406, 805.8095703125, 351.94097900390625, 154.49656677246094, 181.6177520751953, 138.76620483398438, 159.9151611328125, 180.2394256591797, 158.19837951660156, 141.91018676757812, 139.79006958007812], \"Term\": [\" \", \"food\", \"come\", \"get\", \"\\n\\n\", \"order\", \"\\n\", \"wait\", \"like\", \"place\", \"service\", \"$\", \"time\", \"say\", \"find\", \"good\", \"great\", \"restaurant\", \"recommend\", \"price\", \"experience\", \"thing\", \"know\", \"go\", \"give\", \"customer\", \"chicken\", \"definitely\", \"want\", \"eat\", \"paced\", \"avarage\", \"packer\", \"Buy\", \"difinitly\", \"Delivary\", \"walkthrough\", \"PAN\", \"BLODE\", \"booster\", \"cowabunga\", \"ROAST\", \"33\", \"Ppl\", \"Aknowledgement\", \"packin\", \"HOUSE\", \"livermush\", \"pj\", \"\\u30cd\\u30a4\\u30eb\\u306f\\u3061\\u306f\\u308b\\u3055\\u3093\\u3068\\u8a00\\u3046\\u7dba\\u9e97\\u306a\\u65b9\\u304c\\u4e01\\u5be7\\u306b\\u3057\\u3066\\u304f\\u308c\\u3068\\u3066\\u3082\\u6c17\\u306b\\u5165\\u308a\\u307e\\u3057\\u305f\", \"\\u4e88\\u5b9a\\u306b\\u306a\\u304b\\u3063\\u305f\\u307e\\u3064\\u6bdb\\u30a8\\u30af\\u30b9\\u30c6\\u3082\\u304a\\u9858\\u3044\\u3057\\u3001\\u65e5\\u672c\\u3067\\u306f\\u307e\\u3060\\u3042\\u307e\\u308a\\u306a\\u3044\\u30d6\\u30e9\\u30a6\\u30f3\\u30ab\\u30e9\\u30fc\\u306e\\u30a8\\u30af\\u30b9\\u30c6\\u3092\\u3057\\u3066\\u3082\\u3089\\u3044\\u3001\\u3068\\u3066\\u3082\\u6c17\\u306b\\u5165\\u308a\\u307e\\u3057\\u305f\", \"bi\\u00e8re\", \"butterfly\", \"swedish\", \"chrohn\", \"\\u8cea\\u554f\\u306b\\u3082\\u4e01\\u5be7\\u306b\\u7b54\\u3048\\u3066\\u304f\\u308c\\u307e\\u3057\\u305f\\u3057\\u3001\\u65e5\\u672c\\u4eba\\u306e\\u65b9\\u3082\\u65e5\\u672c\\u8a9e\\u304c\\u8a71\\u305b\\u308b\\u65b9\\u3082\\u5c45\\u3066\\u3001\\u3068\\u3066\\u3082\\u7dba\\u9e97\\u3067\\u5c45\\u5fc3\\u5730\\u306e\\u3044\\u3044\\u304a\\u5e97\\u3067\\u3057\\u305f\", \"Bello\", \"Jina\", \"22.50\", \"ridgeville\", \"LMT\", \"Cleveland\", \"\\n\\n\", \"need\", \"actually\", \"order\", \"team\", \"boba\", \" \", \"number\", \"cool\", \"sure\", \"15\", \"customer\", \"awesome\", \"$\", \"year\", \"time\", \"drink\", \"call\", \"review\", \"wait\", \"menu\", \"house\", \"good\", \"place\", \"4\", \"selection\", \"owner\", \"serve\", \"food\", \"love\", \"bad\", \"try\", \"work\", \"clean\", \"great\", \"come\", \"like\", \"say\", \"nice\", \"bar\", \"take\", \"\\n\", \"experience\", \"price\", \"want\", \"service\", \"tell\", \"get\", \"go\", \"look\", \"Hana\", \"Aryon\", \"Melinda\", \"intelligently\", \"Chen\", \"fullset\", \"Shelepak\", \"discontinue\", \"Coe\", \"weeknite\", \"Orlando\", \"romper\", \"PNW\", \"Avalon\", \"bathing\", \"fungus\", \"sour-\", \"Outbacks\", \"halo\", \"stationnement\", \"Talia\", \"swimsuit\", \"Tammie\", \"greatness\", \"Chaufa\", \"shimp\", \"megaphone\", \"braider\", \"arrowhead\", \"Liaw\", \"gumbo\", \"outsource\", \"impersonal\", \"polite\", \" \", \"professional\", \"stop\", \"kind\", \"get\", \"taco\", \"recommend\", \"people\", \"cookie\", \"place\", \"old\", \"way\", \"service\", \"use\", \"clean\", \"kid\", \"time\", \"drink\", \"say\", \"ask\", \"breakfast\", \"\\n\\n\", \"go\", \"check\", \"quick\", \"visit\", \"price\", \"bar\", \"know\", \"nice\", \"think\", \"look\", \"restaurant\", \"good\", \"\\n\", \"love\", \"$\", \"try\", \"definitely\", \"amazing\", \"like\", \"great\", \"wait\", \"order\", \"food\", \"come\", \"want\", \"beause\", \"Delta\", \"martial\", \"Seven\", \"Donny\", \"oho\", \"imcomplete\", \"tastey\", \"jackpot\", \"Karl\", \"Eron\", \"Moving\", \"acrobatic\", \"service.maybe\", \"Darlene\", \"biriyani\", \"place----a\", \"cigaret\", \"mouse\", \"liberte\", \"wheelchair\", \"humbled\", \"squishie\", \"shipper\", \"ghetto\", \"suitemate\", \"Croquetas\", \"couture\", \"CWRU\", \"sill\", \"trust\", \"clinic\", \"food\", \"Stack\", \"plus\", \"yummy\", \"place\", \"lot\", \"chicken\", \"recommend\", \"perfect\", \"piece\", \"compare\", \"ok\", \"salad\", \"definitely\", \"come\", \"night\", \"feel\", \"super\", \"roll\", \"great\", \"1\", \"go\", \"excellent\", \"hour\", \" \", \"amazing\", \"take\", \"right\", \"want\", \"\\n\", \"work\", \"\\n\\n\", \"time\", \"good\", \"delicious\", \"people\", \"nice\", \"menu\", \"get\", \"like\", \"try\", \"love\", \"service\", \"price\", \"know\", \"look\", \"order\", \"wait\", \"$\", \"Greater\", \"golfweek.com\", \"Talia\", \"guacamolie\", \"Cellos\", \"bathing\", \"romper\", \"SUPER\", \"Ranchero\", \"bariatric\", \"accounting\", \"10.80\", \"momma\", \"palma\", \"Quiznos\", \"Flavyour\", \"Brookline\", \"Amanda\", \"Pizio\", \"Caliente\", \"solash\", \"john\", \"Aaron\", \"Hikari\", \"cartridge\", \"Keri\", \"Seed\", \"resume\", \"enclosure\", \"dor\", \"Kelly\", \"tire\", \" \", \"home\", \"love\", \"make\", \"buy\", \"room\", \"good\", \"minute\", \"price\", \"company\", \"long\", \"quality\", \"find\", \"tell\", \"staff\", \"better\", \"clean\", \"close\", \"like\", \"enjoy\", \"go\", \"shop\", \"pretty\", \"area\", \"little\", \"place\", \"bar\", \"hair\", \"\\n\\n\", \"order\", \"great\", \"try\", \"nice\", \"\\n\", \"day\", \"food\", \"service\", \"know\", \"get\", \"look\", \"say\", \"come\", \"time\", \"want\", \"people\", \"ask\", \"$\", \"luxuriously\", \"Hq\", \"Brookstone\", \"YC\", \"Neil\", \"Buy\", \"mmmmmm\", \"posoleshawwty\", \"operative\", \"Delivary\", \"Reyna\", \"fridendly\", \"Sono\", \"woot\", \"Yeahhhhhhh\", \"Greulich\", \"dolci\", \"Yasmin\", \"8/12/12\", \"momma\", \"Dental\", \"gifting\", \"vanya\", \"Erika\", \"PNW\", \"UMAMI\", \"12:45pm\", \"Vanya\", \"LGBTQ\", \"substandard\", \"nan\", \"auto\", \"massage\", \"customer\", \"price\", \"head\", \"like\", \"good\", \"thank\", \"definitely\", \" \", \"ask\", \"huge\", \"star\", \"fry\", \"time\", \"fantastic\", \"service\", \"show\", \"nice\", \"look\", \"\\n\\n\", \"work\", \"enjoy\", \"experience\", \"year\", \"review\", \"eat\", \"friendly\", \"great\", \"need\", \"go\", \"take\", \"\\n\", \"place\", \"order\", \"$\", \"day\", \"want\", \"find\", \"food\", \"come\", \"get\", \"try\", \"love\", \"shoooz\", \"insectarium\", \"stab\", \"Nervous\", \"botanical\", \"Greenway\", \"Colleen\", \"yuk\", \"AKRON\", \"Chihuly\", \"ganoush\", \"integration\", \"Aptive\", \"restauran\", \"Astor\", \"Serendipity\", \"cantina\", \"tar\", \"keychain\", \"Hip\", \"work/\", \"yuca\", \"Willies\", \"HomeTeam\", \"sauga\", \"Juvaderm\", \"placard\", \"YOUNGSTOWN\", \"marinate\", \"rivalisent\", \"jellyfish\", \"Guys\", \"unorganized\", \"handedly\", \"lack\", \"restaurant\", \"date\", \"tasty\", \"come\", \"today\", \"Cox\", \"food\", \"dinner\", \"decent\", \"time\", \" \", \"great\", \"check\", \"place\", \"cook\", \"start\", \"roll\", \"location\", \"point\", \"get\", \"year\", \"table\", \"new\", \"think\", \"service\", \"eat\", \"meal\", \"good\", \"little\", \"taste\", \"go\", \"\\n\\n\", \"find\", \"well\", \"like\", \"day\", \"\\n\", \"amazing\", \"try\", \"thing\", \"order\", \"wait\", \"want\", \"look\", \"ask\", \"love\", \"price\", \"work\", \"$\", \"nice\", \"axle\", \"Matthew\", \"nipple\", \"pok\\u00e9\", \"dolci\", \"Chad\", \"bod\", \"savouriness\", \"Funds\", \"Singapore\", \"Harlow\", \"gooood\", \"Yo\", \"rural\", \"Daaayyuuummm\", \"kindof\", \"whatdya\", \"juste\", \"2ea\", \"instinctively\", \"25th\", \"Navy\", \"bingsu\", \"Conrad\", \"Pruitt\", \"swordfish\", \"CPAP\", \"respectueux\", \"accurately\", \"scandalous\", \"BB\", \"creme\", \"farm\", \"know\", \"Aaron\", \"coffee\", \"choose\", \"Saturday\", \"guest\", \"local\", \"store\", \"experience\", \"look\", \"sure\", \"change\", \"chicken\", \"2\", \"flavor\", \"fry\", \"dessert\", \"\\n\\n\", \" \", \"little\", \"Vegas\", \"save\", \"pretty\", \"service\", \"feel\", \"room\", \"perfect\", \"like\", \"area\", \"line\", \"food\", \"definitely\", \"good\", \"eat\", \"\\n\", \"love\", \"great\", \"try\", \"$\", \"come\", \"place\", \"order\", \"staff\", \"time\", \"go\", \"find\", \"get\", \"think\", \"restaurant\", \"take\", \"tell\", \"\\u5929\\u6c23\\u5f88\\u71b1\\u5403\\u4e0d\\u4e0b\\u6771\\u897f\\uff0c\\u4eca\\u5929\\u6211\\u9ede\\u4e86\\u4e00\\u500b\\u97d3\\u570b\\u51b7\\u9762\\u6e6f\\u3001\\u9910\\u5f8c\\u9ede\\u4e86\\u751c\\u9ede\\uff0c\\u51b0\\u6c99\\u7cfb\\u5217\\u4e0d\\u6703\\u592a\\u751c\\u81a9\\uff0c\\u89ba\\u5f97\\u5e97\\u5bb6\\u5f88\\u7528\\u5fc3\\u88fd\\u4f5c\\uff0c\\u5305\\u542b\\u64fa\\u76e4\\u7cbe\\u7dfb\\u3001\\u4f50\\u6599\\u885b\\u751f\\uff0c\\u590f\\u65e5\\u60f3\\u958b\\u80c3\\uff0c\\u9019\\u662f\\u4e00\\u500b\\u4e0d\\u932f\\u7684\\u9078\\u64c7\\uff0c\\u670d\\u52d9\\u4eba\\u54e1\\u4e5f\\u5f88\\u656c\\u696d\\uff0c\\u4ee5\\u5f8c\\u6703\\u5e38\\u5e38\\u4f86\", \"everrrrr\", \"Environmental\", \"Devin\", \"Aptive\", \"demonic\", \"cranny\", \"TSO\", \"unite\", \"CHOP\", \"BSC\", \"Sheldon\", \"Estate\", \"pesky\", \"Katelyn\", \"slimey\", \"calabash\", \"nook\", \"rename\", \"yuk\", \"fungus\", \"Wachtel\", \"blowingly\", \"Darlene\", \"imcomplete\", \"cramp\", \"Wacs\", \"fizz\", \"everhthe\", \"Lankan\", \"Laguna\", \"session\", \"Daniel\", \"get\", \"hair\", \"salsa\", \"problem\", \"food\", \"try\", \"family\", \"restaurant\", \"wait\", \"deal\", \"chip\", \"stylist\", \"time\", \"come\", \"end\", \"night\", \"good\", \"go\", \"lot\", \" \\n\\n\", \"meat\", \"order\", \"\\n\\n\", \"way\", \" \", \"like\", \"experience\", \"place\", \"sauce\", \"$\", \"friend\", \"well\", \"friendly\", \"nice\", \"service\", \"walk\", \"great\", \"price\", \"\\n\", \"work\", \"love\", \"eat\", \"want\", \"staff\", \"day\", \"look\", \"tell\", \"Purolator\", \"obsessed\", \"Roomtastic\", \".worth\", \"ANgelica\", \"12:45pm\", \"Hunan\", \"shimp\", \"Smoker\", \"Bomb\", \"shoots.but\", \"8/12/12\", \"Wor\", \"Austin's\", \"stinkin\", \"pom\", \"unite\", \"Bregman\", \"BB\", \"Lincoln\", \"Lomax\", \"TSO\", \"coffe\", \"abiance\", \"really??all\", \"gardein\", \"couture\", \"fastest\", \"WORST\", \"Char\", \"mistreat\", \"give\", \"absolute\", \"thing\", \"come\", \"find\", \"want\", \"sushi\", \"star\", \"order\", \"wing\", \"reason\", \"screw\", \"lady\", \"\\n\\n\", \"sit\", \"minute\", \"hear\", \"get\", \"review\", \"bad\", \"go\", \"try\", \"great\", \"care\", \"talk\", \"food\", \"later\", \"say\", \"look\", \"love\", \"service\", \"$\", \"dish\", \" \", \"good\", \"tell\", \"staff\", \"fresh\", \"drink\", \"time\", \"place\", \"restaurant\", \"\\n\", \"like\", \"ask\", \"nice\", \"Cary\", \"Krung\", \"menudo\", \"palma\", \"Jina\", \"Bourgogne\", \"Matthew\", \"Kiki\", \"Quiche\", \"Brookline\", \"Kitsch\", \"fuckin\", \"aweful\", \"luddite\", \"Ian\", \"\\u53d7\\u4ed8\\u304c\\u7d42\\u308f\\u3063\\u305f\\u3089\\u3001\\u60c5\\u5831\\u304c\\u4fdd\\u5b58\\u3055\\u308c\\u305f\\u8155\\u6642\\u8a08\\u578b\\u306e\\u30c7\\u30d0\\u30a4\\u30b9\\u3092\\u3064\\u3051\\u3066\\u30b3\\u30fc\\u30b9\\u3078\\u51fa\\u307e\\u3059\", \"Luci\", \"satifie\", \"Siam\", \"Skinny\", \"sevice\", \"\\u6b21\\u306b\\u30e9\\u30b9\\u30d9\\u30ac\\u30b9\\u3092\\u8a2a\\u308c\\u305f\\u3068\\u304d\\u3082\\u307e\\u305f\\u884c\\u304d\\u305f\\u3044\\u3002\\u305d\\u306e\\u6642\\u306f\\u308f\\u305f\\u3057\\u3082\\u4f55\\u304b\\u3001\\u4e57\\u3063\\u3066\\u307f\\u305f\\u3044\\u306a\", \"poisoning\", \"huge-\", \"McKnight\", \"\\u3061\\u306a\\u307f\\u306b\\u3001\\u30ab\\u30fc\\u30c9\\u3067\\u652f\\u6255\\u3044\\u3092\\u3057\\u307e\\u3059\\u304c\\u3001\\u7d19\\u3067\\u306e\\u30ec\\u30b7\\u30fc\\u30c8\\u306f\\u3082\\u3089\\u3048\\u305a\\u3001\\u30e1\\u30fc\\u30eb\\u30a2\\u30c9\\u30ec\\u30b9\\u306b\\u9818\\u53ce\\u8a3c\\u304c\\u9001\\u3089\\u308c\\u3066\\u304f\\u308b\\u30b7\\u30b9\\u30c6\\u30e0\\u3067\\u3059\\u3002\\u8a00\\u3048\\u3070\\u30ec\\u30b7\\u30fc\\u30c8\\u3082\\u3082\\u3089\\u3048\\u308b\\u304b\\u3082\\u3057\\u308c\\u307e\\u305b\\u3093\", \"\\u53d7\\u4ed8\\u3067\\u306fipad\\u3092\\u5229\\u7528\\u3057\\u3066\\u3001\\u8996\\u899a\\u7684\\u306b\\u5206\\u304b\\u308a\\u3084\\u3059\\u304f\\u9032\\u3081\\u308b\\u3053\\u3068\\u304c\\u3067\\u304d\\u307e\\u3059\\u3002\\u308f\\u305f\\u3057\\u9054\\u306f\\u4e88\\u7d04\\u3057\\u306a\\u3044\\u3067\\u884c\\u304d\\u307e\\u3057\\u305f\\u304c\\u3001\\u554f\\u984c\\u3042\\u308a\\u307e\\u305b\\u3093\\u3067\\u3057\\u305f\", \"lyonnais\", \"fetta\", \"basa\", \"cocky\", \"Rockets\", \"700\", \"squishie\", \"\\n\", \"potato\", \"recently\", \"wait\", \"hand\", \"especially\", \"$\", \"ok\", \"say\", \"new\", \"great\", \"service\", \"recommend\", \"like\", \"taste\", \"big\", \"think\", \"fresh\", \"good\", \"delicious\", \"eat\", \"time\", \"order\", \"\\n\\n\", \"way\", \"day\", \"experience\", \"place\", \"sandwich\", \"come\", \"go\", \"serve\", \" \", \"food\", \"restaurant\", \"love\", \"work\", \"look\", \"get\", \"try\", \"want\", \"know\"], \"Total\": [13764.0, 4687.0, 3756.0, 2792.0, 8150.0, 3291.0, 3053.0, 1623.0, 3634.0, 5070.0, 3449.0, 2165.0, 4123.0, 1486.0, 1540.0, 6134.0, 4126.0, 1615.0, 1370.0, 1690.0, 1411.0, 1140.0, 1630.0, 2964.0, 977.0, 1071.0, 1162.0, 1395.0, 1751.0, 1490.0, 4.404809474945068, 1.6165114641189575, 1.6149544715881348, 3.7457737922668457, 1.6158087253570557, 3.086543560028076, 3.0934371948242188, 1.6158984899520874, 3.088965892791748, 3.0842716693878174, 1.6148884296417236, 1.6159310340881348, 3.5955135822296143, 3.088798761367798, 1.6135916709899902, 3.0916013717651367, 2.275712728500366, 3.0928213596343994, 3.7496588230133057, 1.616079568862915, 1.6158803701400757, 8.68425178527832, 3.093050241470337, 5.883515357971191, 3.0906906127929688, 1.6154447793960571, 3.091737747192383, 3.0942537784576416, 1.6160519123077393, 3.0884103775024414, 5.226722240447998, 49.533790588378906, 8150.74462890625, 1325.5040283203125, 481.2605895996094, 3291.409912109375, 130.9619903564453, 60.67304611206055, 13764.986328125, 184.1675262451172, 388.207763671875, 919.4473876953125, 313.2349548339844, 1071.044189453125, 649.585693359375, 2165.345458984375, 958.9429931640625, 4123.7734375, 1362.2542724609375, 775.7425537109375, 882.83935546875, 1623.7701416015625, 1241.051513671875, 420.17431640625, 6134.181640625, 5070.994140625, 613.1873779296875, 515.5717163085938, 536.573974609375, 669.8532104492188, 4687.36767578125, 2145.61474609375, 1175.7745361328125, 2460.81201171875, 1465.6087646484375, 846.7083129882812, 4126.12451171875, 3756.9658203125, 3634.631591796875, 1486.4852294921875, 1733.4093017578125, 998.7584838867188, 1435.6644287109375, 3053.383056640625, 1411.86279296875, 1690.9493408203125, 1751.85546875, 3449.70166015625, 1466.85595703125, 2792.78857421875, 2964.871337890625, 1909.2861328125, 1.610701560974121, 3.0807995796203613, 2.2697010040283203, 3.0819578170776367, 1.6116960048675537, 1.6110001802444458, 3.0875067710876465, 5.060985088348389, 3.085447072982788, 1.6119418144226074, 3.0836117267608643, 3.0851242542266846, 1.6106961965560913, 3.0846786499023438, 3.085852861404419, 5.214617729187012, 1.613155722618103, 1.6126121282577515, 3.585149049758911, 3.7458858489990234, 3.0863001346588135, 3.587246894836426, 3.085266590118408, 7.194640636444092, 1.6129441261291504, 3.0880720615386963, 3.7455592155456543, 1.6129721403121948, 3.086803913116455, 1.6130839586257935, 10.493597030639648, 5.063050746917725, 6.53761625289917, 105.68707275390625, 13764.986328125, 349.02880859375, 637.5391845703125, 500.57208251953125, 2792.78857421875, 417.1845703125, 1370.9249267578125, 1324.8326416015625, 151.08592224121094, 5070.994140625, 536.6790771484375, 1104.5926513671875, 3449.70166015625, 495.5827331542969, 846.7083129882812, 449.9128723144531, 4123.7734375, 1362.2542724609375, 1486.4852294921875, 1452.8408203125, 446.475341796875, 8150.74462890625, 2964.871337890625, 902.6659545898438, 430.8781433105469, 870.0114135742188, 1690.9493408203125, 998.7584838867188, 1630.34814453125, 1733.4093017578125, 1421.3258056640625, 1909.2861328125, 1615.6749267578125, 6134.181640625, 3053.383056640625, 2145.61474609375, 2165.345458984375, 2460.81201171875, 1395.6536865234375, 1301.8818359375, 3634.631591796875, 4126.12451171875, 1623.7701416015625, 3291.409912109375, 4687.36767578125, 3756.9658203125, 1751.85546875, 3.0949745178222656, 1.6172810792922974, 6.553252220153809, 1.6161648035049438, 1.6160427331924438, 3.09261417388916, 3.092898368835449, 5.736182689666748, 3.0951409339904785, 1.6167137622833252, 1.616266131401062, 5.579246997833252, 6.394504070281982, 3.0903408527374268, 3.0894782543182373, 1.616194725036621, 3.0916547775268555, 1.6151551008224487, 4.414710998535156, 3.0918993949890137, 7.211664199829102, 1.615678071975708, 5.235057830810547, 3.089451313018799, 7.7178425788879395, 3.0946521759033203, 1.6151875257492065, 3.0927999019622803, 3.0938963890075684, 2.934906482696533, 110.87043762207031, 22.90782356262207, 4687.36767578125, 7.2116804122924805, 248.9228515625, 220.84971618652344, 5070.994140625, 1063.547119140625, 1162.785888671875, 1370.9249267578125, 530.7720947265625, 313.215087890625, 149.87237548828125, 474.45111083984375, 685.7660522460938, 1395.6536865234375, 3756.9658203125, 995.86669921875, 1111.6685791015625, 628.2503662109375, 516.05224609375, 4126.12451171875, 485.35107421875, 2964.871337890625, 642.5888061523438, 938.2413330078125, 13764.986328125, 1301.8818359375, 1435.6644287109375, 1056.2178955078125, 1751.85546875, 3053.383056640625, 1465.6087646484375, 8150.74462890625, 4123.7734375, 6134.181640625, 1177.227783203125, 1324.8326416015625, 1733.4093017578125, 1241.051513671875, 2792.78857421875, 3634.631591796875, 2460.81201171875, 2145.61474609375, 3449.70166015625, 1690.9493408203125, 1630.34814453125, 1909.2861328125, 3291.409912109375, 1623.7701416015625, 2165.345458984375, 3.0918781757354736, 3.0933096408843994, 3.0863001346588135, 1.6173131465911865, 1.614736557006836, 3.085852861404419, 3.0851242542266846, 2.273071527481079, 4.4086785316467285, 1.6146109104156494, 3.0913684368133545, 1.6147832870483398, 3.085736036300659, 3.09877610206604, 3.747894048690796, 3.0930228233337402, 3.0965871810913086, 12.646551132202148, 2.275869131088257, 5.230226993560791, 1.6167757511138916, 2.2741894721984863, 4.410317420959473, 11.628028869628906, 5.225942611694336, 7.369044780731201, 3.0924735069274902, 15.080495834350586, 3.7471656799316406, 1.6168028116226196, 12.645485877990723, 118.76410675048828, 13764.986328125, 739.6048583984375, 2145.61474609375, 595.4631958007812, 472.36651611328125, 1088.5057373046875, 6134.181640625, 1014.7357788085938, 1690.9493408203125, 359.2901611328125, 748.6464233398438, 647.6224975585938, 1540.984130859375, 1466.85595703125, 1481.87255859375, 197.29763793945312, 846.7083129882812, 577.1622924804688, 3634.631591796875, 865.6640625, 2964.871337890625, 526.9607543945312, 1023.2848510742188, 794.946533203125, 1360.5289306640625, 5070.994140625, 998.7584838867188, 377.6426696777344, 8150.74462890625, 3291.409912109375, 4126.12451171875, 2460.81201171875, 1733.4093017578125, 3053.383056640625, 1458.5628662109375, 4687.36767578125, 3449.70166015625, 1630.34814453125, 2792.78857421875, 1909.2861328125, 1486.4852294921875, 3756.9658203125, 4123.7734375, 1751.85546875, 1324.8326416015625, 1452.8408203125, 2165.345458984375, 1.6118171215057373, 3.0865657329559326, 3.0878524780273438, 1.6104960441589355, 1.6133313179016113, 3.7457737922668457, 3.087853193283081, 3.084897994995117, 3.0896527767181396, 3.086543560028076, 1.6134227514266968, 3.09047269821167, 3.090212821960449, 5.88635778427124, 3.0860910415649414, 3.088224411010742, 3.088676929473877, 5.224618911743164, 3.090949058532715, 3.085736036300659, 7.046815872192383, 1.6124497652053833, 1.6124330759048462, 3.0911290645599365, 1.6106961965560913, 3.0880720615386963, 3.0917861461639404, 1.6124999523162842, 1.613939642906189, 3.748330593109131, 6.54130744934082, 34.36403274536133, 164.01783752441406, 1071.044189453125, 1690.9493408203125, 196.4533233642578, 3634.631591796875, 6134.181640625, 574.9240112304688, 1395.6536865234375, 13764.986328125, 1452.8408203125, 485.784912109375, 920.1760864257812, 1016.1685791015625, 4123.7734375, 389.8118896484375, 3449.70166015625, 310.4538879394531, 1733.4093017578125, 1909.2861328125, 8150.74462890625, 1465.6087646484375, 865.6640625, 1411.86279296875, 958.9429931640625, 882.83935546875, 1490.7615966796875, 1347.160400390625, 4126.12451171875, 1325.5040283203125, 2964.871337890625, 1435.6644287109375, 3053.383056640625, 5070.994140625, 3291.409912109375, 2165.345458984375, 1458.5628662109375, 1751.85546875, 1540.984130859375, 4687.36767578125, 3756.9658203125, 2792.78857421875, 2460.81201171875, 2145.61474609375, 1.617256760597229, 3.0907063484191895, 2.276902198791504, 3.0905654430389404, 3.751304864883423, 4.411370754241943, 3.092277765274048, 30.000288009643555, 3.089345693588257, 3.7532966136932373, 7.718568325042725, 3.093254566192627, 3.089517593383789, 3.091341733932495, 4.4145307540893555, 5.887024402618408, 3.09362530708313, 7.877079010009766, 2.27522349357605, 1.6158161163330078, 1.6152592897415161, 2.275390625, 2.2774572372436523, 2.27518892288208, 1.6162723302841187, 3.0907278060913086, 3.0903213024139404, 3.090029716491699, 22.8580379486084, 3.091157913208008, 3.59570574760437, 15.442607879638672, 14.790510177612305, 5.883810997009277, 204.08665466308594, 1615.6749267578125, 209.64500427246094, 476.69818115234375, 3756.9658203125, 394.6627502441406, 20.85403823852539, 4687.36767578125, 683.5717163085938, 368.1016845703125, 4123.7734375, 13764.986328125, 4126.12451171875, 902.6659545898438, 5070.994140625, 500.1519775390625, 736.8047485351562, 516.05224609375, 921.8497314453125, 398.36370849609375, 2792.78857421875, 958.9429931640625, 1101.126220703125, 1060.81396484375, 1421.3258056640625, 3449.70166015625, 1490.7615966796875, 866.8113403320312, 6134.181640625, 1360.5289306640625, 1104.6767578125, 2964.871337890625, 8150.74462890625, 1540.984130859375, 911.8848266601562, 3634.631591796875, 1458.5628662109375, 3053.383056640625, 1301.8818359375, 2460.81201171875, 1140.6455078125, 3291.409912109375, 1623.7701416015625, 1751.85546875, 1909.2861328125, 1452.8408203125, 2145.61474609375, 1690.9493408203125, 1465.6087646484375, 2165.345458984375, 1733.4093017578125, 3.746690034866333, 3.758239269256592, 1.6153051853179932, 2.2745306491851807, 3.088676929473877, 2.274338722229004, 2.274334192276001, 3.090625762939453, 3.088876962661743, 7.36907958984375, 3.7452213764190674, 1.6143543720245361, 8.01618480682373, 4.408537864685059, 3.0870518684387207, 1.6147007942199707, 1.6151677370071411, 5.725662708282471, 3.0902302265167236, 3.0892271995544434, 5.224828243255615, 2.273869752883911, 7.362142562866211, 2.2765607833862305, 1.6151562929153442, 2.274374008178711, 3.0886073112487793, 3.0894808769226074, 4.407355785369873, 2.2734131813049316, 3.096543550491333, 37.98390197753906, 28.437408447265625, 1630.34814453125, 4.410317420959473, 596.3438720703125, 359.1389465332031, 209.2460174560547, 163.3209228515625, 336.6707763671875, 782.7463989257812, 1411.86279296875, 1909.2861328125, 919.4473876953125, 437.45501708984375, 1162.785888671875, 1011.5746459960938, 719.7986450195312, 1016.1685791015625, 492.14013671875, 8150.74462890625, 13764.986328125, 1360.5289306640625, 810.4945068359375, 170.25094604492188, 1023.2848510742188, 3449.70166015625, 1111.6685791015625, 1088.5057373046875, 530.7720947265625, 3634.631591796875, 794.946533203125, 587.0828247070312, 4687.36767578125, 1395.6536865234375, 6134.181640625, 1490.7615966796875, 3053.383056640625, 2145.61474609375, 4126.12451171875, 2460.81201171875, 2165.345458984375, 3756.9658203125, 5070.994140625, 3291.409912109375, 1481.87255859375, 4123.7734375, 2964.871337890625, 1540.984130859375, 2792.78857421875, 1421.3258056640625, 1615.6749267578125, 1435.6644287109375, 1466.85595703125, 1.6125720739364624, 1.6132959127426147, 3.088299036026001, 3.7477588653564453, 3.089517593383789, 3.7518887519836426, 3.749692678451538, 3.0941214561462402, 1.6144514083862305, 3.0848286151885986, 1.614144206047058, 3.7470154762268066, 3.090144634246826, 4.408044815063477, 5.224560737609863, 3.7494828701019287, 3.0856151580810547, 4.409876823425293, 5.226905822753906, 30.000288009643555, 5.214617729187012, 3.0879414081573486, 1.6143823862075806, 3.0894782543182373, 3.092898368835449, 7.199249744415283, 3.0880842208862305, 3.087341547012329, 1.6139849424362183, 1.6138474941253662, 3.0894107818603516, 58.053558349609375, 9.340984344482422, 2792.78857421875, 377.6426696777344, 217.62005615234375, 368.04791259765625, 4687.36767578125, 2460.81201171875, 510.74627685546875, 1615.6749267578125, 1623.7701416015625, 471.5525207519531, 314.3692932128906, 69.74038696289062, 4123.7734375, 3756.9658203125, 693.549560546875, 995.86669921875, 6134.181640625, 2964.871337890625, 1063.547119140625, 580.0645141601562, 629.9365844726562, 3291.409912109375, 8150.74462890625, 1104.5926513671875, 13764.986328125, 3634.631591796875, 1411.86279296875, 5070.994140625, 887.8599853515625, 2165.345458984375, 1037.6474609375, 911.8848266601562, 1347.160400390625, 1733.4093017578125, 3449.70166015625, 847.9242553710938, 4126.12451171875, 1690.9493408203125, 3053.383056640625, 1465.6087646484375, 2145.61474609375, 1490.7615966796875, 1751.85546875, 1481.87255859375, 1458.5628662109375, 1909.2861328125, 1466.85595703125, 3.7529516220092773, 6.554983139038086, 3.097130537033081, 3.0891785621643066, 3.0886969566345215, 3.0917861461639404, 2.934011697769165, 3.0880720615386963, 1.6148126125335693, 5.892843246459961, 1.616726279258728, 3.090949058532715, 4.413881778717041, 1.6158956289291382, 1.6162948608398438, 4.409867763519287, 1.6144514083862305, 2.2795839309692383, 3.096543550491333, 3.0909156799316406, 3.0946309566497803, 3.0941214561462402, 3.0884621143341064, 1.6156753301620483, 1.616275429725647, 2.2747461795806885, 3.0927999019622803, 2.276350975036621, 3.093413829803467, 3.0909512042999268, 5.890260219573975, 977.8834838867188, 71.3229751586914, 1140.6455078125, 3756.9658203125, 1540.984130859375, 1751.85546875, 461.2701110839844, 920.1760864257812, 3291.409912109375, 200.54127502441406, 314.3101806640625, 44.242523193359375, 263.14251708984375, 8150.74462890625, 747.132568359375, 1014.7357788085938, 311.1706848144531, 2792.78857421875, 882.83935546875, 1175.7745361328125, 2964.871337890625, 2460.81201171875, 4126.12451171875, 605.613525390625, 394.2184753417969, 4687.36767578125, 357.3340759277344, 1486.4852294921875, 1909.2861328125, 2145.61474609375, 3449.70166015625, 2165.345458984375, 837.6585693359375, 13764.986328125, 6134.181640625, 1466.85595703125, 1481.87255859375, 911.3108520507812, 1362.2542724609375, 4123.7734375, 5070.994140625, 1615.6749267578125, 3053.383056640625, 3634.631591796875, 1452.8408203125, 1733.4093017578125, 3.1002092361450195, 3.103261947631836, 5.744568347930908, 3.09877610206604, 3.0942537784576416, 3.0979366302490234, 3.758239269256592, 1.6181061267852783, 5.2385454177856445, 3.0965871810913086, 3.095677137374878, 1.617652177810669, 1.6180776357650757, 3.0952353477478027, 10.832144737243652, 1.6176731586456299, 3.59969425201416, 1.618298053741455, 7.064078330993652, 3.093743085861206, 1.6170074939727783, 1.6170861721038818, 13.314956665039062, 1.6159244775772095, 3.0954904556274414, 1.6171602010726929, 1.6171334981918335, 1.6169058084487915, 1.6163506507873535, 5.234002113342285, 6.552465438842773, 9.352442741394043, 7.214344024658203, 5.235057830810547, 3053.383056640625, 335.2438659667969, 170.3723602294922, 1623.7701416015625, 351.28515625, 373.65557861328125, 2165.345458984375, 474.45111083984375, 1486.4852294921875, 1060.81396484375, 4126.12451171875, 3449.70166015625, 1370.9249267578125, 3634.631591796875, 1104.6767578125, 715.5673217773438, 1421.3258056640625, 911.3108520507812, 6134.181640625, 1177.227783203125, 1490.7615966796875, 4123.7734375, 3291.409912109375, 8150.74462890625, 1104.5926513671875, 1458.5628662109375, 1411.86279296875, 5070.994140625, 540.5160522460938, 3756.9658203125, 2964.871337890625, 669.8532104492188, 13764.986328125, 4687.36767578125, 1615.6749267578125, 2145.61474609375, 1465.6087646484375, 1909.2861328125, 2792.78857421875, 2460.81201171875, 1751.85546875, 1630.34814453125], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.0658999681472778, 0.9829000234603882, 0.9761999845504761, 0.972100019454956, 0.9574000239372253, 0.9355999827384949, 0.8884000182151794, 0.8740000128746033, 0.8414999842643738, 0.8306999802589417, 0.8237000107765198, 0.8095999956130981, 0.8086000084877014, 0.8033000230789185, 0.7968999743461609, 0.791700005531311, 0.7843000292778015, 0.7842000126838684, 0.7734000086784363, 0.7692999839782715, 0.760200023651123, 0.7513999938964844, 0.7372000217437744, 0.7355999946594238, 0.7207000255584717, 0.6998999714851379, 0.694599986076355, 0.689300000667572, 0.6880000233650208, 0.6819999814033508, 0.6722999811172485, 0.5760999917984009, 0.3345000147819519, 0.41999998688697815, 0.45570001006126404, 0.33340001106262207, 0.5006999969482422, 0.5462999939918518, 0.18060000240802765, 0.4587000012397766, 0.39959999918937683, 0.3109000027179718, 0.3919000029563904, 0.2924000024795532, 0.32839998602867126, 0.2199999988079071, 0.28679999709129333, 0.14579999446868896, 0.24240000545978546, 0.2937999963760376, 0.2732999920845032, 0.2101999968290329, 0.22419999539852142, 0.33820000290870667, 0.030899999663233757, 0.03799999877810478, 0.289900004863739, 0.31029999256134033, 0.30480000376701355, 0.26899999380111694, 0.026200000196695328, 0.11169999837875366, 0.19169999659061432, 0.07490000128746033, 0.1437000036239624, 0.22750000655651093, -0.033399999141693115, -0.031700000166893005, -0.03610000014305115, 0.11749999970197678, 0.06300000101327896, 0.18379999697208405, 0.09640000015497208, -0.12200000137090683, 0.08479999750852585, 0.022600000724196434, -0.01730000041425228, -0.28189998865127563, 0.046799998730421066, -0.3560999929904938, -0.3991999924182892, -0.24690000712871552, 1.0257999897003174, 1.0225000381469727, 0.9997000098228455, 0.9652000069618225, 0.9430999755859375, 0.932200014591217, 0.9154999852180481, 0.8737999796867371, 0.8726000189781189, 0.8658000230789185, 0.8460999727249146, 0.8284000158309937, 0.8238000273704529, 0.8181999921798706, 0.7986999750137329, 0.79830002784729, 0.777999997138977, 0.76910001039505, 0.7473999857902527, 0.7414000034332275, 0.7407000064849854, 0.7368999719619751, 0.7360000014305115, 0.7294999957084656, 0.7038999795913696, 0.6987000107765198, 0.6969000101089478, 0.6830999851226807, 0.6758999824523926, 0.6751000285148621, 0.6682999730110168, 0.6740000247955322, 0.6689000129699707, 0.5924999713897705, 0.24570000171661377, 0.46149998903274536, 0.41920000314712524, 0.423799991607666, 0.2957000136375427, 0.42399999499320984, 0.32030001282691956, 0.30480000376701355, 0.4602000117301941, 0.16680000722408295, 0.3384000062942505, 0.265500009059906, 0.15530000627040863, 0.326200008392334, 0.2648000121116638, 0.3192000091075897, 0.09000000357627869, 0.2037999927997589, 0.1907999962568283, 0.1923999935388565, 0.3158000111579895, -0.010900000110268593, 0.10000000149011612, 0.2328999936580658, 0.31790000200271606, 0.22300000488758087, 0.13019999861717224, 0.20029999315738678, 0.131400004029274, 0.1177000030875206, 0.1454000025987625, 0.09200000017881393, 0.11509999632835388, -0.11810000240802765, -0.002099999925121665, 0.05869999900460243, 0.04569999873638153, 0.004999999888241291, 0.12929999828338623, 0.1429000049829483, -0.1193000003695488, -0.1728000044822693, 0.0689999982714653, -0.22599999606609344, -0.4242999851703644, -0.3441999852657318, -0.011300000362098217, 1.420799970626831, 1.257599949836731, 1.0370999574661255, 0.9878000020980835, 0.9653000235557556, 0.9629999995231628, 0.9502000212669373, 0.9433000087738037, 0.9398000240325928, 0.9265999794006348, 0.9088000059127808, 0.9063000082969666, 0.9041000008583069, 0.8999000191688538, 0.8995000123977661, 0.8483999967575073, 0.838699996471405, 0.8317000269889832, 0.8156999945640564, 0.8090999722480774, 0.8062999844551086, 0.7872999906539917, 0.7853999733924866, 0.7835999727249146, 0.7768999934196472, 0.761900007724762, 0.7551000118255615, 0.7545999884605408, 0.753600001335144, 0.7468000054359436, 0.6442999839782715, 0.678600013256073, 0.4081999957561493, 0.7174999713897705, 0.5257999897003174, 0.5156000256538391, 0.32260000705718994, 0.39309999346733093, 0.3815999925136566, 0.33570000529289246, 0.4050999879837036, 0.43959999084472656, 0.5023000240325928, 0.37880000472068787, 0.32690000534057617, 0.251800000667572, 0.147599995136261, 0.2750000059604645, 0.25859999656677246, 0.31349998712539673, 0.3314000070095062, 0.07559999823570251, 0.3319999873638153, 0.11060000211000443, 0.2915000021457672, 0.24140000343322754, -0.12189999967813492, 0.19580000638961792, 0.18150000274181366, 0.22269999980926514, 0.13009999692440033, 0.02759999968111515, 0.15039999783039093, -0.156700000166893, -0.03970000147819519, -0.12999999523162842, 0.18039999902248383, 0.15000000596046448, 0.08219999819993973, 0.14890000224113464, -0.05559999868273735, -0.13619999587535858, -0.051100000739097595, -0.03739999979734421, -0.19200000166893005, 0.02710000053048134, 0.03359999880194664, -0.04089999943971634, -0.2987000048160553, 0.03269999846816063, -0.16259999573230743, 1.22160005569458, 1.1334999799728394, 1.0525000095367432, 1.0506999492645264, 1.0134999752044678, 1.0095000267028809, 0.9703999757766724, 0.9562000036239624, 0.9452999830245972, 0.9279000163078308, 0.9128999710083008, 0.9128000140190125, 0.9082000255584717, 0.9032999873161316, 0.8684999942779541, 0.8119999766349792, 0.8116999864578247, 0.8111000061035156, 0.76910001039505, 0.7663999795913696, 0.7659000158309937, 0.7645000219345093, 0.7581999897956848, 0.7437000274658203, 0.7426999807357788, 0.7246999740600586, 0.7239000201225281, 0.7224000096321106, 0.7087000012397766, 0.6958000063896179, 0.6819000244140625, 0.5098999738693237, 0.19460000097751617, 0.3880999982357025, 0.29159998893737793, 0.37790000438690186, 0.38119998574256897, 0.2793999910354614, 0.12129999697208405, 0.2793999910354614, 0.22499999403953552, 0.3693999946117401, 0.29269999265670776, 0.3009999990463257, 0.20250000059604645, 0.20640000700950623, 0.1881999969482422, 0.4237000048160553, 0.2476000040769577, 0.29339998960494995, 0.06809999793767929, 0.23639999330043793, 0.07530000060796738, 0.299699991941452, 0.2092999964952469, 0.23270000517368317, 0.15209999680519104, -0.05050000175833702, 0.18690000474452972, 0.3325999975204468, -0.14010000228881836, -0.0035000001080334187, -0.041200000792741776, 0.027799999341368675, 0.08900000154972076, -0.029999999329447746, 0.09769999980926514, -0.22050000727176666, -0.14920000731945038, 0.047200001776218414, -0.10530000180006027, -0.009800000116229057, 0.05829999968409538, -0.27900001406669617, -0.3271999955177307, -0.08609999716281891, 0.05139999836683273, -0.02199999988079071, -0.3546999990940094, 1.3258999586105347, 1.226099967956543, 1.198799967765808, 1.1166000366210938, 1.113800048828125, 0.9710999727249146, 0.9710000157356262, 0.9668999910354614, 0.9298999905586243, 0.9196000099182129, 0.9046000242233276, 0.8867999911308289, 0.8676999807357788, 0.8672000169754028, 0.8605999946594238, 0.8604000210762024, 0.8493000268936157, 0.8457000255584717, 0.8312000036239624, 0.8227999806404114, 0.8109999895095825, 0.8108999729156494, 0.8011999726295471, 0.7731000185012817, 0.7648000121116638, 0.7578999996185303, 0.7472000122070312, 0.7301999926567078, 0.7081000208854675, 0.7067000269889832, 0.7046999931335449, 0.6341999769210815, 0.5591999888420105, 0.4275999963283539, 0.3765000104904175, 0.48069998621940613, 0.26460000872612, 0.2273000031709671, 0.3630000054836273, 0.289900004863739, 0.052299998700618744, 0.24300000071525574, 0.350600004196167, 0.27129998803138733, 0.25529998540878296, 0.09740000218153, 0.35339999198913574, 0.08889999985694885, 0.3714999854564667, 0.1607999950647354, 0.13840000331401825, -0.054099999368190765, 0.16580000519752502, 0.21879999339580536, 0.1492999941110611, 0.20340000092983246, 0.21089999377727509, 0.12970000505447388, 0.14329999685287476, -0.05260000005364418, 0.12680000066757202, -0.02070000022649765, 0.10409999638795853, -0.051100000739097595, -0.17900000512599945, -0.09510000050067902, -0.020099999383091927, 0.07500000298023224, 0.016100000590085983, 0.033399999141693115, -0.3517000079154968, -0.2897999882698059, -0.1834000051021576, -0.18870000541210175, -0.3001999855041504, 1.3372999429702759, 1.076300024986267, 1.0687999725341797, 0.9969000220298767, 0.9921000003814697, 0.9801999926567078, 0.8769999742507935, 0.8754000067710876, 0.8500999808311462, 0.8442000150680542, 0.8371999859809875, 0.8051000237464905, 0.8014000058174133, 0.7943000197410583, 0.789900004863739, 0.7854999899864197, 0.7684000134468079, 0.7677000164985657, 0.7497000098228455, 0.7491999864578247, 0.7480000257492065, 0.7476999759674072, 0.7361999750137329, 0.7304999828338623, 0.7294999957084656, 0.7240999937057495, 0.7026000022888184, 0.6974999904632568, 0.6952999830245972, 0.6938999891281128, 0.6930000185966492, 0.6680999994277954, 0.6549000144004822, 0.6784999966621399, 0.515500009059906, 0.3504999876022339, 0.45989999175071716, 0.40959998965263367, 0.2529999911785126, 0.39660000801086426, 0.5947999954223633, 0.1965000033378601, 0.33169999718666077, 0.3799999952316284, 0.1688999980688095, 0.05920000001788139, 0.15710000693798065, 0.2847999930381775, 0.10589999705553055, 0.3215000033378601, 0.28130000829696655, 0.3156000077724457, 0.24819999933242798, 0.34040001034736633, 0.12610000371932983, 0.24240000545978546, 0.2143000066280365, 0.2175000011920929, 0.17309999465942383, 0.0478999987244606, 0.16019999980926514, 0.22750000655651093, -0.08089999854564667, 0.15199999511241913, 0.18400000035762787, -0.0026000000070780516, -0.2037999927997589, 0.10769999772310257, 0.20190000534057617, -0.08240000158548355, 0.10180000215768814, -0.06440000236034393, 0.12229999899864197, -0.08060000091791153, 0.12950000166893005, -0.2687999904155731, -0.014600000344216824, -0.06360000371932983, -0.11379999667406082, 0.019300000742077827, -0.288100004196167, -0.13779999315738678, -0.05920000001788139, -0.4417000114917755, -0.22619999945163727, 1.0542999505996704, 1.0471999645233154, 1.0128999948501587, 0.9208999872207642, 0.9092000126838684, 0.8812999725341797, 0.8339999914169312, 0.8087999820709229, 0.8072999715805054, 0.8052999973297119, 0.8033999800682068, 0.7861999869346619, 0.7677000164985657, 0.7617999911308289, 0.7577000260353088, 0.7527999877929688, 0.7457000017166138, 0.7454000115394592, 0.7418000102043152, 0.7372999787330627, 0.7240999937057495, 0.7189000248908997, 0.7174000144004822, 0.7110999822616577, 0.7071999907493591, 0.6934000253677368, 0.6862000226974487, 0.6836000084877014, 0.6827999949455261, 0.6805999875068665, 0.6804999709129333, 0.6223000288009644, 0.6190000176429749, 0.46560001373291016, 0.6725999712944031, 0.44999998807907104, 0.4546999931335449, 0.47839999198913574, 0.4909999966621399, 0.4471000134944916, 0.3813999891281128, 0.3066999912261963, 0.27379998564720154, 0.33090001344680786, 0.3714999854564667, 0.29190000891685486, 0.29980000853538513, 0.3280999958515167, 0.28700000047683716, 0.3449999988079071, 0.07699999958276749, 0.017799999564886093, 0.2337000072002411, 0.28380000591278076, 0.4381999969482422, 0.23899999260902405, 0.1046999990940094, 0.2184000015258789, 0.22089999914169312, 0.30399999022483826, 0.07000000029802322, 0.249099999666214, 0.28380000591278076, 0.009800000116229057, 0.16899999976158142, -0.04349999874830246, 0.14139999449253082, -0.003000000026077032, 0.05000000074505806, -0.07800000160932541, 0.02199999988079071, 0.036400001496076584, -0.08489999920129776, -0.1808999925851822, -0.1177000030875206, 0.11110000312328339, -0.2531000077724457, -0.14579999446868896, 0.01510000042617321, -0.3368000090122223, 0.04830000177025795, -0.0575999990105629, 0.026799999177455902, -0.023800000548362732, 1.3496999740600586, 1.2963000535964966, 1.2309999465942383, 1.0801000595092773, 1.0613000392913818, 1.0405999422073364, 1.0139000415802002, 0.9785000085830688, 0.9553999900817871, 0.919700026512146, 0.9125999808311462, 0.8794000148773193, 0.8650000095367432, 0.863099992275238, 0.8248000144958496, 0.8159000277519226, 0.8105000257492065, 0.8100000023841858, 0.7932999730110168, 0.7914000153541565, 0.7896999716758728, 0.7849000096321106, 0.7813000082969666, 0.7750999927520752, 0.7716000080108643, 0.7674000263214111, 0.7603999972343445, 0.7569000124931335, 0.7483000159263611, 0.7371000051498413, 0.7358999848365784, 0.66839998960495, 0.7109000086784363, 0.4327000081539154, 0.5047000050544739, 0.4878999888896942, 0.447299987077713, 0.21660000085830688, 0.2648000121116638, 0.38830000162124634, 0.28349998593330383, 0.26489999890327454, 0.3528999984264374, 0.39079999923706055, 0.5361999869346619, 0.1136000007390976, 0.12060000002384186, 0.28870001435279846, 0.24889999628067017, 0.04540000110864639, 0.12380000203847885, 0.23350000381469727, 0.303600013256073, 0.2919999957084656, 0.0786999985575676, -0.0869000032544136, 0.20080000162124634, -0.17870000004768372, 0.013199999928474426, 0.16249999403953552, -0.052299998700618744, 0.22630000114440918, 0.07360000163316727, 0.19779999554157257, 0.21060000360012054, 0.13570000231266022, 0.07760000228881836, -0.0803999975323677, 0.21930000185966492, -0.19939999282360077, 0.03400000184774399, -0.13249999284744263, 0.05999999865889549, -0.06689999997615814, 0.05220000073313713, -0.00839999970048666, 0.010400000028312206, -0.022299999371170998, -0.23119999468326569, -0.04500000178813934, 1.243499994277954, 1.2345999479293823, 1.0994999408721924, 1.0699000358581543, 0.9907000064849854, 0.9807999730110168, 0.9645000100135803, 0.9531000256538391, 0.9444000124931335, 0.9218000173568726, 0.8992999792098999, 0.8942999839782715, 0.8766999840736389, 0.8626999855041504, 0.8460999727249146, 0.8453999757766724, 0.8363000154495239, 0.8069000244140625, 0.7910000085830688, 0.7778000235557556, 0.775600016117096, 0.7694000005722046, 0.7630000114440918, 0.7483000159263611, 0.7371000051498413, 0.7371000051498413, 0.729200005531311, 0.7250000238418579, 0.72079998254776, 0.6991000175476074, 0.6855000257492065, 0.5116999745368958, 0.5879999995231628, 0.46209999918937683, 0.3605000078678131, 0.38659998774528503, 0.37720000743865967, 0.45590001344680786, 0.4092999994754791, 0.3046000003814697, 0.4934999942779541, 0.4336000084877014, 0.583299994468689, 0.43140000104904175, 0.15479999780654907, 0.3467000126838684, 0.30379998683929443, 0.4090000092983246, 0.17980000376701355, 0.2915000021457672, 0.23849999904632568, 0.12610000371932983, 0.14409999549388885, 0.06419999897480011, 0.31139999628067017, 0.3560999929904938, 0.026900000870227814, 0.36579999327659607, 0.1639000028371811, 0.1257999986410141, 0.09709999710321426, 0.01119999960064888, 0.0625, 0.22470000386238098, -0.28110000491142273, -0.16259999573230743, 0.1080000028014183, 0.09070000052452087, 0.1931000053882599, 0.09730000048875809, -0.18549999594688416, -0.24770000576972961, 0.02459999918937683, -0.2574999928474426, -0.3538999855518341, 0.04270000010728836, -0.09000000357627869, 1.4038000106811523, 1.1750999689102173, 1.0717999935150146, 1.065999984741211, 1.0426000356674194, 1.0414999723434448, 1.0296000242233276, 1.0181000232696533, 0.9754999876022339, 0.95169997215271, 0.911899983882904, 0.9036999940872192, 0.8416000008583069, 0.833299994468689, 0.8246999979019165, 0.8230999708175659, 0.7912999987602234, 0.7875999808311462, 0.786899983882904, 0.7868000268936157, 0.7581999897956848, 0.7567999958992004, 0.7552000284194946, 0.7494000196456909, 0.7488999962806702, 0.7479000091552734, 0.7455999851226807, 0.7409999966621399, 0.7371000051498413, 0.7332000136375427, 0.7279000282287598, 0.7221999764442444, 0.7232999801635742, 0.7231000065803528, 0.4839000105857849, 0.5181999802589417, 0.5421000123023987, 0.3862000107765198, 0.47769999504089355, 0.45840001106262207, 0.3176000118255615, 0.414900004863739, 0.3156000077724457, 0.3357999920845032, 0.22370000183582306, 0.21960000693798065, 0.2937999963760376, 0.18940000236034393, 0.28600001335144043, 0.3230000138282776, 0.2515999972820282, 0.2709999978542328, 0.042100001126527786, 0.2312999963760376, 0.18379999697208405, 0.04320000112056732, 0.07280000299215317, -0.059300001710653305, 0.20509999990463257, 0.1565999984741211, 0.13019999861717224, -0.1185000017285347, 0.27880001068115234, -0.07639999687671661, -0.06729999929666519, 0.23569999635219574, -0.4611000120639801, -0.21220000088214874, 0.029600000008940697, -0.09229999780654907, 0.019700000062584877, -0.10289999842643738, -0.3635999858379364, -0.36739999055862427, -0.1362999975681305, -0.0794999971985817], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -10.614500045776367, -11.699899673461914, -11.707599639892578, -10.87030029296875, -11.725799560546875, -11.1003999710083, -11.145400047302246, -11.809100151062012, -11.193699836730957, -11.206000328063965, -11.860099792480469, -11.873600006103516, -11.074799537658691, -11.232000350952148, -11.887700080871582, -11.24269962310791, -11.556500434875488, -11.249699592590332, -11.067999839782715, -11.913700103759766, -11.92300033569336, -10.250200271606445, -11.296699523925781, -10.65530014038086, -11.314000129699707, -11.983599662780762, -11.339699745178223, -11.344200134277344, -11.995100021362305, -11.353400230407715, -10.836999893188477, -8.684200286865234, -3.822700023651123, -5.553500175476074, -6.531000137329102, -4.730599880218506, -7.787399768829346, -8.511300086975098, -3.4526000022888184, -7.488500118255615, -6.802000045776367, -6.02839994430542, -7.024199962615967, -5.8942999839782715, -6.35830020904541, -5.262800216674805, -6.01039981842041, -4.692699909210205, -5.703700065612793, -6.215400218963623, -6.106599807739258, -5.560299873352051, -5.815199851989746, -6.78410005569458, -4.4105000495910645, -4.593800067901611, -6.454500198364258, -6.607500076293945, -6.5731000900268555, -6.38700008392334, -4.684199810028076, -5.380099773406982, -5.901599884033203, -5.279900074005127, -5.729300022125244, -6.194200038909912, -4.871399879455566, -4.963399887084961, -5.000899791717529, -5.741399765014648, -5.642199993133545, -6.072700023651123, -5.797299861907959, -5.261000156402588, -5.8256001472473145, -5.707399845123291, -5.711900234222412, -5.298900127410889, -5.825399875640869, -5.584400177001953, -5.567699909210205, -5.855400085449219, -11.660599708557129, -11.015299797058105, -11.343700408935547, -11.072199821472168, -11.74269962310791, -11.753999710083008, -11.120200157165527, -10.667699813842773, -11.163800239562988, -11.819899559020996, -11.190799713134766, -11.208100318908691, -11.862600326538086, -11.218500137329102, -11.237500190734863, -10.713199615478516, -11.906800270080566, -11.916099548339844, -11.138799667358398, -11.10099983215332, -11.295299530029297, -11.148799896240234, -11.30049991607666, -10.460200309753418, -11.981100082397461, -11.336799621582031, -11.145600318908691, -12.001899719238281, -11.359999656677246, -12.00979995727539, -10.144000053405762, -10.86709976196289, -10.616600036621094, -7.910099983215332, -3.387500047683716, -6.846399784088135, -6.286200046539307, -6.523499965667725, -4.932499885559082, -6.70550012588501, -5.619500160217285, -5.6691999435424805, -7.684999942779541, -4.465000152587891, -6.5391998291015625, -5.890399932861328, -4.861700057983398, -6.631100177764893, -6.156799793243408, -6.734799861907959, -4.748499870300293, -5.742400169372559, -5.668000221252441, -5.6894001960754395, -6.7459001541137695, -4.168099880218506, -5.06850004196167, -6.124800205230713, -6.779300212860107, -6.171500205993652, -5.599800109863281, -6.056300163269043, -5.6350998878479, -5.587500095367432, -5.758299827575684, -5.516499996185303, -5.6605000495910645, -4.559500217437744, -5.141200065612793, -5.433199882507324, -5.436999797821045, -5.349800109863281, -5.792600154876709, -5.848599910736084, -5.084099769592285, -5.010799884796143, -5.701600074768066, -5.289999961853027, -5.134699821472168, -5.275899887084961, -5.705900192260742, -10.612500190734863, -11.424799919128418, -10.246000289916992, -11.695199966430664, -11.71780014038086, -11.071100234985352, -11.083700180053711, -10.472999572753906, -11.093400001525879, -11.756099700927734, -11.774100303649902, -10.537699699401855, -10.403499603271484, -11.13479995727539, -11.135600090026855, -11.834500312805176, -11.195599555969238, -11.851900100708008, -10.86240005493164, -11.225099563598633, -10.380999565124512, -11.895999908447266, -10.722299575805664, -11.251500129699707, -10.342599868774414, -11.271499633789062, -11.928500175476074, -11.279399871826172, -11.279999732971191, -11.339599609375, -7.810400009155273, -9.352999687194824, -4.302199840545654, -10.469799995422363, -7.120100021362305, -7.25, -4.309100151062012, -5.80049991607666, -5.722899913787842, -5.604100227355957, -6.48360013961792, -6.976500034332275, -7.650899887084961, -6.622099876403809, -6.305600166320801, -5.670100212097168, -4.78410005569458, -5.984399795532227, -5.8907999992370605, -6.406599998474121, -6.5854997634887695, -4.762400150299072, -6.646200180053711, -5.0578999519348145, -6.406099796295166, -6.077700138092041, -3.755000114440918, -5.795599937438965, -5.712100028991699, -5.97790002822876, -5.564499855041504, -5.111400127410889, -5.722599983215332, -4.313899993896484, -4.878300189971924, -4.571400165557861, -5.9116997718811035, -5.823999881744385, -5.623000144958496, -5.890399932861328, -5.283899784088135, -5.10099983215332, -5.405900001525879, -5.529300212860107, -5.209000110626221, -5.702899932861328, -5.732900142669678, -5.649499893188477, -5.36269998550415, -5.737800121307373, -5.645299911499023, -10.812600135803223, -10.900300025939941, -10.983599662780762, -11.631600379943848, -11.67039966583252, -11.026700019836426, -11.066100120544434, -11.385700225830078, -10.734100341796875, -11.756099700927734, -11.121600151062012, -11.771100044250488, -11.128100395202637, -11.128800392150879, -10.973400115966797, -11.22189998626709, -11.221099853515625, -9.814599990844727, -11.571599960327148, -10.742199897766113, -11.91670036315918, -11.576899528503418, -10.920900344848633, -9.965999603271484, -10.76669979095459, -10.441100120544434, -11.310199737548828, -9.72719955444336, -11.133399963378906, -11.986800193786621, -9.943900108337402, -7.875999927520752, -3.4386000633239746, -6.168900012969971, -5.200300216674805, -6.395899772644043, -6.624100208282471, -5.890999794006348, -4.320099830627441, -5.961299896240234, -5.505000114440918, -6.9095001220703125, -6.252099990844727, -6.388700008392334, -5.6203999519348145, -5.665800094604492, -5.673799991607666, -7.454699993133545, -6.173999786376953, -6.511499881744385, -4.896699905395508, -6.163099765777588, -5.093100070953369, -6.59630012512207, -6.0229997634887695, -6.252099990844727, -5.795300006866455, -4.682300090789795, -6.0696001052856445, -6.896500110626221, -4.297299861907959, -5.067500114440918, -4.879199981689453, -5.327000141143799, -5.616199970245361, -5.169099807739258, -5.780200004577637, -4.9309000968933105, -5.166200160980225, -5.719299793243408, -5.333600044250488, -5.6184000968933105, -5.80049991607666, -5.210700035095215, -5.1656999588012695, -5.780700206756592, -5.922599792480469, -5.903800010681152, -5.837399959564209, -11.359800338745117, -10.809900283813477, -10.836799621582031, -11.569899559020996, -11.571000099182129, -10.871299743652344, -11.064599990844727, -11.069600105285645, -11.10509967803955, -11.116399765014648, -11.779999732971191, -11.147899627685547, -11.167200088500977, -10.523300170898438, -11.17549991607666, -11.175000190734863, -11.185999870300293, -10.663900375366211, -11.203399658203125, -11.213399887084961, -10.399499893188477, -11.87440013885498, -11.884099960327148, -11.26140022277832, -11.921600341796875, -11.277600288391113, -11.287099838256836, -11.955100059509277, -11.976300239562988, -11.135000228881836, -10.5802001953125, -8.991900444030762, -7.503799915313721, -5.758999824523926, -5.353499889373779, -7.401899814605713, -4.700200080871582, -4.214099884033203, -6.445799827575684, -5.631999969482422, -3.580899953842163, -5.638800144195557, -6.626699924468994, -6.067200183868408, -5.98390007019043, -4.741099834442139, -6.843999862670898, -4.928100109100342, -7.053500175476074, -5.544400215148926, -5.470200061798096, -4.211299896240234, -5.707300186157227, -6.180699825286865, -5.761099815368652, -6.093900203704834, -6.169000148773193, -5.726399898529053, -5.814000129699707, -4.890600204467773, -5.846700191497803, -5.1890997886657715, -5.7895002365112305, -5.190199851989746, -4.810699939727783, -5.15910005569458, -5.502799987792969, -5.8028998374938965, -5.678500175476074, -5.7895002365112305, -5.062099933624268, -5.221499919891357, -5.411600112915039, -5.543499946594238, -5.792099952697754, -11.345000267028809, -10.958399772644043, -11.271499633789062, -11.037799835205078, -10.848899841308594, -10.698699951171875, -11.15719985961914, -8.88640022277832, -11.184900283813477, -10.996199607849121, -10.282299995422363, -11.228699684143066, -11.233699798583984, -11.24020004272461, -10.888199806213379, -10.6048002243042, -11.265299797058105, -10.331399917602539, -11.591300010681152, -11.934100151062012, -11.935600280761719, -11.593199729919434, -11.603799819946289, -11.61050033569336, -11.953499794006348, -11.310500144958496, -11.332200050354004, -11.337400436401367, -9.338399887084961, -11.34060001373291, -11.190299987792969, -9.757800102233887, -9.814200401306152, -10.712400436401367, -7.329100131988525, -5.425000190734863, -7.357699871063232, -6.58650016784668, -4.678599834442139, -6.788400173187256, -9.530699729919434, -4.513899803161621, -6.303999900817871, -6.87470006942749, -4.669600009918213, -3.5739998817443848, -4.680799961090088, -6.07289981842041, -4.5258002281188965, -6.626699924468994, -6.2795000076293945, -6.60129976272583, -6.088399887084961, -6.835299968719482, -5.102200031280518, -6.054900169372559, -5.9446001052856445, -5.978799819946289, -5.730599880218506, -4.969099998474121, -5.695799827575684, -6.1707000732421875, -4.522299766540527, -5.795400142669678, -5.971700191497803, -5.17110013961792, -4.361000061035156, -5.715199947357178, -6.145599842071533, -5.0472002029418945, -5.776000022888184, -5.203499794006348, -5.869200229644775, -5.435400009155273, -5.994200229644775, -5.332799911499023, -5.785099983215332, -5.758299827575684, -5.722300052642822, -5.862400054931641, -5.78000020980835, -5.867800235748291, -5.932199954986572, -5.924499988555908, -5.931399822235107, -10.7878999710083, -10.791899681091309, -11.670700073242188, -11.42039966583252, -11.126099586486816, -11.460100173950195, -11.507399559020996, -11.225899696350098, -11.227999687194824, -10.360400199890137, -11.039199829101562, -11.89799976348877, -10.313899993896484, -10.917699813842773, -11.278200149536133, -11.931099891662598, -11.937999725341797, -10.672699928283691, -11.293000221252441, -11.297800064086914, -10.785499572753906, -11.622599601745605, -10.449299812316895, -11.629199981689453, -11.976400375366211, -11.647899627685547, -11.349200248718262, -11.351499557495117, -10.996999740600586, -11.661199569702148, -11.352299690246582, -8.903499603271484, -9.19629955291748, -5.300899982452393, -11.006500244140625, -6.322199821472168, -6.8246002197265625, -7.341100215911865, -7.576300144195557, -6.8968000411987305, -6.118899822235107, -5.603700160980225, -5.334799766540527, -6.008399963378906, -6.7104997634887695, -5.812600135803223, -5.943999767303467, -6.25600004196167, -5.952199935913086, -6.61929988861084, -4.0802001953125, -3.6154000759124756, -5.713699817657471, -6.181600093841553, -7.587600231170654, -5.993299961090088, -4.912300109863281, -5.931000232696533, -5.9496002197265625, -6.584700107574463, -4.894800186157227, -6.2357001304626465, -6.504000186920166, -4.7006001472473145, -5.752900123596191, -4.484899997711182, -5.714600086212158, -5.142000198364258, -5.441800117492676, -4.915900230407715, -5.332799911499023, -5.446300029754639, -5.016600131988525, -4.812699794769287, -5.181700229644775, -5.750899791717529, -5.091599941253662, -5.314199924468994, -5.807799816131592, -5.565100193023682, -5.855400085449219, -5.833099842071533, -5.866799831390381, -5.895999908447266, -11.33549976348877, -11.388500213623047, -10.804400444030762, -10.761799812316895, -10.973699569702148, -10.800200462341309, -10.827400207519531, -11.055100440979004, -11.728599548339844, -11.116800308227539, -11.771599769592285, -10.962699890136719, -11.169899940490723, -10.816499710083008, -10.684800148010254, -11.025500297546387, -11.225799560546875, -10.869199752807617, -10.715900421142578, -8.970499992370605, -10.72189998626709, -11.250699996948242, -11.902799606323242, -11.259900093078613, -11.262399673461914, -10.421699523925781, -11.275099754333496, -11.278900146484375, -11.935999870300293, -11.94729995727539, -11.299200057983398, -8.433300018310547, -10.217700004577637, -4.795599937438965, -6.724400043487549, -7.292399883270264, -6.807499885559082, -4.493800163269043, -5.090000152587891, -6.53879976272583, -5.492000102996826, -5.5055999755859375, -6.654099941253662, -7.021699905395508, -8.381999969482422, -4.724999904632568, -4.811100006103516, -6.332600116729736, -6.0106000900268555, -4.395999908447266, -5.0447001457214355, -5.960100173950195, -6.496300220489502, -6.42549991607666, -4.985199928283691, -4.244100093841553, -5.955100059509277, -3.8118999004364014, -4.951600074768066, -5.747900009155273, -4.684000015258789, -6.147900104522705, -5.409200191497803, -6.020500183105469, -6.136899948120117, -5.821599960327148, -5.627600193023682, -5.097400188446045, -6.201000213623047, -5.037300109863281, -5.696000099182129, -5.271500110626221, -5.813000202178955, -5.558700084686279, -5.803800106048584, -5.703100204467773, -5.851600170135498, -5.900100231170654, -5.839799880981445, -5.917099952697754, -10.597000122070312, -10.048299789428711, -10.933099746704102, -10.965200424194336, -11.044599533081055, -11.053500175476074, -11.122200012207031, -11.08240032196045, -11.739399909973145, -10.467599868774414, -11.783300399780273, -11.140299797058105, -10.801600456237793, -11.820500373840332, -11.836899757385254, -10.833800315856934, -11.847800254821777, -11.53219985961914, -11.241800308227539, -11.256699562072754, -11.257699966430664, -11.264200210571289, -11.27239990234375, -11.9350004196167, -11.945799827575684, -11.604100227355957, -11.304699897766113, -11.615500450134277, -11.312999725341797, -11.33549976348877, -10.704299926757812, -5.765900135040283, -8.307900428771973, -5.661600112915039, -4.571199893951416, -5.436299800872803, -5.317399978637695, -6.573200225830078, -5.929200172424316, -4.759399890899658, -7.368500232696533, -6.979000091552734, -8.79010009765625, -7.158899784088135, -4.002399921417236, -6.200099945068359, -5.936800003051758, -7.013700008392334, -5.048399925231934, -6.088399887084961, -5.854800224304199, -5.042399883270264, -5.210700035095215, -4.773799896240234, -6.445400238037109, -6.830100059509277, -4.683499813079834, -6.918600082397461, -5.695000171661377, -5.482800006866455, -5.394800186157227, -5.005799770355225, -5.420199871063232, -6.207799911499023, -3.914299964904785, -4.604000091552734, -5.764200210571289, -5.771299839019775, -6.15500020980835, -5.848899841308594, -5.024099826812744, -4.879499912261963, -5.750999927520752, -5.396500110626221, -5.318699836730957, -5.839099884033203, -5.795199871063232, -10.627799987792969, -10.855500221252441, -10.343000411987305, -10.966099739074707, -10.990900039672852, -10.990799903869629, -10.809499740600586, -11.663700103759766, -10.531499862670898, -11.081100463867188, -11.121199607849121, -11.778400421142578, -11.840200424194336, -11.199799537658691, -9.955900192260742, -11.859000205993652, -11.090900421142578, -11.894100189208984, -10.421099662780762, -11.246899604797363, -11.924300193786621, -11.925600051879883, -9.819000244140625, -11.933699607849121, -11.284199714660645, -11.934499740600586, -11.936800003051758, -11.941499710083008, -11.945799827575684, -10.774700164794922, -10.555299758911133, -10.2052001953125, -10.463700294494629, -10.784500122070312, -4.655200004577637, -6.829999923706055, -7.482999801635742, -5.384399890899658, -6.823800086975098, -6.781300067901611, -5.16510009765625, -6.585999965667725, -5.543300151824951, -5.860499858856201, -4.614299774169922, -4.797399997711182, -5.645999908447266, -4.775300025939941, -5.869800090789795, -6.267000198364258, -5.652100086212158, -6.077199935913086, -4.3993000984191895, -5.860799789428711, -5.6722002029418945, -4.795300006866455, -4.991199970245361, -4.2164998054504395, -5.950699806213379, -5.721199989318848, -5.780200004577637, -4.750199794769287, -6.591800212860107, -5.007999897003174, -5.2357001304626465, -6.420300006866455, -4.094200134277344, -4.922599792480469, -5.7459001541137695, -5.584199905395508, -5.853300094604492, -5.71150016784668, -5.591800212860107, -5.7221999168396, -5.830900192260742, -5.8460001945495605]}, \"token.table\": {\"Topic\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 4, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 6, 7, 8, 7, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 9, 6, 2, 9, 4, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 6, 8, 2, 5, 6, 2, 7, 9, 10, 1, 1, 5, 1, 3, 4, 7, 9, 10, 4, 10, 5, 1, 5, 1, 7, 8, 7, 1, 3, 8, 3, 4, 5, 7, 9, 5, 10, 7, 7, 9, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 6, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 7, 1, 2, 3, 4, 6, 7, 8, 9, 10, 3, 8, 1, 5, 3, 1, 2, 3, 5, 6, 7, 8, 9, 10, 4, 6, 8, 6, 8, 5, 8, 4, 7, 1, 4, 3, 6, 7, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 9, 1, 2, 3, 4, 6, 7, 8, 9, 10, 1, 5, 10, 6, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 6, 7, 8, 10, 10, 3, 10, 1, 2, 6, 9, 10, 8, 9, 9, 6, 10, 7, 10, 10, 2, 3, 5, 7, 9, 6, 2, 4, 1, 8, 9, 8, 9, 10, 4, 5, 1, 4, 1, 2, 3, 4, 5, 6, 7, 9, 10, 7, 9, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 3, 5, 6, 8, 9, 8, 2, 1, 2, 3, 4, 6, 7, 9, 10, 1, 3, 5, 7, 9, 10, 10, 1, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 8, 9, 2, 4, 2, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 8, 8, 5, 9, 6, 1, 3, 5, 9, 5, 3, 4, 5, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 5, 7, 1, 2, 3, 6, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 5, 6, 7, 10, 2, 4, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 5, 6, 7, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 7, 1, 2, 7, 4, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 6, 7, 8, 9, 10, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 8, 1, 2, 3, 4, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 8, 1, 2, 4, 5, 7, 9, 10, 6, 7, 6, 2, 3, 5, 6, 9, 4, 1, 2, 3, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 5, 6, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 4, 7, 9, 10, 5, 4, 5, 3, 1, 5, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 8, 9, 10, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 5, 1, 4, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 8, 9, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 6, 8, 7, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 4, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 4, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 4, 9, 3, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 8, 2, 3, 4, 10, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 1, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 6, 7, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 8], \"Freq\": [0.09497661888599396, 0.10480178892612457, 0.10709432512521744, 0.09956169873476028, 0.094321608543396, 0.09170155972242355, 0.0975966602563858, 0.08547895401716232, 0.07368875294923782, 0.15065257251262665, 0.14992494881153107, 0.10379419475793839, 0.08894892781972885, 0.08931699395179749, 0.09397914260625839, 0.07987000048160553, 0.1057572066783905, 0.08956237137317657, 0.11127817630767822, 0.08747667074203491, 0.12858712673187256, 0.13418102264404297, 0.0921177789568901, 0.12480942159891129, 0.1044679582118988, 0.10395941883325577, 0.09960053116083145, 0.08172910660505295, 0.07199425995349884, 0.05855436250567436, 0.0913691446185112, 0.12757201492786407, 0.10688466578722, 0.1292959600687027, 0.07930152118206024, 0.07757757604122162, 0.0913691446185112, 0.13274385035037994, 0.07068179547786713, 0.09309308975934982, 0.1339278221130371, 0.1099131777882576, 0.08866945654153824, 0.07204393297433853, 0.09698221832513809, 0.06280753016471863, 0.10160041600465775, 0.10529498010873795, 0.10160041600465775, 0.12746234238147736, 0.3237106502056122, 0.3237106502056122, 0.3237106502056122, 0.08447493612766266, 0.096837118268013, 0.14422549307346344, 0.1318633109331131, 0.12774258852005005, 0.07211274653673172, 0.08035420626401901, 0.09477675706148148, 0.07829384505748749, 0.08653529733419418, 0.3234376311302185, 0.3234376311302185, 0.1596245914697647, 0.09577475488185883, 0.07981229573488235, 0.07981229573488235, 0.07023482024669647, 0.09577475488185883, 0.10215973854064941, 0.10535223037004471, 0.10854472219944, 0.10854472219944, 0.11763837933540344, 0.0978672206401825, 0.10676424205303192, 0.08897019922733307, 0.08699308335781097, 0.08205029368400574, 0.13246674835681915, 0.10478712618350983, 0.0771075114607811, 0.10676424205303192, 0.19139385223388672, 0.19139385223388672, 0.19139385223388672, 0.3236004710197449, 0.2781243920326233, 0.14351241290569305, 0.09948019683361053, 0.13698911666870117, 0.09295690059661865, 0.09784936904907227, 0.09458772838115692, 0.07827949523925781, 0.09132608026266098, 0.08154114335775375, 0.08317196369171143, 0.1386127471923828, 0.1386127471923828, 0.1386127471923828, 0.1386127471923828, 0.1386127471923828, 0.1386127471923828, 0.1386127471923828, 0.1386127471923828, 0.1386127471923828, 0.1386127471923828, 0.32352522015571594, 0.32352522015571594, 0.3236931264400482, 0.32376113533973694, 0.32376113533973694, 0.22674104571342468, 0.22674104571342468, 0.07907294481992722, 0.07907294481992722, 0.07907294481992722, 0.23721882700920105, 0.07907294481992722, 0.15814588963985443, 0.07907294481992722, 0.07907294481992722, 0.07907294481992722, 0.07907294481992722, 0.32367512583732605, 0.32367512583732605, 0.3245910704135895, 0.3245910704135895, 0.22652463614940643, 0.3241828680038452, 0.3229407072067261, 0.3229407072067261, 0.3229407072067261, 0.3237329423427582, 0.3234426975250244, 0.3234426975250244, 0.1696973741054535, 0.1696973741054535, 0.1696973741054535, 0.1696973741054535, 0.1696973741054535, 0.3227955102920532, 0.32293617725372314, 0.32293617725372314, 0.3238496780395508, 0.2669675350189209, 0.2669675350189209, 0.3241671025753021, 0.3241671025753021, 0.3241671025753021, 0.32377052307128906, 0.3232170343399048, 0.3232170343399048, 0.3232170343399048, 0.1911962926387787, 0.1911962926387787, 0.1911962926387787, 0.1911962926387787, 0.1911962926387787, 0.3225588798522949, 0.3225588798522949, 0.43968823552131653, 0.32352501153945923, 0.32352501153945923, 0.26643243432044983, 0.1816941499710083, 0.08075295388698578, 0.10094119608402252, 0.08075295388698578, 0.08075295388698578, 0.08075295388698578, 0.10094119608402252, 0.12112943083047867, 0.08075295388698578, 0.10094119608402252, 0.32410213351249695, 0.32338622212409973, 0.32338622212409973, 0.32338622212409973, 0.04795234277844429, 0.09590468555688858, 0.09590468555688858, 0.09590468555688858, 0.09590468555688858, 0.19180937111377716, 0.09590468555688858, 0.04795234277844429, 0.09590468555688858, 0.14385703206062317, 0.3239336609840393, 0.3239336609840393, 0.10705509781837463, 0.10705509781837463, 0.10705509781837463, 0.10705509781837463, 0.10705509781837463, 0.10705509781837463, 0.21411019563674927, 0.10705509781837463, 0.10705509781837463, 0.32367923855781555, 0.32367923855781555, 0.3239870071411133, 0.3239870071411133, 0.6183217167854309, 0.14190806448459625, 0.14190806448459625, 0.14190806448459625, 0.2838161289691925, 0.14190806448459625, 0.14190806448459625, 0.14190806448459625, 0.14190806448459625, 0.14190806448459625, 0.2668261229991913, 0.2668261229991913, 0.2668261229991913, 0.3238028287887573, 0.3238028287887573, 0.3235063850879669, 0.32360944151878357, 0.3233083188533783, 0.3237422704696655, 0.3234280049800873, 0.3234280049800873, 0.22668690979480743, 0.22668690979480743, 0.22668690979480743, 0.3238106667995453, 0.06475590169429779, 0.06475590169429779, 0.12951180338859558, 0.06475590169429779, 0.06475590169429779, 0.19426770508289337, 0.06475590169429779, 0.12951180338859558, 0.06475590169429779, 0.06475590169429779, 0.4394227862358093, 0.26700690388679504, 0.26700690388679504, 0.26700690388679504, 0.08599910140037537, 0.08599910140037537, 0.08599910140037537, 0.2579973042011261, 0.08599910140037537, 0.08599910140037537, 0.08599910140037537, 0.08599910140037537, 0.08599910140037537, 0.08599910140037537, 0.32398468255996704, 0.3408302664756775, 0.09231781959533691, 0.09231781959533691, 0.09231781959533691, 0.09231781959533691, 0.09231781959533691, 0.09231781959533691, 0.09231781959533691, 0.09231781959533691, 0.18463563919067383, 0.323179692029953, 0.323179692029953, 0.323179692029953, 0.3235483765602112, 0.19140365719795227, 0.19140365719795227, 0.19140365719795227, 0.07907960563898087, 0.07907960563898087, 0.07907960563898087, 0.237238809466362, 0.07907960563898087, 0.07907960563898087, 0.15815921127796173, 0.07907960563898087, 0.07907960563898087, 0.15815921127796173, 0.13570280373096466, 0.13570280373096466, 0.2714056074619293, 0.13570280373096466, 0.13570280373096466, 0.13570280373096466, 0.13570280373096466, 0.32303109765052795, 0.32224157452583313, 0.32224157452583313, 0.19132450222969055, 0.19132450222969055, 0.19132450222969055, 0.19132450222969055, 0.19132450222969055, 0.3236863315105438, 0.3235287368297577, 0.32314029335975647, 0.27780136466026306, 0.27780136466026306, 0.2660820484161377, 0.2660820484161377, 0.3230505883693695, 0.440586656332016, 0.17923565208911896, 0.17923565208911896, 0.17923565208911896, 0.17923565208911896, 0.32356539368629456, 0.3242950439453125, 0.4393925666809082, 0.32375046610832214, 0.32375046610832214, 0.2664569318294525, 0.19089268147945404, 0.19089268147945404, 0.19089268147945404, 0.26681649684906006, 0.26681649684906006, 0.22682534158229828, 0.22682534158229828, 0.10692393779754639, 0.10692393779754639, 0.10692393779754639, 0.10692393779754639, 0.10692393779754639, 0.10692393779754639, 0.10692393779754639, 0.10692393779754639, 0.21384787559509277, 0.32287952303886414, 0.32287952303886414, 0.4399333596229553, 0.12425564974546432, 0.1338137835264206, 0.0669068917632103, 0.07646501809358597, 0.09080220758914948, 0.12425564974546432, 0.15770909190177917, 0.0812440812587738, 0.10036033391952515, 0.05256969854235649, 0.3233657479286194, 0.16986510157585144, 0.16986510157585144, 0.16986510157585144, 0.16986510157585144, 0.16986510157585144, 0.2668790817260742, 0.3238859176635742, 0.14156128466129303, 0.14156128466129303, 0.14156128466129303, 0.14156128466129303, 0.14156128466129303, 0.14156128466129303, 0.14156128466129303, 0.14156128466129303, 0.13570216298103333, 0.13570216298103333, 0.13570216298103333, 0.27140432596206665, 0.13570216298103333, 0.13570216298103333, 0.323233038187027, 0.3236023187637329, 0.3236023187637329, 0.13866393268108368, 0.13866393268108368, 0.27732786536216736, 0.13866393268108368, 0.13866393268108368, 0.13866393268108368, 0.13866393268108368, 0.13866393268108368, 0.13866393268108368, 0.13866393268108368, 0.3231935203075409, 0.3231935203075409, 0.32401254773139954, 0.32401254773139954, 0.3241210877895355, 0.3238266408443451, 0.074028879404068, 0.11844620853662491, 0.09376991540193558, 0.11721239238977432, 0.11351095139980316, 0.10240661352872849, 0.1295505315065384, 0.0913022831082344, 0.07032743841409683, 0.08883465826511383, 0.32326745986938477, 0.32384034991264343, 0.3238253593444824, 0.22655795514583588, 0.22655795514583588, 0.3236214816570282, 0.19140152633190155, 0.19140152633190155, 0.19140152633190155, 0.19140152633190155, 0.32403451204299927, 0.1247476264834404, 0.1247476264834404, 0.1247476264834404, 0.2494952529668808, 0.1247476264834404, 0.1247476264834404, 0.0841243639588356, 0.0841243639588356, 0.09814509004354477, 0.07010363787412643, 0.07010363787412643, 0.11216582357883453, 0.11216582357883453, 0.0841243639588356, 0.1682487279176712, 0.11216582357883453, 0.32348135113716125, 0.22689341008663177, 0.22689341008663177, 0.15638428926467896, 0.15638428926467896, 0.3127685785293579, 0.15638428926467896, 0.15638428926467896, 0.15638428926467896, 0.16830798983573914, 0.07480354607105255, 0.12882833182811737, 0.06856992095708847, 0.10181593894958496, 0.09973806142807007, 0.07480354607105255, 0.0914265587925911, 0.11636107414960861, 0.07480354607105255, 0.09063802659511566, 0.12136278301477432, 0.12673960626125336, 0.09063802659511566, 0.07143505662679672, 0.11060911417007446, 0.10369604825973511, 0.10292793065309525, 0.10062357038259506, 0.08065248280763626, 0.0817665085196495, 0.11195721477270126, 0.06792909651994705, 0.1295684576034546, 0.10189364850521088, 0.09434596449136734, 0.12579461932182312, 0.09434596449136734, 0.09183007478713989, 0.10189364850521088, 0.32395967841148376, 0.1018693819642067, 0.12733672559261322, 0.10531091690063477, 0.10049276798963547, 0.1266484260559082, 0.09980446100234985, 0.0681423619389534, 0.08947986364364624, 0.09980446100234985, 0.08122018724679947, 0.08730058372020721, 0.08730058372020721, 0.08730058372020721, 0.08730058372020721, 0.17460116744041443, 0.08730058372020721, 0.08730058372020721, 0.11640077084302902, 0.08730058372020721, 0.08730058372020721, 0.14932595193386078, 0.11237931996583939, 0.12315542250871658, 0.06773547828197479, 0.12777374684810638, 0.11237931996583939, 0.08466935157775879, 0.08774823695421219, 0.07081437110900879, 0.06311715394258499, 0.26690223813056946, 0.26690223813056946, 0.1301269829273224, 0.10801390558481216, 0.10801390558481216, 0.09525635838508606, 0.09270484745502472, 0.07654529064893723, 0.09525635838508606, 0.10461189597845078, 0.12077145278453827, 0.06974126398563385, 0.12916035950183868, 0.12815910577774048, 0.10813424736261368, 0.12415414303541183, 0.08410441875457764, 0.10312803834676743, 0.07709571719169617, 0.08610690385103226, 0.10312803834676743, 0.05807209759950638, 0.19105838239192963, 0.19105838239192963, 0.19105838239192963, 0.19105838239192963, 0.19105838239192963, 0.32405951619148254, 0.32405951619148254, 0.3231044411659241, 0.0760272666811943, 0.09123271703720093, 0.09630120545625687, 0.15712301433086395, 0.08616423606872559, 0.1368490755558014, 0.08616423606872559, 0.08616423606872559, 0.08616423606872559, 0.10136968642473221, 0.09642698615789413, 0.1090044155716896, 0.1285693198442459, 0.09363199770450592, 0.10481194406747818, 0.06288716197013855, 0.10061946511268616, 0.0712721198797226, 0.10481194406747818, 0.1285693198442459, 0.13583002984523773, 0.13583002984523773, 0.13583002984523773, 0.13583002984523773, 0.13583002984523773, 0.13583002984523773, 0.13583002984523773, 0.13583002984523773, 0.23030193150043488, 0.11515096575021744, 0.11515096575021744, 0.11515096575021744, 0.11515096575021744, 0.11515096575021744, 0.11515096575021744, 0.11515096575021744, 0.11515096575021744, 0.11515096575021744, 0.1812996119260788, 0.13185426592826843, 0.09889069944620132, 0.06592713296413422, 0.08240891993045807, 0.08240891993045807, 0.11537248641252518, 0.09889069944620132, 0.06592713296413422, 0.08240891993045807, 0.43968912959098816, 0.32422566413879395, 0.32422566413879395, 0.32422566413879395, 0.2665739059448242, 0.2665739059448242, 0.0806315541267395, 0.14334498345851898, 0.11646779626607895, 0.08959061652421951, 0.08287131786346436, 0.11198826879262924, 0.08511108160018921, 0.10974850505590439, 0.06271342933177948, 0.11646779626607895, 0.3233054578304291, 0.3233054578304291, 0.07832900434732437, 0.09949900954961777, 0.07832900434732437, 0.15030701458454132, 0.10373301059007645, 0.07832900434732437, 0.1122010126709938, 0.12913700938224792, 0.10373301059007645, 0.06774400919675827, 0.32408449053764343, 0.14437779784202576, 0.06574346125125885, 0.11086152493953705, 0.11859604716300964, 0.06961072236299515, 0.10312699526548386, 0.08507977426052094, 0.10441608726978302, 0.11086152493953705, 0.08894703537225723, 0.3232453465461731, 0.07595603168010712, 0.11063160747289658, 0.08916577696800232, 0.12879501283168793, 0.12219014018774033, 0.05944385007023811, 0.08751455694437027, 0.11558526754379272, 0.13044622540473938, 0.08256090432405472, 0.19135303795337677, 0.19135303795337677, 0.19135303795337677, 0.19135303795337677, 0.19135303795337677, 0.08229417353868484, 0.0891520231962204, 0.11658341437578201, 0.11429746448993683, 0.09372392296791077, 0.07772227376699448, 0.14172886312007904, 0.07543633133172989, 0.10515367239713669, 0.10058177262544632, 0.09527333825826645, 0.13293954730033875, 0.10302814841270447, 0.08973418921232224, 0.10856729745864868, 0.13072387874126434, 0.10524380207061768, 0.0786558985710144, 0.08419504761695862, 0.07200892269611359, 0.07224030047655106, 0.10062041878700256, 0.15222063660621643, 0.10406043380498886, 0.07052028924226761, 0.09976041316986084, 0.13072054088115692, 0.08170033991336823, 0.09718040376901627, 0.0911603793501854, 0.13360083103179932, 0.07634333521127701, 0.09542916715145111, 0.09224819391965866, 0.08588624745607376, 0.12087694555521011, 0.10815306007862091, 0.14314375817775726, 0.06998138874769211, 0.07634333521127701, 0.13643744587898254, 0.11973081529140472, 0.08353313058614731, 0.08910200744867325, 0.07239537686109543, 0.08353313058614731, 0.15314407646656036, 0.10023975372314453, 0.08910200744867325, 0.07239537686109543, 0.32355228066444397, 0.13463903963565826, 0.1370011419057846, 0.10393189638853073, 0.13109591603279114, 0.0779489204287529, 0.06495743244886398, 0.08385414630174637, 0.10629398375749588, 0.09212145209312439, 0.0685005709528923, 0.0873064175248146, 0.0436532087624073, 0.21826604008674622, 0.0873064175248146, 0.0873064175248146, 0.1309596300125122, 0.0873064175248146, 0.1309596300125122, 0.0873064175248146, 0.0873064175248146, 0.12994611263275146, 0.08489812910556793, 0.1316787302494049, 0.13860920071601868, 0.09356120228767395, 0.09009597450494766, 0.09356120228767395, 0.07450243830680847, 0.08489812910556793, 0.0797002837061882, 0.1526143103837967, 0.1526143103837967, 0.1526143103837967, 0.1526143103837967, 0.1526143103837967, 0.1526143103837967, 0.1526143103837967, 0.1526143103837967, 0.1526143103837967, 0.3237857520580292, 0.0536603145301342, 0.09222866594791412, 0.1358276754617691, 0.10899751633405685, 0.0821673572063446, 0.08552112430334091, 0.1542734056711197, 0.0788135901093483, 0.10564374178647995, 0.10564374178647995, 0.10407334566116333, 0.07426205277442932, 0.12057602405548096, 0.07772229611873627, 0.07426205277442932, 0.12616564333438873, 0.08996621519327164, 0.11019530892372131, 0.13681252300739288, 0.08597363531589508, 0.10576409846544266, 0.08628123998641968, 0.11133062839508057, 0.14751309156417847, 0.10576409846544266, 0.07514817267656326, 0.08906450122594833, 0.08628123998641968, 0.10019756853580475, 0.09184776991605759, 0.11342984437942505, 0.08006812632083893, 0.17348094284534454, 0.08006812632083893, 0.10008515417575836, 0.09341280907392502, 0.09341280907392502, 0.10008515417575836, 0.060051094740629196, 0.1067574992775917, 0.08197508007287979, 0.12396232038736343, 0.09197204560041428, 0.10596778988838196, 0.0779763013124466, 0.13595867156982422, 0.0719781219959259, 0.09797022491693497, 0.11596475541591644, 0.09597083181142807, 0.11913750320672989, 0.1654687523841858, 0.10590000450611115, 0.12575626373291016, 0.0926625058054924, 0.07280625402927399, 0.07942500710487366, 0.0926625058054924, 0.07942500710487366, 0.06618750095367432, 0.15970829129219055, 0.10561355203390121, 0.12106919288635254, 0.07727820426225662, 0.07212632894515991, 0.06439850479364395, 0.08758196979761124, 0.07470227032899857, 0.11591731011867523, 0.12106919288635254, 0.3233316242694855, 0.3233316242694855, 0.13890336453914642, 0.13890336453914642, 0.13890336453914642, 0.13890336453914642, 0.13890336453914642, 0.13890336453914642, 0.13890336453914642, 0.27780672907829285, 0.13890336453914642, 0.13890336453914642, 0.2666885256767273, 0.2666885256767273, 0.10530777275562286, 0.07898082584142685, 0.10530777275562286, 0.07898082584142685, 0.10530777275562286, 0.10530777275562286, 0.18428859114646912, 0.07898082584142685, 0.07898082584142685, 0.07898082584142685, 0.14378491044044495, 0.074693463742733, 0.09990251064300537, 0.1017698422074318, 0.15218792855739594, 0.06722411513328552, 0.10363718122243881, 0.0653567835688591, 0.09803517162799835, 0.09243316203355789, 0.09539936482906342, 0.11447923630475998, 0.09539936482906342, 0.09062939882278442, 0.09062939882278442, 0.1574089527130127, 0.1049392968416214, 0.08108945935964584, 0.09539936482906342, 0.08108945935964584, 0.08638640493154526, 0.09461367875337601, 0.1110682338476181, 0.11312504857778549, 0.10695459693670273, 0.10832580924034119, 0.0850151926279068, 0.09529928863048553, 0.0898144319653511, 0.10832580924034119, 0.1166360005736351, 0.10179141908884048, 0.10179141908884048, 0.08482617884874344, 0.09330879896879196, 0.10391207039356232, 0.08058487623929977, 0.13996319472789764, 0.10391207039356232, 0.07422290742397308, 0.12496546655893326, 0.10323234647512436, 0.08149921894073486, 0.10866562277078629, 0.10323234647512436, 0.1439819484949112, 0.09236577898263931, 0.07063265889883041, 0.09779906272888184, 0.07063265889883041, 0.09028027951717377, 0.11965718865394592, 0.1339873969554901, 0.07236751914024353, 0.13255436718463898, 0.07595007121562958, 0.11607464402914047, 0.09099678695201874, 0.09529584646224976, 0.07380054146051407, 0.09513876587152481, 0.11467619240283966, 0.12486962974071503, 0.10703111439943314, 0.10108494013547897, 0.07560133934020996, 0.08579478412866592, 0.09004204720258713, 0.08834314346313477, 0.1172245517373085, 0.26653242111206055, 0.26653242111206055, 0.11988455057144165, 0.09346931427717209, 0.13410814106464386, 0.10159707814455032, 0.09956513345241547, 0.10159707814455032, 0.13817203044891357, 0.07721377909183502, 0.06705407053232193, 0.06705407053232193, 0.10679201036691666, 0.08192264288663864, 0.10094039887189865, 0.12873557209968567, 0.09508877992630005, 0.13605010509490967, 0.06436778604984283, 0.09362587332725525, 0.10825491696596146, 0.08192264288663864, 0.19758999347686768, 0.19758999347686768, 0.19758999347686768, 0.10027951747179031, 0.10266713052988052, 0.0990857183933258, 0.10266713052988052, 0.11102375388145447, 0.08117865771055222, 0.08953528851270676, 0.1181865781545639, 0.11938038468360901, 0.07520963996648788, 0.3237632215023041, 0.3237632215023041, 0.13653838634490967, 0.1284635365009308, 0.09763228893280029, 0.10203675180673599, 0.09763228893280029, 0.07928035408258438, 0.08368481695652008, 0.09910044074058533, 0.10497306287288666, 0.0712055042386055, 0.07311698794364929, 0.11403567343950272, 0.08854534476995468, 0.08384975790977478, 0.11269407719373703, 0.11470647156238556, 0.11269407719373703, 0.10263210535049438, 0.08452055603265762, 0.11135248094797134, 0.26686838269233704, 0.26686838269233704, 0.10525563359260559, 0.09227891266345978, 0.07641847431659698, 0.10669749230146408, 0.11534864455461502, 0.08362776786088943, 0.09372077137231827, 0.12976722419261932, 0.09516263008117676, 0.10093006491661072, 0.11089752614498138, 0.11205270886421204, 0.0912594199180603, 0.13053563237190247, 0.12360452860593796, 0.09010423719882965, 0.10050088167190552, 0.07046613097190857, 0.08317314088344574, 0.0866386890411377, 0.10169793665409088, 0.09099289029836655, 0.07761158049106598, 0.10972671955823898, 0.08296409994363785, 0.11507923901081085, 0.09099289029836655, 0.08831662684679031, 0.09634540975093842, 0.14719437062740326, 0.6198490858078003, 0.09025989472866058, 0.07469784468412399, 0.1400584578514099, 0.09492851048707962, 0.09337230771780014, 0.11515917629003525, 0.10115333646535873, 0.0964847207069397, 0.09959712624549866, 0.09492851048707962, 0.11686687916517258, 0.06303728371858597, 0.0892437994480133, 0.07436983287334442, 0.11545031517744064, 0.09491007030010223, 0.133157417178154, 0.11474202573299408, 0.09207693487405777, 0.10553433746099472, 0.0959380492568016, 0.0861484482884407, 0.0763588547706604, 0.10181180387735367, 0.10376972705125809, 0.0763588547706604, 0.10376972705125809, 0.14488603174686432, 0.12726475298404694, 0.08419053256511688, 0.10261359810829163, 0.11800563335418701, 0.12057097256183624, 0.08209087699651718, 0.14109370112419128, 0.07952553778886795, 0.07952553778886795, 0.12057097256183624, 0.09748291969299316, 0.059002816677093506, 0.07032989710569382, 0.10549484193325043, 0.10549484193325043, 0.07032989710569382, 0.07032989710569382, 0.07032989710569382, 0.17582474648952484, 0.10549484193325043, 0.07032989710569382, 0.10549484193325043, 0.11694132536649704, 0.11604177951812744, 0.13493229448795319, 0.08095937967300415, 0.10164900124073029, 0.07916028052568436, 0.12143906950950623, 0.07916028052568436, 0.08095937967300415, 0.08905532211065292, 0.05645742639899254, 0.10447868704795837, 0.1018829420208931, 0.12589357793331146, 0.10253188014030457, 0.10902123898267746, 0.09928719699382782, 0.08176592737436295, 0.1401701718568802, 0.07787231355905533, 0.32390326261520386, 0.09447086602449417, 0.09447086602449417, 0.08057808876037598, 0.07641025632619858, 0.08752447366714478, 0.10280653089284897, 0.13614918291568756, 0.12225641310214996, 0.09030303359031677, 0.11392074823379517, 0.11008311063051224, 0.06869526952505112, 0.15659108757972717, 0.08234899491071701, 0.0697619691491127, 0.11925669759511948, 0.09877612441778183, 0.12139009684324265, 0.09792276471853256, 0.07509545236825943, 0.061449941247701645, 0.10424543917179108, 0.11851060390472412, 0.07900706678628922, 0.10644008219242096, 0.11631596088409424, 0.08559099584817886, 0.09327223151922226, 0.1152186468243599, 0.12180256843566895, 0.32357510924339294, 0.32357510924339294, 0.11275506019592285, 0.10408159345388412, 0.07998862862586975, 0.09155325591564178, 0.10311787575483322, 0.10408159345388412, 0.11468249559402466, 0.11950108408927917, 0.07035144418478012, 0.10022671520709991, 0.10911840945482254, 0.09946848452091217, 0.11208761483430862, 0.0935300663113594, 0.11431452631950378, 0.09575697034597397, 0.09501466900110245, 0.11208761483430862, 0.0861070454120636, 0.08165322989225388, 0.09053616225719452, 0.10136113315820694, 0.07675891369581223, 0.09545660018920898, 0.12793153524398804, 0.1072656661272049, 0.13088379800319672, 0.07675891369581223, 0.09644068777561188, 0.09644068777561188, 0.191768616437912, 0.191768616437912, 0.191768616437912, 0.12955769896507263, 0.12955769896507263, 0.12955769896507263, 0.12955769896507263, 0.12955769896507263, 0.25911539793014526, 0.12955769896507263, 0.12955769896507263, 0.12955769896507263, 0.07519365847110748, 0.14107763767242432, 0.09846789389848709, 0.09238078445196152, 0.08271303027868271, 0.1110001653432846, 0.06982268393039703, 0.15074539184570312, 0.11422275006771088, 0.06445170938968658, 0.1295698881149292, 0.2591397762298584, 0.1295698881149292, 0.1295698881149292, 0.1295698881149292, 0.1295698881149292, 0.1295698881149292, 0.07158316671848297, 0.07874149084091187, 0.0971485897898674, 0.10737475752830505, 0.0971485897898674, 0.08487718552350998, 0.10123905539512634, 0.11248783767223358, 0.1595282107591629, 0.09101288765668869, 0.07184122502803802, 0.11602526903152466, 0.1163625568151474, 0.11062874644994736, 0.0971374362707138, 0.09781200438737869, 0.08465797454118729, 0.11062874644994736, 0.10826776921749115, 0.08668167144060135, 0.3232783377170563, 0.3232783377170563, 0.11069121211767197, 0.09324797242879868, 0.09145474433898926, 0.11607090383768082, 0.12438496947288513, 0.09031359851360321, 0.09373703598976135, 0.10221412032842636, 0.08102140575647354, 0.09683443605899811, 0.10372930020093918, 0.08821837604045868, 0.11221183091402054, 0.09863977879285812, 0.09403496980667114, 0.1146354153752327, 0.09064196050167084, 0.07997819781303406, 0.10154807567596436, 0.11608956754207611, 0.13899235427379608, 0.27798470854759216, 0.13899235427379608, 0.13899235427379608, 0.13899235427379608, 0.13899235427379608, 0.13899235427379608, 0.13899235427379608, 0.13899235427379608, 0.10408954322338104, 0.08572079986333847, 0.07959788292646408, 0.10408954322338104, 0.10408954322338104, 0.07347497344017029, 0.15919576585292816, 0.07959788292646408, 0.1285811960697174, 0.09184371680021286, 0.09529620409011841, 0.19059240818023682, 0.09529620409011841, 0.09529620409011841, 0.09529620409011841, 0.09529620409011841, 0.09529620409011841, 0.09529620409011841, 0.09529620409011841, 0.09529620409011841, 0.10062422603368759, 0.10856824368238449, 0.1271042823791504, 0.1429923176765442, 0.0635521411895752, 0.055608123540878296, 0.0714961588382721, 0.16152836382389069, 0.079440176486969, 0.0873841941356659, 0.2789284288883209, 0.2789284288883209, 0.2789284288883209, 0.12240767478942871, 0.09109408408403397, 0.08540070801973343, 0.11956098675727844, 0.09963415563106537, 0.06832056492567062, 0.09963415563106537, 0.08255401253700256, 0.08255401253700256, 0.15087458491325378, 0.16995787620544434, 0.16995787620544434, 0.16995787620544434, 0.16995787620544434, 0.16995787620544434, 0.08653455227613449, 0.09162481874227524, 0.08653455227613449, 0.1425274908542633, 0.1628885716199875, 0.09671508520841599, 0.10180535167455673, 0.07126374542713165, 0.07126374542713165, 0.09671508520841599, 0.1285468190908432, 0.089982770383358, 0.1092647910118103, 0.09319643676280975, 0.08034175634384155, 0.089982770383358, 0.06748707592487335, 0.08676909655332565, 0.14461515843868256, 0.1092647910118103, 0.06625159084796906, 0.08923684060573578, 0.08923684060573578, 0.15143221616744995, 0.12168660014867783, 0.08247647434473038, 0.09734927862882614, 0.11627830564975739, 0.08788476884365082, 0.09734927862882614, 0.09166084975004196, 0.11404315382242203, 0.1321621537208557, 0.10018744319677353, 0.10445073992013931, 0.09379249811172485, 0.0927266776561737, 0.08206843584775925, 0.09912162274122238, 0.08952920138835907, 0.14993776381015778, 0.10471843928098679, 0.07615886628627777, 0.08805868774652481, 0.09519858658313751, 0.11899822950363159, 0.11899822950363159, 0.09519858658313751, 0.0737789049744606, 0.0809187963604927, 0.07410687208175659, 0.12762850522994995, 0.08851654082536697, 0.1214529275894165, 0.13997964560985565, 0.11939440667629242, 0.09057506173849106, 0.07616539299488068, 0.09057506173849106, 0.06998982280492783, 0.32332131266593933, 0.32332131266593933, 0.15296095609664917, 0.15296095609664917, 0.15296095609664917, 0.15296095609664917, 0.15296095609664917, 0.15296095609664917, 0.15296095609664917, 0.32355064153671265, 0.3237055540084839, 0.3232840895652771, 0.32446905970573425, 0.32308706641197205, 0.27810952067375183, 0.27810952067375183, 0.27810952067375183, 0.4397171139717102, 0.17465227842330933, 0.17465227842330933, 0.17465227842330933, 0.17465227842330933, 0.17465227842330933, 0.12446854263544083, 0.1444724202156067, 0.11335527896881104, 0.10224201530218124, 0.08223814517259598, 0.0844607949256897, 0.08890610188245773, 0.08890610188245773, 0.09112875163555145, 0.08001549541950226, 0.09788800030946732, 0.1598171442747116, 0.09788800030946732, 0.11386971175670624, 0.08989714086055756, 0.08590171486139297, 0.09389257431030273, 0.09189485758543015, 0.0998857170343399, 0.06992000341415405, 0.0840311348438263, 0.11960635334253311, 0.1073390394449234, 0.10795240104198456, 0.09445834904909134, 0.08157766610383987, 0.15579494833946228, 0.08219103515148163, 0.08035093545913696, 0.08587122708559036, 0.09799759089946747, 0.10779734700918198, 0.08329795300960541, 0.10779734700918198, 0.07839807122945786, 0.16169601678848267, 0.0734981894493103, 0.09799759089946747, 0.12249698489904404, 0.06859831511974335, 0.09500554949045181, 0.07600443810224533, 0.10260599851608276, 0.1140066608786583, 0.08740510791540146, 0.10640621930360794, 0.09500554949045181, 0.08740510791540146, 0.14820866286754608, 0.09120532870292664, 0.10074605792760849, 0.11194006353616714, 0.09794756025075912, 0.09514905512332916, 0.08955205231904984, 0.08395504951477051, 0.08675355464220047, 0.10914156585931778, 0.13712657988071442, 0.08955205231904984, 0.3234257996082306, 0.1034492775797844, 0.09326942265033722, 0.09079324454069138, 0.11005242168903351, 0.12931159138679504, 0.09024298191070557, 0.10510005801916122, 0.09904717653989792, 0.06685684621334076, 0.11225346475839615, 0.07494683563709259, 0.10901357978582382, 0.10560690611600876, 0.10901357978582382, 0.09368354827165604, 0.11071691662073135, 0.1294536292552948, 0.09538688510656357, 0.09368354827165604, 0.07835350930690765, 0.09408105909824371, 0.09922611713409424, 0.11098624765872955, 0.11980634927749634, 0.09481606632471085, 0.11392627656459808, 0.123481385409832, 0.07570584863424301, 0.08085090667009354, 0.0867309719324112, 0.3233293890953064, 0.3233293890953064, 0.0742565169930458, 0.10692938417196274, 0.11881043016910553, 0.12772122025489807, 0.08910782635211945, 0.0742565169930458, 0.1544535607099533, 0.10395912826061249, 0.0742565169930458, 0.0742565169930458, 0.09762979298830032, 0.10088412463665009, 0.10413844883441925, 0.10956232249736786, 0.10522322356700897, 0.12583395838737488, 0.1117318794131279, 0.09871456772089005, 0.0607474260032177, 0.08569726347923279, 0.08548761904239655, 0.12155270576477051, 0.07747315615415573, 0.13758163154125214, 0.1162097305059433, 0.06678720563650131, 0.10953101515769958, 0.10552377998828888, 0.10151654481887817, 0.07880889624357224, 0.08380095660686493, 0.11522631347179413, 0.10003738850355148, 0.10160865634679794, 0.11365504562854767, 0.0874672457575798, 0.12884396314620972, 0.07751588523387909, 0.10789372771978378, 0.08380095660686493, 0.1156507283449173, 0.09872623533010483, 0.1542009711265564, 0.08744323253631592, 0.08368223160505295, 0.11094947904348373, 0.0893237367272377, 0.12317273020744324, 0.05923574045300484, 0.0771004855632782, 0.12024525552988052, 0.11138998717069626, 0.10020438581705093, 0.13748973608016968, 0.07363856583833694, 0.07363856583833694, 0.10300078243017197, 0.09134911000728607, 0.10486505180597305, 0.08482417464256287, 0.3230772018432617, 0.6204177737236023, 0.12259364128112793, 0.1259523630142212, 0.11083808541297913, 0.1494634747505188, 0.1091587170958519, 0.068853959441185, 0.08732697367668152, 0.07221268862485886, 0.07893015444278717, 0.07725078612565994, 0.0874965712428093, 0.0874965712428093, 0.0874965712428093, 0.0874965712428093, 0.0874965712428093, 0.1749931424856186, 0.0874965712428093, 0.04374828562140465, 0.13124485313892365, 0.0874965712428093, 0.15259598195552826, 0.3051919639110565, 0.15259598195552826, 0.15259598195552826, 0.15259598195552826, 0.15259598195552826, 0.10974416136741638, 0.14022864401340485, 0.07925967127084732, 0.07925967127084732, 0.1707131415605545, 0.07925967127084732, 0.09755036234855652, 0.06706587970256805, 0.06706587970256805, 0.10974416136741638, 0.09113863110542297, 0.1165190115571022, 0.08652401715517044, 0.08421671390533447, 0.10382882505655289, 0.1234409287571907, 0.09344594180583954, 0.0957532450556755, 0.09690690040588379, 0.10844343900680542, 0.13175930082798004, 0.08096052706241608, 0.10001006722450256, 0.10318499058485031, 0.10159752517938614, 0.1285843700170517, 0.06349845230579376, 0.13017183542251587, 0.073023222386837, 0.085722915828228, 0.2669828236103058, 0.13456331193447113, 0.09185758978128433, 0.12086524814367294, 0.09669219702482224, 0.10233257710933685, 0.0950806587934494, 0.08702298253774643, 0.10313834249973297, 0.07413068413734436, 0.09427489340305328, 0.17407748103141785, 0.17407748103141785, 0.17407748103141785, 0.3481549620628357, 0.10248973220586777, 0.10347521305084229, 0.11727190762758255, 0.1359959989786148, 0.06109964847564697, 0.09460590779781342, 0.10248973220586777, 0.07489634305238724, 0.12909764051437378, 0.07982373237609863, 0.16977179050445557, 0.16977179050445557, 0.16977179050445557, 0.16977179050445557, 0.16977179050445557, 0.3238495886325836, 0.3240717947483063, 0.3240717947483063, 0.22651539742946625, 0.1528746336698532, 0.1528746336698532, 0.1528746336698532, 0.1528746336698532, 0.1528746336698532, 0.1528746336698532, 0.1637113094329834, 0.08147843927145004, 0.11542779207229614, 0.08826830983161926, 0.11241006851196289, 0.08977717161178589, 0.09807589650154114, 0.09430374950170517, 0.07846072316169739, 0.07770629227161407, 0.10463663190603256, 0.06881508231163025, 0.10369396209716797, 0.09709525108337402, 0.10086594521999359, 0.12160473316907883, 0.08295516669750214, 0.0999232679605484, 0.09049654752016068, 0.13008879125118256, 0.11422576010227203, 0.11826404929161072, 0.113071970641613, 0.1124950721859932, 0.11653335392475128, 0.07788120210170746, 0.07845810055732727, 0.10557229816913605, 0.08711156994104385, 0.07672740519046783, 0.10945239663124084, 0.09639844298362732, 0.1365644633769989, 0.1044316440820694, 0.10141919553279877, 0.11146070063114166, 0.08535278588533401, 0.1255188137292862, 0.0733029842376709, 0.05522827431559563, 0.22676371037960052, 0.22676371037960052, 0.22676371037960052, 0.1683250069618225, 0.12488629668951035, 0.08144758641719818, 0.09773710370063782, 0.07601774483919144, 0.08687742054462433, 0.09773710370063782, 0.07058790326118469, 0.12488629668951035, 0.07601774483919144, 0.1525556892156601, 0.1525556892156601, 0.3051113784313202, 0.1525556892156601, 0.32335102558135986, 0.11170803010463715, 0.09273874014616013, 0.15175430476665497, 0.10960033535957336, 0.07166175544261932, 0.08220025151968002, 0.063230961561203, 0.09273874014616013, 0.08009254932403564, 0.14121581614017487, 0.09502885490655899, 0.1472015678882599, 0.10620872676372528, 0.07453243434429169, 0.09130223840475082, 0.09689217060804367, 0.11366196721792221, 0.0838489904999733, 0.1043454110622406, 0.08757561445236206, 0.3236609697341919, 0.14978383481502533, 0.08385464549064636, 0.07717057317495346, 0.10238773375749588, 0.09023488312959671, 0.07474000751972198, 0.08689285069704056, 0.10572976619005203, 0.12942782044410706, 0.09995716065168381, 0.19750937819480896, 0.19750937819480896, 0.19750937819480896, 0.19750937819480896, 0.14536672830581665, 0.08945644646883011, 0.08200173825025558, 0.08945644646883011, 0.08572909235954285, 0.10809320211410522, 0.08945644646883011, 0.11927525699138641, 0.08200173825025558, 0.10622952878475189, 0.22702457010746002, 0.22702457010746002, 0.22702457010746002, 0.3234569728374481, 0.32270804047584534, 0.32270804047584534, 0.10793816298246384, 0.142659530043602, 0.12076997011899948, 0.10793816298246384, 0.0966159775853157, 0.08227454125881195, 0.09133229404687881, 0.0747264176607132, 0.08378417044878006, 0.09208710491657257, 0.1017385795712471, 0.09797048568725586, 0.1563759744167328, 0.1017385795712471, 0.1017385795712471, 0.07536191493272781, 0.13188335299491882, 0.07913000881671906, 0.06782571971416473, 0.08666619658470154, 0.22685794532299042, 0.22685794532299042, 0.09578082710504532, 0.09258813410997391, 0.16282740235328674, 0.10216621309518814, 0.06704657524824142, 0.09578082710504532, 0.1181296855211258, 0.09578082710504532, 0.09578082710504532, 0.07662466168403625, 0.2666909396648407, 0.3235909342765808, 0.1114179939031601, 0.12403879314661026, 0.14375878870487213, 0.09761399775743484, 0.08282399922609329, 0.10885439813137054, 0.08164079487323761, 0.09268399327993393, 0.07434439659118652, 0.08242959529161453, 0.32345137000083923, 0.32345137000083923, 0.08838079869747162, 0.0522250160574913, 0.17676159739494324, 0.1124846488237381, 0.08436349034309387, 0.11650195717811584, 0.1044500321149826, 0.08436349034309387, 0.08838079869747162, 0.09239810705184937, 0.08032860606908798, 0.10041075199842453, 0.14559559524059296, 0.12802371382713318, 0.06526698917150497, 0.13806478679180145, 0.0552259124815464, 0.07530806213617325, 0.1029210239648819, 0.1104518249630928, 0.0751035138964653, 0.0751035138964653, 0.0751035138964653, 0.0751035138964653, 0.0751035138964653, 0.0751035138964653, 0.0751035138964653, 0.1502070277929306, 0.0751035138964653, 0.2253105342388153, 0.43965113162994385, 0.10408084839582443, 0.18923790752887726, 0.10408084839582443, 0.05677137151360512, 0.07569516450166702, 0.06623326241970062, 0.13246652483940125, 0.09461895376443863, 0.09461895376443863, 0.07569516450166702, 0.22676417231559753, 0.22676417231559753, 0.22676417231559753, 0.3241598308086395, 0.3241598308086395, 0.09246999770402908, 0.12229903042316437, 0.10738451778888702, 0.07457257807254791, 0.08053838461637497, 0.08948709815740585, 0.1044016107916832, 0.08948709815740585, 0.0835212916135788, 0.15511097013950348, 0.07720235735177994, 0.08990653604269028, 0.11629215627908707, 0.12704184651374817, 0.1016334816813469, 0.10651970654726028, 0.12411011755466461, 0.08795204758644104, 0.08502031117677689, 0.08306582272052765, 0.10999738425016403, 0.11945952475070953, 0.10704046487808228, 0.12892165780067444, 0.14429764449596405, 0.08515926450490952, 0.06623498350381851, 0.10112662613391876, 0.08279372751712799, 0.054998692125082016, 0.11139854043722153, 0.10324742645025253, 0.10053038597106934, 0.08694519847631454, 0.10053038597106934, 0.12770076096057892, 0.08422816544771194, 0.1521541029214859, 0.06249186396598816, 0.07064297795295715, 0.10600844025611877, 0.16617539525032043, 0.0945480689406395, 0.08881788700819016, 0.10887353122234344, 0.04870657995343208, 0.08881788700819016, 0.08595278859138489, 0.12319900095462799, 0.08881788700819016, 0.09419067203998566, 0.12816108763217926, 0.05558793991804123, 0.13896985352039337, 0.12352875620126724, 0.08492601662874222, 0.08801423758268356, 0.10808765888214111, 0.07720547169446945, 0.09882300347089767, 0.1067587211728096, 0.14389218389987946, 0.14621303975582123, 0.08355030417442322, 0.08355030417442322, 0.07890862226486206, 0.1067587211728096, 0.06730441004037857, 0.11372124403715134, 0.06962525099515915, 0.12089967727661133, 0.08272083103656769, 0.0986286848783493, 0.07953926175832748, 0.0986286848783493, 0.11453653872013092, 0.08908397704362869, 0.07953926175832748, 0.14635224640369415, 0.08272083103656769, 0.09391194581985474, 0.08804245293140411, 0.09978144615888596, 0.07630345970392227, 0.12325943261384964, 0.07043395936489105, 0.10565093904733658, 0.09391194581985474, 0.09391194581985474, 0.15847641229629517, 0.10868574678897858, 0.1444280445575714, 0.14588691294193268, 0.08461440354585648, 0.09701479226350784, 0.06637854129076004, 0.07950836420059204, 0.08461440354585648, 0.06419023871421814, 0.12473330646753311, 0.19131778180599213, 0.19131778180599213, 0.19131778180599213, 0.19131778180599213, 0.32367897033691406, 0.3234841227531433, 0.07736704498529434, 0.11759791523218155, 0.09036470949649811, 0.07117768377065659, 0.08850790560245514, 0.1392606794834137, 0.09222152084112167, 0.12997664511203766, 0.09779194742441177, 0.09531620144844055, 0.066310815513134, 0.132621631026268, 0.066310815513134, 0.1989324539899826, 0.132621631026268, 0.066310815513134, 0.066310815513134, 0.066310815513134, 0.066310815513134, 0.066310815513134, 0.14158861339092255, 0.08042233437299728, 0.08042233437299728, 0.12346526980400085, 0.12233256548643112, 0.09401483833789825, 0.06456440687179565, 0.08155503869056702, 0.12799610197544098, 0.08495316654443741, 0.32379117608070374, 0.09562420845031738, 0.09846453368663788, 0.12970808148384094, 0.11455969512462616, 0.11171936988830566, 0.08899679034948349, 0.09657098352909088, 0.08899679034948349, 0.07479517161846161, 0.09941130876541138, 0.32350337505340576, 0.11045393347740173, 0.06976038217544556, 0.1453341245651245, 0.09495162963867188, 0.08526268601417542, 0.13370738923549652, 0.1027027815580368, 0.09688941389322281, 0.09301383793354034, 0.06976038217544556, 0.32413604855537415, 0.32413604855537415, 0.09278775006532669, 0.08911298960447311, 0.12034846842288971, 0.1359662115573883, 0.07349524646997452, 0.09738120436668396, 0.1221858486533165, 0.09278775006532669, 0.09738120436668396, 0.07808870077133179, 0.2268325686454773, 0.2268325686454773, 0.2268325686454773, 0.09478451311588287, 0.09332628548145294, 0.1443641036748886, 0.09624273329973221, 0.10207562893629074, 0.10936674475669861, 0.10061740130186081, 0.10936674475669861, 0.09186806529760361, 0.05832893028855324, 0.11487911641597748, 0.08271296322345734, 0.11487911641597748, 0.12866461277008057, 0.06433230638504028, 0.07811780273914337, 0.08271296322345734, 0.1608307659626007, 0.09649845957756042, 0.07352263480424881, 0.09435427188873291, 0.06475293636322021, 0.1276557892560959, 0.11840536445379257, 0.07770352065563202, 0.09435427188873291, 0.12950587272644043, 0.09250418841838837, 0.07770352065563202, 0.12210553139448166, 0.11263037472963333, 0.08447277545928955, 0.08897799253463745, 0.07884126156568527, 0.10587254911661148, 0.11826188862323761, 0.1002410277724266, 0.12276710569858551, 0.10361994057893753, 0.08559907972812653, 0.0822315514087677, 0.11747364699840546, 0.08810523897409439, 0.09985259920358658, 0.07048419117927551, 0.10572628676891327, 0.1527157425880432, 0.09397891908884048, 0.08810523897409439, 0.10572628676891327, 0.32355907559394836, 0.12041828036308289, 0.12714555859565735, 0.04709094762802124, 0.1089819073677063, 0.09754553437232971, 0.081400066614151, 0.0901455283164978, 0.08745461702346802, 0.11234554648399353, 0.12714555859565735, 0.0904107540845871, 0.11301344633102417, 0.11301344633102417, 0.11301344633102417, 0.06780806928873062, 0.0904107540845871, 0.11301344633102417, 0.0904107540845871, 0.1808215081691742, 0.06780806928873062, 0.1454695761203766, 0.09310053288936615, 0.09891930967569351, 0.09310053288936615, 0.10667768865823746, 0.1008589044213295, 0.09310053288936615, 0.07370458543300629, 0.08922134339809418, 0.10473809391260147, 0.1403292566537857, 0.09405045956373215, 0.1015147790312767, 0.09554331749677658, 0.11047196388244629, 0.07464321702718735, 0.09405045956373215, 0.11047196388244629, 0.05971457436680794, 0.11793628334999084, 0.08087655901908875, 0.12261930108070374, 0.0858045220375061, 0.08841343969106674, 0.10841517150402069, 0.1029074490070343, 0.1087050512433052, 0.09015272557735443, 0.09653008729219437, 0.11566217243671417, 0.32358890771865845, 0.10335283726453781, 0.08612736314535141, 0.06890188902616501, 0.051676418632268906, 0.08612736314535141, 0.10335283726453781, 0.06890188902616501, 0.18948020040988922, 0.10335283726453781, 0.10335283726453781, 0.3238266408443451, 0.3238266408443451, 0.3238266408443451, 0.3236820697784424, 0.6183310151100159, 0.11386047303676605, 0.08349768072366714, 0.08160000294446945, 0.13853023946285248, 0.07211162894964218, 0.08349768072366714, 0.11196279525756836, 0.11765582114458084, 0.08349768072366714, 0.11386047303676605, 0.13528579473495483, 0.11273816227912903, 0.08696943521499634, 0.09985379874706268, 0.1449490636587143, 0.0966327115893364, 0.08696943521499634, 0.07730616629123688, 0.0966327115893364, 0.06764289736747742, 0.3407263457775116, 0.09502998739480972, 0.11376829445362091, 0.09101463854312897, 0.1151067465543747, 0.10975294560194016, 0.08164548128843307, 0.08967618644237518, 0.09502998739480972, 0.13518351316452026, 0.07361478358507156, 0.26670345664024353, 0.26670345664024353, 0.19101986289024353, 0.19101986289024353, 0.19101986289024353, 0.19101986289024353, 0.4391932189464569, 0.09177577495574951, 0.10122328251600266, 0.10864631831645966, 0.12416722625494003, 0.09784916788339615, 0.07827933877706528, 0.10932113975286484, 0.09852398931980133, 0.10459738969802856, 0.08637719601392746, 0.09237362444400787, 0.0978073701262474, 0.10324110835790634, 0.09563387185335159, 0.1304098218679428, 0.09998086094856262, 0.08802662789821625, 0.09020012617111206, 0.1434508115053177, 0.058684419840574265, 0.08143270015716553, 0.0977192372083664, 0.0909331813454628, 0.0977192372083664, 0.11807741224765778, 0.13029232621192932, 0.10450530052185059, 0.10721971839666367, 0.09500481933355331, 0.08007548749446869, 0.266959547996521, 0.266959547996521, 0.09881745278835297, 0.15999016165733337, 0.1082286387681961, 0.08470067381858826, 0.09881745278835297, 0.11136570572853088, 0.0941118597984314, 0.08470067381858826, 0.0784265547990799, 0.08313214778900146, 0.10986955463886261, 0.1047593429684639, 0.08942870795726776, 0.09837158024311066, 0.08304094523191452, 0.06515520066022873, 0.14308594167232513, 0.10348179191350937, 0.11625732481479645, 0.0868736058473587, 0.08603335916996002, 0.12905004620552063, 0.12905004620552063, 0.11471115052700043, 0.057355575263500214, 0.08603335916996002, 0.08603335916996002, 0.17206671833992004, 0.08603335916996002, 0.057355575263500214, 0.2667854428291321, 0.3231380879878998, 0.3231380879878998, 0.0859529972076416, 0.1177874356508255, 0.14166326820850372, 0.09391160309314728, 0.10982882231473923, 0.08913643658161163, 0.054118551313877106, 0.10187021642923355, 0.1177874356508255, 0.08913643658161163, 0.1468273252248764, 0.10549814999103546, 0.10441054403781891, 0.0935344472527504, 0.08483356237411499, 0.0978848859667778, 0.1359512209892273, 0.08592117577791214, 0.08374595642089844, 0.060906149446964264, 0.09755672514438629, 0.08888501673936844, 0.12140391767024994, 0.0823812335729599, 0.08671708405017853, 0.11056428402662277, 0.08454915881156921, 0.09755672514438629, 0.1495869755744934, 0.08021330833435059, 0.1699664145708084, 0.1699664145708084, 0.1699664145708084, 0.1699664145708084, 0.27876532077789307, 0.10171404480934143, 0.08899978548288345, 0.07719369232654572, 0.10443852841854095, 0.0944487527012825, 0.12169358879327774, 0.10262220352888107, 0.1053466871380806, 0.10625484585762024, 0.09717323631048203, 0.11985103040933609, 0.16060037910938263, 0.10067486763000488, 0.09108678251504898, 0.09588082134723663, 0.10786592960357666, 0.09588082134723663, 0.0743076354265213, 0.07191061973571777, 0.08149869740009308, 0.1184120699763298, 0.11492936313152313, 0.12468094378709793, 0.09124694764614105, 0.11005357652902603, 0.08428153395652771, 0.10030198842287064, 0.0884607806801796, 0.09472965449094772, 0.07313686609268188, 0.12936988472938538, 0.08624659478664398, 0.09892991185188293, 0.08878325670957565, 0.06341661512851715, 0.09131991863250732, 0.08624659478664398, 0.11668656766414642, 0.136979877948761, 0.10400324314832687, 0.12695060670375824, 0.12695060670375824, 0.2539012134075165, 0.12695060670375824, 0.12695060670375824, 0.12695060670375824, 0.12695060670375824, 0.10229236632585526, 0.11858672648668289, 0.06970364600419998, 0.09323994815349579, 0.11134479194879532, 0.11768148094415665, 0.08418752253055573, 0.09505043178796768, 0.08418752253055573, 0.12401817739009857, 0.34866392612457275, 0.17433196306228638, 0.10279040783643723, 0.08600830286741257, 0.09230159223079681, 0.09859488159418106, 0.10069264471530914, 0.14684343338012695, 0.09230159223079681, 0.08600830286741257, 0.10279040783643723, 0.0943993553519249, 0.17562347650527954, 0.11453705281019211, 0.10690124332904816, 0.09162963926792145, 0.06872223317623138, 0.12217285484075546, 0.09162963926792145, 0.07635803520679474, 0.0839938372373581, 0.07635803520679474, 0.11248548328876495, 0.10771337151527405, 0.10907682776451111, 0.12612009048461914, 0.08044416457414627, 0.07908070087432861, 0.09544222801923752, 0.09339703619480133, 0.10634990781545639, 0.08930665254592896, 0.1339307427406311, 0.09392545372247696, 0.09392545372247696, 0.12697330117225647, 0.1426275372505188, 0.06957440823316574, 0.07827121019363403, 0.07305312901735306, 0.1130584180355072, 0.07479248940944672, 0.08240947872400284, 0.08503956347703934, 0.07802599668502808, 0.11572394520044327, 0.07802599668502808, 0.1113404631614685, 0.1139705553650856, 0.09555992484092712, 0.1516685038805008, 0.0894230529665947, 0.09427817165851593, 0.12171734124422073, 0.0977960154414177, 0.0928710326552391, 0.09005676209926605, 0.11679236590862274, 0.10272099077701569, 0.08794605731964111, 0.07809609919786453, 0.1196066364645958, 0.12415812909603119, 0.11494326591491699, 0.0999084934592247, 0.07396138459444046, 0.10936585068702698, 0.1159132570028305, 0.07590135931968689, 0.10936585068702698, 0.0792963057756424, 0.09699854254722595, 0.10104063153266907, 0.10104063153266907, 0.10104063153266907, 0.16840104758739471, 0.09262057393789291, 0.10946068167686462, 0.08420052379369736, 0.10104063153266907, 0.06736041605472565, 0.0757804736495018, 0.13175806403160095, 0.09375093132257462, 0.09628473967313766, 0.09628473967313766, 0.09375093132257462, 0.1444271057844162, 0.06334522366523743, 0.09628473967313766, 0.11655521392822266, 0.06841284036636353, 0.11725398153066635, 0.07215629518032074, 0.19842980802059174, 0.09019536525011063, 0.08117583394050598, 0.09019536525011063, 0.09921490401029587, 0.08117583394050598, 0.10823444277048111, 0.07215629518032074, 0.1158154308795929, 0.10565618425607681, 0.09874789416790009, 0.10565618425607681, 0.0820867270231247, 0.09021412581205368, 0.09996700286865234, 0.12719379365444183, 0.11012624949216843, 0.06420645117759705, 0.06761091947555542, 0.06761091947555542, 0.06761091947555542, 0.06761091947555542, 0.06761091947555542, 0.20283275842666626, 0.06761091947555542, 0.06761091947555542, 0.13522183895111084, 0.13522183895111084, 0.12914089858531952, 0.145283505320549, 0.09483785182237625, 0.0726417526602745, 0.08676654100418091, 0.07869523763656616, 0.10694480687379837, 0.10492698103189468, 0.09282001852989197, 0.08676654100418091, 0.09884927421808243, 0.1310327649116516, 0.09540104866027832, 0.1011480987071991, 0.08045871555805206, 0.10344691574573517, 0.09884927421808243, 0.09884927421808243, 0.09540104866027832, 0.09540104866027832, 0.13240790367126465, 0.1127006784081459, 0.10777387768030167, 0.06835942715406418, 0.08929835259914398, 0.09668856114149094, 0.060353368520736694, 0.1274811029434204, 0.06959112733602524, 0.13671885430812836, 0.11675570905208588, 0.08845129609107971, 0.10850025713443756, 0.11439701169729233, 0.09081000089645386, 0.10142415016889572, 0.0931686982512474, 0.12147311121225357, 0.0931686982512474, 0.07076103985309601, 0.323265016078949, 0.10560231655836105, 0.1038898453116417, 0.11873125284910202, 0.09418585151433945, 0.10103572905063629, 0.09190256148576736, 0.06849880516529083, 0.09703996777534485, 0.13928090035915375, 0.08105691522359848, 0.06699302047491074, 0.13670197129249573, 0.09415235370397568, 0.09867890924215317, 0.110447958111763, 0.09234173595905304, 0.0950576663017273, 0.11950106918811798, 0.07061426341533661, 0.11406920105218887, 0.0932135283946991, 0.10637307912111282, 0.11624275147914886, 0.09102026373147964, 0.0767640769481659, 0.11953263729810715, 0.09979330748319626, 0.12062926590442657, 0.08553712069988251, 0.08882700651884079, 0.13866424560546875, 0.13866424560546875, 0.2773284912109375, 0.13866424560546875, 0.13866424560546875, 0.13866424560546875, 0.13866424560546875, 0.13866424560546875, 0.07479757070541382, 0.09973008930683136, 0.11468960344791412, 0.0847705826163292, 0.07978407293558121, 0.07978407293558121, 0.08975708484649658, 0.1246626153588295, 0.15458165109157562, 0.08975708484649658, 0.1698843389749527, 0.1698843389749527, 0.1698843389749527, 0.1698843389749527, 0.12418048083782196, 0.10507579147815704, 0.12076892703771591, 0.09620575606822968, 0.11735737323760986, 0.09211189299821854, 0.07232489436864853, 0.10371117293834686, 0.07300720363855362, 0.0948411375284195, 0.1428656280040741, 0.08655363321304321, 0.10115303844213486, 0.12096652388572693, 0.12200933694839478, 0.12513777613639832, 0.07091140747070312, 0.07821111381053925, 0.07925392687320709, 0.07508266717195511, 0.03333301469683647, 0.06666602939367294, 0.03333301469683647, 0.03333301469683647, 0.09999904036521912, 0.233331099152565, 0.06666602939367294, 0.19999808073043823, 0.03333301469683647, 0.16666506230831146, 0.08603135496377945, 0.10414321720600128, 0.1720627099275589, 0.09055931866168976, 0.09055931866168976, 0.10867118090391159, 0.08150338381528854, 0.09961525350809097, 0.054335590451955795, 0.10867118090391159, 0.6201273202896118], \"Term\": [\"\\n\", \"\\n\", \"\\n\", \"\\n\", \"\\n\", \"\\n\", \"\\n\", \"\\n\", \"\\n\", \"\\n\", \"\\n\\n\", \"\\n\\n\", \"\\n\\n\", \"\\n\\n\", \"\\n\\n\", \"\\n\\n\", \"\\n\\n\", \"\\n\\n\", \"\\n\\n\", \"\\n\\n\", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \\n\\n\", \" \\n\\n\", \" \\n\\n\", \" \\n\\n\", \" \\n\\n\", \" \\n\\n\", \" \\n\\n\", \" \\n\\n\", \" \\n\\n\", \" \\n\\n\", \"$\", \"$\", \"$\", \"$\", \"$\", \"$\", \"$\", \"$\", \"$\", \"$\", \".worth\", \".worth\", \".worth\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"12:45pm\", \"12:45pm\", \"15\", \"15\", \"15\", \"15\", \"15\", \"15\", \"15\", \"15\", \"15\", \"15\", \"2\", \"2\", \"2\", \"2\", \"2\", \"2\", \"2\", \"2\", \"2\", \"2\", \"25th\", \"25th\", \"25th\", \"2ea\", \"33\", \"4\", \"4\", \"4\", \"4\", \"4\", \"4\", \"4\", \"4\", \"4\", \"4\", \"700\", \"700\", \"700\", \"700\", \"700\", \"700\", \"700\", \"700\", \"700\", \"700\", \"8/12/12\", \"8/12/12\", \"AKRON\", \"ANgelica\", \"ANgelica\", \"Aaron\", \"Aaron\", \"Amanda\", \"Amanda\", \"Amanda\", \"Amanda\", \"Amanda\", \"Amanda\", \"Amanda\", \"Amanda\", \"Amanda\", \"Amanda\", \"Aptive\", \"Aptive\", \"Aryon\", \"Aryon\", \"Astor\", \"Avalon\", \"BB\", \"BB\", \"BB\", \"BLODE\", \"Bello\", \"Bello\", \"Bomb\", \"Bomb\", \"Bomb\", \"Bomb\", \"Bomb\", \"Bourgogne\", \"Brookline\", \"Brookline\", \"Brookstone\", \"Buy\", \"Buy\", \"CHOP\", \"CHOP\", \"CHOP\", \"CPAP\", \"CWRU\", \"CWRU\", \"CWRU\", \"Caliente\", \"Caliente\", \"Caliente\", \"Caliente\", \"Caliente\", \"Cary\", \"Cary\", \"Chad\", \"Char\", \"Char\", \"Chihuly\", \"Cleveland\", \"Cleveland\", \"Cleveland\", \"Cleveland\", \"Cleveland\", \"Cleveland\", \"Cleveland\", \"Cleveland\", \"Cleveland\", \"Cleveland\", \"Coe\", \"Colleen\", \"Colleen\", \"Colleen\", \"Cox\", \"Cox\", \"Cox\", \"Cox\", \"Cox\", \"Cox\", \"Cox\", \"Cox\", \"Cox\", \"Cox\", \"Daaayyuuummm\", \"Daaayyuuummm\", \"Daniel\", \"Daniel\", \"Daniel\", \"Daniel\", \"Daniel\", \"Daniel\", \"Daniel\", \"Daniel\", \"Daniel\", \"Darlene\", \"Darlene\", \"Delivary\", \"Delivary\", \"Delta\", \"Dental\", \"Dental\", \"Dental\", \"Dental\", \"Dental\", \"Dental\", \"Dental\", \"Dental\", \"Dental\", \"Devin\", \"Devin\", \"Devin\", \"Environmental\", \"Environmental\", \"Erika\", \"Estate\", \"Flavyour\", \"Funds\", \"Greater\", \"Greater\", \"Greenway\", \"Greenway\", \"Greenway\", \"Greulich\", \"Guys\", \"Guys\", \"Guys\", \"Guys\", \"Guys\", \"Guys\", \"Guys\", \"Guys\", \"Guys\", \"Guys\", \"HOUSE\", \"Harlow\", \"Harlow\", \"Harlow\", \"Hikari\", \"Hikari\", \"Hikari\", \"Hikari\", \"Hikari\", \"Hikari\", \"Hikari\", \"Hikari\", \"Hikari\", \"Hikari\", \"Hq\", \"Hunan\", \"Ian\", \"Ian\", \"Ian\", \"Ian\", \"Ian\", \"Ian\", \"Ian\", \"Ian\", \"Ian\", \"Jina\", \"Jina\", \"Jina\", \"Juvaderm\", \"Katelyn\", \"Katelyn\", \"Katelyn\", \"Kelly\", \"Kelly\", \"Kelly\", \"Kelly\", \"Kelly\", \"Kelly\", \"Kelly\", \"Kelly\", \"Kelly\", \"Kelly\", \"Keri\", \"Keri\", \"Keri\", \"Keri\", \"Keri\", \"Keri\", \"Keri\", \"Kitsch\", \"Krung\", \"Krung\", \"LMT\", \"LMT\", \"LMT\", \"LMT\", \"LMT\", \"Laguna\", \"Lincoln\", \"Lomax\", \"Luci\", \"Luci\", \"Matthew\", \"Matthew\", \"McKnight\", \"Melinda\", \"Moving\", \"Moving\", \"Moving\", \"Moving\", \"Nervous\", \"Orlando\", \"Pizio\", \"Ppl\", \"Ppl\", \"Purolator\", \"Quiche\", \"Quiche\", \"Quiche\", \"Quiznos\", \"Quiznos\", \"Ranchero\", \"Ranchero\", \"Rockets\", \"Rockets\", \"Rockets\", \"Rockets\", \"Rockets\", \"Rockets\", \"Rockets\", \"Rockets\", \"Rockets\", \"Roomtastic\", \"Roomtastic\", \"SUPER\", \"Saturday\", \"Saturday\", \"Saturday\", \"Saturday\", \"Saturday\", \"Saturday\", \"Saturday\", \"Saturday\", \"Saturday\", \"Saturday\", \"Seed\", \"Serendipity\", \"Serendipity\", \"Serendipity\", \"Serendipity\", \"Serendipity\", \"Sheldon\", \"Shelepak\", \"Siam\", \"Siam\", \"Siam\", \"Siam\", \"Siam\", \"Siam\", \"Siam\", \"Siam\", \"Singapore\", \"Singapore\", \"Singapore\", \"Singapore\", \"Singapore\", \"Singapore\", \"Skinny\", \"Sono\", \"Sono\", \"Stack\", \"Stack\", \"Stack\", \"Stack\", \"Stack\", \"Stack\", \"Stack\", \"Stack\", \"Stack\", \"Stack\", \"TSO\", \"TSO\", \"Talia\", \"Talia\", \"Tammie\", \"UMAMI\", \"Vegas\", \"Vegas\", \"Vegas\", \"Vegas\", \"Vegas\", \"Vegas\", \"Vegas\", \"Vegas\", \"Vegas\", \"Vegas\", \"WORST\", \"Wachtel\", \"Wacs\", \"Wor\", \"Wor\", \"YOUNGSTOWN\", \"Yasmin\", \"Yasmin\", \"Yasmin\", \"Yasmin\", \"Yeahhhhhhh\", \"Yo\", \"Yo\", \"Yo\", \"Yo\", \"Yo\", \"Yo\", \"absolute\", \"absolute\", \"absolute\", \"absolute\", \"absolute\", \"absolute\", \"absolute\", \"absolute\", \"absolute\", \"absolute\", \"accounting\", \"accurately\", \"accurately\", \"acrobatic\", \"acrobatic\", \"acrobatic\", \"acrobatic\", \"acrobatic\", \"acrobatic\", \"actually\", \"actually\", \"actually\", \"actually\", \"actually\", \"actually\", \"actually\", \"actually\", \"actually\", \"actually\", \"amazing\", \"amazing\", \"amazing\", \"amazing\", \"amazing\", \"amazing\", \"amazing\", \"amazing\", \"amazing\", \"amazing\", \"area\", \"area\", \"area\", \"area\", \"area\", \"area\", \"area\", \"area\", \"area\", \"area\", \"arrowhead\", \"ask\", \"ask\", \"ask\", \"ask\", \"ask\", \"ask\", \"ask\", \"ask\", \"ask\", \"ask\", \"auto\", \"auto\", \"auto\", \"auto\", \"auto\", \"auto\", \"auto\", \"auto\", \"auto\", \"auto\", \"awesome\", \"awesome\", \"awesome\", \"awesome\", \"awesome\", \"awesome\", \"awesome\", \"awesome\", \"awesome\", \"awesome\", \"axle\", \"axle\", \"bad\", \"bad\", \"bad\", \"bad\", \"bad\", \"bad\", \"bad\", \"bad\", \"bad\", \"bad\", \"bar\", \"bar\", \"bar\", \"bar\", \"bar\", \"bar\", \"bar\", \"bar\", \"bar\", \"bar\", \"basa\", \"basa\", \"basa\", \"basa\", \"basa\", \"bathing\", \"bathing\", \"beause\", \"better\", \"better\", \"better\", \"better\", \"better\", \"better\", \"better\", \"better\", \"better\", \"better\", \"big\", \"big\", \"big\", \"big\", \"big\", \"big\", \"big\", \"big\", \"big\", \"big\", \"bingsu\", \"bingsu\", \"bingsu\", \"bingsu\", \"bingsu\", \"bingsu\", \"bingsu\", \"bingsu\", \"bi\\u00e8re\", \"bi\\u00e8re\", \"bi\\u00e8re\", \"bi\\u00e8re\", \"bi\\u00e8re\", \"bi\\u00e8re\", \"bi\\u00e8re\", \"bi\\u00e8re\", \"bi\\u00e8re\", \"bi\\u00e8re\", \"boba\", \"boba\", \"boba\", \"boba\", \"boba\", \"boba\", \"boba\", \"boba\", \"boba\", \"boba\", \"bod\", \"booster\", \"booster\", \"booster\", \"botanical\", \"botanical\", \"breakfast\", \"breakfast\", \"breakfast\", \"breakfast\", \"breakfast\", \"breakfast\", \"breakfast\", \"breakfast\", \"breakfast\", \"breakfast\", \"butterfly\", \"butterfly\", \"buy\", \"buy\", \"buy\", \"buy\", \"buy\", \"buy\", \"buy\", \"buy\", \"buy\", \"buy\", \"calabash\", \"call\", \"call\", \"call\", \"call\", \"call\", \"call\", \"call\", \"call\", \"call\", \"call\", \"cantina\", \"care\", \"care\", \"care\", \"care\", \"care\", \"care\", \"care\", \"care\", \"care\", \"care\", \"cartridge\", \"cartridge\", \"cartridge\", \"cartridge\", \"cartridge\", \"change\", \"change\", \"change\", \"change\", \"change\", \"change\", \"change\", \"change\", \"change\", \"change\", \"check\", \"check\", \"check\", \"check\", \"check\", \"check\", \"check\", \"check\", \"check\", \"check\", \"chicken\", \"chicken\", \"chicken\", \"chicken\", \"chicken\", \"chicken\", \"chicken\", \"chicken\", \"chicken\", \"chicken\", \"chip\", \"chip\", \"chip\", \"chip\", \"chip\", \"chip\", \"chip\", \"chip\", \"chip\", \"chip\", \"choose\", \"choose\", \"choose\", \"choose\", \"choose\", \"choose\", \"choose\", \"choose\", \"choose\", \"choose\", \"chrohn\", \"clean\", \"clean\", \"clean\", \"clean\", \"clean\", \"clean\", \"clean\", \"clean\", \"clean\", \"clean\", \"clinic\", \"clinic\", \"clinic\", \"clinic\", \"clinic\", \"clinic\", \"clinic\", \"clinic\", \"clinic\", \"clinic\", \"close\", \"close\", \"close\", \"close\", \"close\", \"close\", \"close\", \"close\", \"close\", \"close\", \"cocky\", \"cocky\", \"cocky\", \"cocky\", \"cocky\", \"cocky\", \"cocky\", \"cocky\", \"cocky\", \"coffe\", \"coffee\", \"coffee\", \"coffee\", \"coffee\", \"coffee\", \"coffee\", \"coffee\", \"coffee\", \"coffee\", \"coffee\", \"come\", \"come\", \"come\", \"come\", \"come\", \"come\", \"come\", \"come\", \"come\", \"come\", \"company\", \"company\", \"company\", \"company\", \"company\", \"company\", \"company\", \"company\", \"company\", \"company\", \"compare\", \"compare\", \"compare\", \"compare\", \"compare\", \"compare\", \"compare\", \"compare\", \"compare\", \"compare\", \"cook\", \"cook\", \"cook\", \"cook\", \"cook\", \"cook\", \"cook\", \"cook\", \"cook\", \"cook\", \"cookie\", \"cookie\", \"cookie\", \"cookie\", \"cookie\", \"cookie\", \"cookie\", \"cookie\", \"cookie\", \"cookie\", \"cool\", \"cool\", \"cool\", \"cool\", \"cool\", \"cool\", \"cool\", \"cool\", \"cool\", \"cool\", \"couture\", \"couture\", \"cramp\", \"cramp\", \"cramp\", \"cramp\", \"cramp\", \"cramp\", \"cramp\", \"cramp\", \"cramp\", \"cramp\", \"cranny\", \"cranny\", \"creme\", \"creme\", \"creme\", \"creme\", \"creme\", \"creme\", \"creme\", \"creme\", \"creme\", \"creme\", \"customer\", \"customer\", \"customer\", \"customer\", \"customer\", \"customer\", \"customer\", \"customer\", \"customer\", \"customer\", \"date\", \"date\", \"date\", \"date\", \"date\", \"date\", \"date\", \"date\", \"date\", \"date\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"day\", \"deal\", \"deal\", \"deal\", \"deal\", \"deal\", \"deal\", \"deal\", \"deal\", \"deal\", \"deal\", \"decent\", \"decent\", \"decent\", \"decent\", \"decent\", \"decent\", \"decent\", \"decent\", \"decent\", \"decent\", \"definitely\", \"definitely\", \"definitely\", \"definitely\", \"definitely\", \"definitely\", \"definitely\", \"definitely\", \"definitely\", \"definitely\", \"delicious\", \"delicious\", \"delicious\", \"delicious\", \"delicious\", \"delicious\", \"delicious\", \"delicious\", \"delicious\", \"delicious\", \"demonic\", \"demonic\", \"dessert\", \"dessert\", \"dessert\", \"dessert\", \"dessert\", \"dessert\", \"dessert\", \"dessert\", \"dessert\", \"dessert\", \"dinner\", \"dinner\", \"dinner\", \"dinner\", \"dinner\", \"dinner\", \"dinner\", \"dinner\", \"dinner\", \"dinner\", \"discontinue\", \"discontinue\", \"discontinue\", \"dish\", \"dish\", \"dish\", \"dish\", \"dish\", \"dish\", \"dish\", \"dish\", \"dish\", \"dish\", \"dolci\", \"dolci\", \"drink\", \"drink\", \"drink\", \"drink\", \"drink\", \"drink\", \"drink\", \"drink\", \"drink\", \"drink\", \"eat\", \"eat\", \"eat\", \"eat\", \"eat\", \"eat\", \"eat\", \"eat\", \"eat\", \"eat\", \"enclosure\", \"enclosure\", \"end\", \"end\", \"end\", \"end\", \"end\", \"end\", \"end\", \"end\", \"end\", \"end\", \"enjoy\", \"enjoy\", \"enjoy\", \"enjoy\", \"enjoy\", \"enjoy\", \"enjoy\", \"enjoy\", \"enjoy\", \"enjoy\", \"especially\", \"especially\", \"especially\", \"especially\", \"especially\", \"especially\", \"especially\", \"especially\", \"especially\", \"especially\", \"everrrrr\", \"excellent\", \"excellent\", \"excellent\", \"excellent\", \"excellent\", \"excellent\", \"excellent\", \"excellent\", \"excellent\", \"excellent\", \"experience\", \"experience\", \"experience\", \"experience\", \"experience\", \"experience\", \"experience\", \"experience\", \"experience\", \"experience\", \"family\", \"family\", \"family\", \"family\", \"family\", \"family\", \"family\", \"family\", \"family\", \"family\", \"fantastic\", \"fantastic\", \"fantastic\", \"fantastic\", \"fantastic\", \"fantastic\", \"fantastic\", \"fantastic\", \"fantastic\", \"fantastic\", \"farm\", \"farm\", \"farm\", \"farm\", \"farm\", \"farm\", \"farm\", \"farm\", \"farm\", \"farm\", \"feel\", \"feel\", \"feel\", \"feel\", \"feel\", \"feel\", \"feel\", \"feel\", \"feel\", \"feel\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"fizz\", \"flavor\", \"flavor\", \"flavor\", \"flavor\", \"flavor\", \"flavor\", \"flavor\", \"flavor\", \"flavor\", \"flavor\", \"food\", \"food\", \"food\", \"food\", \"food\", \"food\", \"food\", \"food\", \"food\", \"food\", \"fresh\", \"fresh\", \"fresh\", \"fresh\", \"fresh\", \"fresh\", \"fresh\", \"fresh\", \"fresh\", \"fresh\", \"fridendly\", \"fridendly\", \"friend\", \"friend\", \"friend\", \"friend\", \"friend\", \"friend\", \"friend\", \"friend\", \"friend\", \"friend\", \"friendly\", \"friendly\", \"friendly\", \"friendly\", \"friendly\", \"friendly\", \"friendly\", \"friendly\", \"friendly\", \"friendly\", \"fry\", \"fry\", \"fry\", \"fry\", \"fry\", \"fry\", \"fry\", \"fry\", \"fry\", \"fry\", \"fungus\", \"fungus\", \"fungus\", \"ganoush\", \"ganoush\", \"ganoush\", \"ganoush\", \"ganoush\", \"ganoush\", \"ganoush\", \"ganoush\", \"ganoush\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"ghetto\", \"ghetto\", \"ghetto\", \"ghetto\", \"ghetto\", \"ghetto\", \"ghetto\", \"give\", \"give\", \"give\", \"give\", \"give\", \"give\", \"give\", \"give\", \"give\", \"give\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"go\", \"golfweek.com\", \"golfweek.com\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"greatness\", \"greatness\", \"greatness\", \"greatness\", \"greatness\", \"greatness\", \"greatness\", \"greatness\", \"greatness\", \"guest\", \"guest\", \"guest\", \"guest\", \"guest\", \"guest\", \"guest\", \"guest\", \"guest\", \"guest\", \"gumbo\", \"gumbo\", \"gumbo\", \"gumbo\", \"gumbo\", \"gumbo\", \"gumbo\", \"gumbo\", \"gumbo\", \"gumbo\", \"hair\", \"hair\", \"hair\", \"hair\", \"hair\", \"hair\", \"hair\", \"hair\", \"hair\", \"hair\", \"halo\", \"halo\", \"halo\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"handedly\", \"handedly\", \"handedly\", \"handedly\", \"handedly\", \"head\", \"head\", \"head\", \"head\", \"head\", \"head\", \"head\", \"head\", \"head\", \"head\", \"hear\", \"hear\", \"hear\", \"hear\", \"hear\", \"hear\", \"hear\", \"hear\", \"hear\", \"hear\", \"home\", \"home\", \"home\", \"home\", \"home\", \"home\", \"home\", \"home\", \"home\", \"home\", \"hour\", \"hour\", \"hour\", \"hour\", \"hour\", \"hour\", \"hour\", \"hour\", \"hour\", \"hour\", \"house\", \"house\", \"house\", \"house\", \"house\", \"house\", \"house\", \"house\", \"house\", \"house\", \"huge\", \"huge\", \"huge\", \"huge\", \"huge\", \"huge\", \"huge\", \"huge\", \"huge\", \"huge\", \"imcomplete\", \"imcomplete\", \"impersonal\", \"impersonal\", \"impersonal\", \"impersonal\", \"impersonal\", \"impersonal\", \"impersonal\", \"insectarium\", \"instinctively\", \"integration\", \"intelligently\", \"jackpot\", \"jellyfish\", \"jellyfish\", \"jellyfish\", \"john\", \"juste\", \"juste\", \"juste\", \"juste\", \"juste\", \"kid\", \"kid\", \"kid\", \"kid\", \"kid\", \"kid\", \"kid\", \"kid\", \"kid\", \"kid\", \"kind\", \"kind\", \"kind\", \"kind\", \"kind\", \"kind\", \"kind\", \"kind\", \"kind\", \"kind\", \"know\", \"know\", \"know\", \"know\", \"know\", \"know\", \"know\", \"know\", \"know\", \"know\", \"lack\", \"lack\", \"lack\", \"lack\", \"lack\", \"lack\", \"lack\", \"lack\", \"lack\", \"lack\", \"lady\", \"lady\", \"lady\", \"lady\", \"lady\", \"lady\", \"lady\", \"lady\", \"lady\", \"lady\", \"later\", \"later\", \"later\", \"later\", \"later\", \"later\", \"later\", \"later\", \"later\", \"later\", \"liberte\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"little\", \"little\", \"little\", \"little\", \"little\", \"little\", \"little\", \"little\", \"little\", \"little\", \"livermush\", \"livermush\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \"location\", \"location\", \"location\", \"location\", \"location\", \"location\", \"location\", \"location\", \"location\", \"location\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"look\", \"look\", \"look\", \"look\", \"look\", \"look\", \"look\", \"look\", \"look\", \"look\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"love\", \"love\", \"love\", \"love\", \"love\", \"love\", \"love\", \"love\", \"love\", \"love\", \"luddite\", \"luxuriously\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"marinate\", \"marinate\", \"marinate\", \"marinate\", \"marinate\", \"marinate\", \"marinate\", \"marinate\", \"marinate\", \"marinate\", \"martial\", \"martial\", \"martial\", \"martial\", \"martial\", \"martial\", \"massage\", \"massage\", \"massage\", \"massage\", \"massage\", \"massage\", \"massage\", \"massage\", \"massage\", \"massage\", \"meal\", \"meal\", \"meal\", \"meal\", \"meal\", \"meal\", \"meal\", \"meal\", \"meal\", \"meal\", \"meat\", \"meat\", \"meat\", \"meat\", \"meat\", \"meat\", \"meat\", \"meat\", \"meat\", \"meat\", \"megaphone\", \"menu\", \"menu\", \"menu\", \"menu\", \"menu\", \"menu\", \"menu\", \"menu\", \"menu\", \"menu\", \"menudo\", \"menudo\", \"menudo\", \"menudo\", \"minute\", \"minute\", \"minute\", \"minute\", \"minute\", \"minute\", \"minute\", \"minute\", \"minute\", \"minute\", \"mistreat\", \"mistreat\", \"mistreat\", \"mistreat\", \"mistreat\", \"mmmmmm\", \"momma\", \"momma\", \"mouse\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"nan\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"nice\", \"nice\", \"nice\", \"nice\", \"nice\", \"nice\", \"nice\", \"nice\", \"nice\", \"nice\", \"night\", \"night\", \"night\", \"night\", \"night\", \"night\", \"night\", \"night\", \"night\", \"night\", \"nook\", \"nook\", \"nook\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"obsessed\", \"obsessed\", \"obsessed\", \"obsessed\", \"oho\", \"ok\", \"ok\", \"ok\", \"ok\", \"ok\", \"ok\", \"ok\", \"ok\", \"ok\", \"ok\", \"old\", \"old\", \"old\", \"old\", \"old\", \"old\", \"old\", \"old\", \"old\", \"old\", \"operative\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"outsource\", \"outsource\", \"outsource\", \"outsource\", \"owner\", \"owner\", \"owner\", \"owner\", \"owner\", \"owner\", \"owner\", \"owner\", \"owner\", \"owner\", \"paced\", \"paced\", \"paced\", \"packin\", \"palma\", \"palma\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"people\", \"perfect\", \"perfect\", \"perfect\", \"perfect\", \"perfect\", \"perfect\", \"perfect\", \"perfect\", \"perfect\", \"perfect\", \"pesky\", \"pesky\", \"piece\", \"piece\", \"piece\", \"piece\", \"piece\", \"piece\", \"piece\", \"piece\", \"piece\", \"piece\", \"pj\", \"placard\", \"place\", \"place\", \"place\", \"place\", \"place\", \"place\", \"place\", \"place\", \"place\", \"place\", \"place----a\", \"place----a\", \"plus\", \"plus\", \"plus\", \"plus\", \"plus\", \"plus\", \"plus\", \"plus\", \"plus\", \"plus\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"poisoning\", \"poisoning\", \"poisoning\", \"poisoning\", \"poisoning\", \"poisoning\", \"poisoning\", \"poisoning\", \"poisoning\", \"poisoning\", \"pok\\u00e9\", \"polite\", \"polite\", \"polite\", \"polite\", \"polite\", \"polite\", \"polite\", \"polite\", \"polite\", \"polite\", \"pom\", \"pom\", \"pom\", \"posoleshawwty\", \"posoleshawwty\", \"potato\", \"potato\", \"potato\", \"potato\", \"potato\", \"potato\", \"potato\", \"potato\", \"potato\", \"potato\", \"pretty\", \"pretty\", \"pretty\", \"pretty\", \"pretty\", \"pretty\", \"pretty\", \"pretty\", \"pretty\", \"pretty\", \"price\", \"price\", \"price\", \"price\", \"price\", \"price\", \"price\", \"price\", \"price\", \"price\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"professional\", \"professional\", \"professional\", \"professional\", \"professional\", \"professional\", \"professional\", \"professional\", \"professional\", \"professional\", \"quality\", \"quality\", \"quality\", \"quality\", \"quality\", \"quality\", \"quality\", \"quality\", \"quality\", \"quality\", \"quick\", \"quick\", \"quick\", \"quick\", \"quick\", \"quick\", \"quick\", \"quick\", \"quick\", \"quick\", \"reason\", \"reason\", \"reason\", \"reason\", \"reason\", \"reason\", \"reason\", \"reason\", \"reason\", \"reason\", \"recently\", \"recently\", \"recently\", \"recently\", \"recently\", \"recently\", \"recently\", \"recently\", \"recently\", \"recently\", \"recommend\", \"recommend\", \"recommend\", \"recommend\", \"recommend\", \"recommend\", \"recommend\", \"recommend\", \"recommend\", \"recommend\", \"rename\", \"rename\", \"rename\", \"rename\", \"respectueux\", \"restauran\", \"restaurant\", \"restaurant\", \"restaurant\", \"restaurant\", \"restaurant\", \"restaurant\", \"restaurant\", \"restaurant\", \"restaurant\", \"restaurant\", \"resume\", \"resume\", \"resume\", \"resume\", \"resume\", \"resume\", \"resume\", \"resume\", \"resume\", \"resume\", \"review\", \"review\", \"review\", \"review\", \"review\", \"review\", \"review\", \"review\", \"review\", \"review\", \"ridgeville\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\", \"rivalisent\", \"roll\", \"roll\", \"roll\", \"roll\", \"roll\", \"roll\", \"roll\", \"roll\", \"roll\", \"roll\", \"romper\", \"romper\", \"room\", \"room\", \"room\", \"room\", \"room\", \"room\", \"room\", \"room\", \"room\", \"room\", \"rural\", \"rural\", \"rural\", \"salad\", \"salad\", \"salad\", \"salad\", \"salad\", \"salad\", \"salad\", \"salad\", \"salad\", \"salad\", \"salsa\", \"salsa\", \"salsa\", \"salsa\", \"salsa\", \"salsa\", \"salsa\", \"salsa\", \"salsa\", \"salsa\", \"sandwich\", \"sandwich\", \"sandwich\", \"sandwich\", \"sandwich\", \"sandwich\", \"sandwich\", \"sandwich\", \"sandwich\", \"sandwich\", \"sauce\", \"sauce\", \"sauce\", \"sauce\", \"sauce\", \"sauce\", \"sauce\", \"sauce\", \"sauce\", \"sauce\", \"save\", \"save\", \"save\", \"save\", \"save\", \"save\", \"save\", \"save\", \"save\", \"save\", \"savouriness\", \"say\", \"say\", \"say\", \"say\", \"say\", \"say\", \"say\", \"say\", \"say\", \"say\", \"screw\", \"screw\", \"screw\", \"screw\", \"screw\", \"screw\", \"screw\", \"screw\", \"screw\", \"screw\", \"selection\", \"selection\", \"selection\", \"selection\", \"selection\", \"selection\", \"selection\", \"selection\", \"selection\", \"selection\", \"serve\", \"serve\", \"serve\", \"serve\", \"serve\", \"serve\", \"serve\", \"serve\", \"serve\", \"serve\", \"service\", \"service\", \"service\", \"service\", \"service\", \"service\", \"service\", \"service\", \"service\", \"service\", \"service.maybe\", \"session\", \"session\", \"session\", \"session\", \"session\", \"session\", \"session\", \"session\", \"session\", \"session\", \"shimp\", \"shimp\", \"shimp\", \"shipper\", \"shoooz\", \"shop\", \"shop\", \"shop\", \"shop\", \"shop\", \"shop\", \"shop\", \"shop\", \"shop\", \"shop\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"sill\", \"sit\", \"sit\", \"sit\", \"sit\", \"sit\", \"sit\", \"sit\", \"sit\", \"sit\", \"sit\", \"slimey\", \"slimey\", \"squishie\", \"squishie\", \"squishie\", \"squishie\", \"stab\", \"staff\", \"staff\", \"staff\", \"staff\", \"staff\", \"staff\", \"staff\", \"staff\", \"staff\", \"staff\", \"star\", \"star\", \"star\", \"star\", \"star\", \"star\", \"star\", \"star\", \"star\", \"star\", \"start\", \"start\", \"start\", \"start\", \"start\", \"start\", \"start\", \"start\", \"start\", \"start\", \"stationnement\", \"stationnement\", \"stop\", \"stop\", \"stop\", \"stop\", \"stop\", \"stop\", \"stop\", \"stop\", \"stop\", \"stop\", \"store\", \"store\", \"store\", \"store\", \"store\", \"store\", \"store\", \"store\", \"store\", \"store\", \"stylist\", \"stylist\", \"stylist\", \"stylist\", \"stylist\", \"stylist\", \"stylist\", \"stylist\", \"stylist\", \"stylist\", \"substandard\", \"suitemate\", \"suitemate\", \"super\", \"super\", \"super\", \"super\", \"super\", \"super\", \"super\", \"super\", \"super\", \"super\", \"sure\", \"sure\", \"sure\", \"sure\", \"sure\", \"sure\", \"sure\", \"sure\", \"sure\", \"sure\", \"sushi\", \"sushi\", \"sushi\", \"sushi\", \"sushi\", \"sushi\", \"sushi\", \"sushi\", \"sushi\", \"sushi\", \"swedish\", \"swedish\", \"swedish\", \"swedish\", \"swimsuit\", \"table\", \"table\", \"table\", \"table\", \"table\", \"table\", \"table\", \"table\", \"table\", \"table\", \"taco\", \"taco\", \"taco\", \"taco\", \"taco\", \"taco\", \"taco\", \"taco\", \"taco\", \"taco\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"talk\", \"talk\", \"talk\", \"talk\", \"talk\", \"talk\", \"talk\", \"talk\", \"talk\", \"talk\", \"tar\", \"tar\", \"tar\", \"tar\", \"tar\", \"tar\", \"tar\", \"taste\", \"taste\", \"taste\", \"taste\", \"taste\", \"taste\", \"taste\", \"taste\", \"taste\", \"taste\", \"tastey\", \"tastey\", \"tasty\", \"tasty\", \"tasty\", \"tasty\", \"tasty\", \"tasty\", \"tasty\", \"tasty\", \"tasty\", \"tasty\", \"team\", \"team\", \"team\", \"team\", \"team\", \"team\", \"team\", \"team\", \"team\", \"team\", \"tell\", \"tell\", \"tell\", \"tell\", \"tell\", \"tell\", \"tell\", \"tell\", \"tell\", \"tell\", \"thank\", \"thank\", \"thank\", \"thank\", \"thank\", \"thank\", \"thank\", \"thank\", \"thank\", \"thank\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"think\", \"think\", \"think\", \"think\", \"think\", \"think\", \"think\", \"think\", \"think\", \"think\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"tire\", \"tire\", \"tire\", \"tire\", \"tire\", \"tire\", \"tire\", \"tire\", \"tire\", \"tire\", \"today\", \"today\", \"today\", \"today\", \"today\", \"today\", \"today\", \"today\", \"today\", \"today\", \"trust\", \"trust\", \"trust\", \"trust\", \"trust\", \"trust\", \"trust\", \"trust\", \"trust\", \"trust\", \"try\", \"try\", \"try\", \"try\", \"try\", \"try\", \"try\", \"try\", \"try\", \"try\", \"unorganized\", \"unorganized\", \"unorganized\", \"unorganized\", \"unorganized\", \"unorganized\", \"unorganized\", \"unorganized\", \"unorganized\", \"unorganized\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"visit\", \"visit\", \"visit\", \"visit\", \"visit\", \"visit\", \"visit\", \"visit\", \"visit\", \"visit\", \"wait\", \"wait\", \"wait\", \"wait\", \"wait\", \"wait\", \"wait\", \"wait\", \"wait\", \"wait\", \"walk\", \"walk\", \"walk\", \"walk\", \"walk\", \"walk\", \"walk\", \"walk\", \"walk\", \"walk\", \"walkthrough\", \"want\", \"want\", \"want\", \"want\", \"want\", \"want\", \"want\", \"want\", \"want\", \"want\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"wheelchair\", \"wheelchair\", \"wheelchair\", \"wheelchair\", \"wheelchair\", \"wheelchair\", \"wheelchair\", \"wheelchair\", \"wing\", \"wing\", \"wing\", \"wing\", \"wing\", \"wing\", \"wing\", \"wing\", \"wing\", \"wing\", \"woot\", \"woot\", \"woot\", \"woot\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"yuk\", \"yuk\", \"yuk\", \"yuk\", \"yuk\", \"yuk\", \"yuk\", \"yuk\", \"yuk\", \"yuk\", \"yummy\", \"yummy\", \"yummy\", \"yummy\", \"yummy\", \"yummy\", \"yummy\", \"yummy\", \"yummy\", \"yummy\", \"\\u5929\\u6c23\\u5f88\\u71b1\\u5403\\u4e0d\\u4e0b\\u6771\\u897f\\uff0c\\u4eca\\u5929\\u6211\\u9ede\\u4e86\\u4e00\\u500b\\u97d3\\u570b\\u51b7\\u9762\\u6e6f\\u3001\\u9910\\u5f8c\\u9ede\\u4e86\\u751c\\u9ede\\uff0c\\u51b0\\u6c99\\u7cfb\\u5217\\u4e0d\\u6703\\u592a\\u751c\\u81a9\\uff0c\\u89ba\\u5f97\\u5e97\\u5bb6\\u5f88\\u7528\\u5fc3\\u88fd\\u4f5c\\uff0c\\u5305\\u542b\\u64fa\\u76e4\\u7cbe\\u7dfb\\u3001\\u4f50\\u6599\\u885b\\u751f\\uff0c\\u590f\\u65e5\\u60f3\\u958b\\u80c3\\uff0c\\u9019\\u662f\\u4e00\\u500b\\u4e0d\\u932f\\u7684\\u9078\\u64c7\\uff0c\\u670d\\u52d9\\u4eba\\u54e1\\u4e5f\\u5f88\\u656c\\u696d\\uff0c\\u4ee5\\u5f8c\\u6703\\u5e38\\u5e38\\u4f86\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [8, 1, 6, 2, 7, 10, 5, 9, 4, 3]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el1412422472182341843397485484\", ldavis_el1412422472182341843397485484_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el1412422472182341843397485484\", ldavis_el1412422472182341843397485484_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el1412422472182341843397485484\", ldavis_el1412422472182341843397485484_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "7      0.003778  0.000700       1        1  10.733779\n",
       "0      0.003480 -0.000765       2        1  10.495767\n",
       "5     -0.000372  0.004705       3        1  10.406908\n",
       "1      0.002419 -0.001157       4        1  10.275527\n",
       "6      0.002005 -0.002763       5        1   9.915990\n",
       "9     -0.001318  0.002090       6        1   9.796505\n",
       "4     -0.000185 -0.001831       7        1   9.787773\n",
       "8     -0.001760  0.001565       8        1   9.771530\n",
       "3     -0.003202  0.001789       9        1   9.532938\n",
       "2     -0.004844 -0.004335      10        1   9.283280, topic_info=     Category          Freq        Term         Total  loglift  logprob\n",
       "0     Default  13764.000000              13764.000000  30.0000  30.0000\n",
       "146   Default   4687.000000        food   4687.000000  29.0000  29.0000\n",
       "43    Default   3756.000000        come   3756.000000  28.0000  28.0000\n",
       "82    Default   2792.000000         get   2792.000000  27.0000  27.0000\n",
       "55    Default   8150.000000        \\n\\n   8150.000000  26.0000  26.0000\n",
       "374   Default   3291.000000       order   3291.000000  25.0000  25.0000\n",
       "321   Default   3053.000000          \\n   3053.000000  24.0000  24.0000\n",
       "41    Default   1623.000000        wait   1623.000000  23.0000  23.0000\n",
       "24    Default   3634.000000        like   3634.000000  22.0000  22.0000\n",
       "131   Default   5070.000000       place   5070.000000  21.0000  21.0000\n",
       "51    Default   3449.000000     service   3449.000000  20.0000  20.0000\n",
       "1     Default   2165.000000           $   2165.000000  19.0000  19.0000\n",
       "39    Default   4123.000000        time   4123.000000  18.0000  18.0000\n",
       "35    Default   1486.000000         say   1486.000000  17.0000  17.0000\n",
       "356   Default   1540.000000        find   1540.000000  16.0000  16.0000\n",
       "46    Default   6134.000000        good   6134.000000  15.0000  15.0000\n",
       "208   Default   4126.000000       great   4126.000000  14.0000  14.0000\n",
       "106   Default   1615.000000  restaurant   1615.000000  13.0000  13.0000\n",
       "184   Default   1370.000000   recommend   1370.000000  12.0000  12.0000\n",
       "156   Default   1690.000000       price   1690.000000  11.0000  11.0000\n",
       "266   Default   1411.000000  experience   1411.000000  10.0000  10.0000\n",
       "298   Default   1140.000000       thing   1140.000000   9.0000   9.0000\n",
       "457   Default   1630.000000        know   1630.000000   8.0000   8.0000\n",
       "83    Default   2964.000000          go   2964.000000   7.0000   7.0000\n",
       "443   Default    977.000000        give    977.000000   6.0000   6.0000\n",
       "429   Default   1071.000000    customer   1071.000000   5.0000   5.0000\n",
       "1193  Default   1162.000000     chicken   1162.000000   4.0000   4.0000\n",
       "344   Default   1395.000000  definitely   1395.000000   3.0000   3.0000\n",
       "301   Default   1751.000000        want   1751.000000   2.0000   2.0000\n",
       "144   Default   1490.000000         eat   1490.000000   1.0000   1.0000\n",
       "...       ...           ...         ...           ...      ...      ...\n",
       "184   Topic10    170.730637   recommend   1370.924927   0.2938  -5.6460\n",
       "24    Topic10    407.788849        like   3634.631592   0.1894  -4.7753\n",
       "704   Topic10    136.501343       taste   1104.676758   0.2860  -5.8698\n",
       "329   Topic10     91.754219         big    715.567322   0.3230  -6.2670\n",
       "164   Topic10    169.697144       think   1421.325806   0.2516  -5.6521\n",
       "358   Topic10    110.928665       fresh    911.310852   0.2710  -6.0772\n",
       "46    Topic10    593.947144        good   6134.181641   0.0421  -4.3993\n",
       "345   Topic10    137.726379   delicious   1177.227783   0.2313  -5.8608\n",
       "144   Topic10    166.312256         eat   1490.761597   0.1838  -5.6722\n",
       "39    Topic10    399.739716        time   4123.773438   0.0432  -4.7953\n",
       "374   Topic10    328.618439       order   3291.409912   0.0728  -4.9912\n",
       "55    Topic10    713.097107        \\n\\n   8150.744629  -0.0593  -4.2165\n",
       "234   Topic10    125.882874         way   1104.592651   0.2051  -5.9507\n",
       "430   Topic10    158.359726         day   1458.562866   0.1566  -5.7212\n",
       "266   Topic10    149.292053  experience   1411.862793   0.1302  -5.7802\n",
       "131   Topic10    418.168274       place   5070.994141  -0.1185  -4.7502\n",
       "920   Topic10     66.308640    sandwich    540.516052   0.2788  -6.5918\n",
       "43    Topic10    323.128265        come   3756.965820  -0.0764  -5.0080\n",
       "83    Topic10    257.334198          go   2964.871338  -0.0673  -5.2357\n",
       "248   Topic10     78.712173       serve    669.853210   0.2357  -6.4203\n",
       "0     Topic10    805.809570              13764.986328  -0.4611  -4.0942\n",
       "146   Topic10    351.940979        food   4687.367676  -0.2122  -4.9226\n",
       "106   Topic10    154.496567  restaurant   1615.674927   0.0296  -5.7459\n",
       "692   Topic10    181.617752        love   2145.614746  -0.0923  -5.5842\n",
       "319   Topic10    138.766205        work   1465.608765   0.0197  -5.8533\n",
       "27    Topic10    159.915161        look   1909.286133  -0.1029  -5.7115\n",
       "82    Topic10    180.239426         get   2792.788574  -0.3636  -5.5918\n",
       "40    Topic10    158.198380         try   2460.812012  -0.3674  -5.7222\n",
       "301   Topic10    141.910187        want   1751.855469  -0.1363  -5.8309\n",
       "457   Topic10    139.790070        know   1630.348145  -0.0795  -5.8460\n",
       "\n",
       "[830 rows x 6 columns], token_table=       Topic      Freq                                               Term\n",
       "term                                                                     \n",
       "321        1  0.094977                                                 \\n\n",
       "321        2  0.104802                                                 \\n\n",
       "321        3  0.107094                                                 \\n\n",
       "321        4  0.099562                                                 \\n\n",
       "321        5  0.094322                                                 \\n\n",
       "321        6  0.091702                                                 \\n\n",
       "321        7  0.097597                                                 \\n\n",
       "321        8  0.085479                                                 \\n\n",
       "321        9  0.073689                                                 \\n\n",
       "321       10  0.150653                                                 \\n\n",
       "55         1  0.149925                                               \\n\\n\n",
       "55         2  0.103794                                               \\n\\n\n",
       "55         3  0.088949                                               \\n\\n\n",
       "55         4  0.089317                                               \\n\\n\n",
       "55         5  0.093979                                               \\n\\n\n",
       "55         6  0.079870                                               \\n\\n\n",
       "55         7  0.105757                                               \\n\\n\n",
       "55         8  0.089562                                               \\n\\n\n",
       "55         9  0.111278                                               \\n\\n\n",
       "55        10  0.087477                                               \\n\\n\n",
       "0          1  0.128587                                                   \n",
       "0          2  0.134181                                                   \n",
       "0          3  0.092118                                                   \n",
       "0          4  0.124809                                                   \n",
       "0          5  0.104468                                                   \n",
       "0          6  0.103959                                                   \n",
       "0          7  0.099601                                                   \n",
       "0          8  0.081729                                                   \n",
       "0          9  0.071994                                                   \n",
       "0         10  0.058554                                                   \n",
       "...      ...       ...                                                ...\n",
       "118        2  0.086554                                               year\n",
       "118        3  0.101153                                               year\n",
       "118        4  0.120967                                               year\n",
       "118        5  0.122009                                               year\n",
       "118        6  0.125138                                               year\n",
       "118        7  0.070911                                               year\n",
       "118        8  0.078211                                               year\n",
       "118        9  0.079254                                               year\n",
       "118       10  0.075083                                               year\n",
       "13048      1  0.033333                                                yuk\n",
       "13048      2  0.066666                                                yuk\n",
       "13048      3  0.033333                                                yuk\n",
       "13048      4  0.033333                                                yuk\n",
       "13048      5  0.099999                                                yuk\n",
       "13048      6  0.233331                                                yuk\n",
       "13048      7  0.066666                                                yuk\n",
       "13048      8  0.199998                                                yuk\n",
       "13048      9  0.033333                                                yuk\n",
       "13048     10  0.166665                                                yuk\n",
       "2375       1  0.086031                                              yummy\n",
       "2375       2  0.104143                                              yummy\n",
       "2375       3  0.172063                                              yummy\n",
       "2375       4  0.090559                                              yummy\n",
       "2375       5  0.090559                                              yummy\n",
       "2375       6  0.108671                                              yummy\n",
       "2375       7  0.081503                                              yummy\n",
       "2375       8  0.099615                                              yummy\n",
       "2375       9  0.054336                                              yummy\n",
       "2375      10  0.108671                                              yummy\n",
       "22619      8  0.620127  天氣很熱吃不下東西，今天我點了一個韓國冷面湯、餐後點了甜點，冰沙系列不會太甜膩，覺得店家很用...\n",
       "\n",
       "[2510 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[8, 1, 6, 2, 7, 10, 5, 9, 4, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyLDAvis.gensim\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim.prepare(lda, corpus, id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "def compute_coherence_values(dictionary, corpus, df_column, limit, start=2, step=3, passes=5):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    path : path to input texts\n",
    "    limit : Max num of topics\n",
    "    passes: the number of times the entire lda model & coherence values are calculated\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    \n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    coherence_values = []\n",
    "    \n",
    "    tokens = list(df_column)\n",
    "    \n",
    "    for iter_ in range(passes):\n",
    "        for num_topics in range(start, limit, step):\n",
    "            stream = df_column\n",
    "            model = LdaMulticore(corpus=corpus,\n",
    "                                 num_topics=num_topics,\n",
    "                                 id2word=dictionary,\n",
    "                                 random_state=723812,\n",
    "                                 workers=8)\n",
    "            coherencemodel = CoherenceModel(model=model,dictionary=dictionary,corpus=corpus, coherence='u_mass')\n",
    "            coherence_values.append({'pass': iter_, \n",
    "                                     'num_topics': num_topics, \n",
    "                                     'coherence_score': coherencemodel.get_coherence()\n",
    "                                    })\n",
    "\n",
    "    return coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-872d787cc02c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m                                                         \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                                                         \u001b[0mstep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                                                         passes=2)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-18-185fdb5397a2>\u001b[0m in \u001b[0;36mcompute_coherence_values\u001b[1;34m(dictionary, corpus, df_column, limit, start, step, passes)\u001b[0m\n\u001b[0;32m     31\u001b[0m                                  \u001b[0mid2word\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m                                  \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m723812\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m                                  workers=8)\n\u001b[0m\u001b[0;32m     34\u001b[0m             \u001b[0mcoherencemodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCoherenceModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoherence\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'u_mass'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             coherence_values.append({'pass': iter_, \n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\gensim\\models\\ldamulticore.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, corpus, num_topics, id2word, workers, chunksize, passes, batch, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, random_state, minimum_probability, minimum_phi_value, per_word_topics, dtype)\u001b[0m\n\u001b[0;32m    151\u001b[0m             \u001b[0mdecay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_every\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_every\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0miterations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m             \u001b[0mgamma_threshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgamma_threshold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminimum_probability\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mminimum_probability\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m             \u001b[0mminimum_phi_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mminimum_phi_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mper_word_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mper_word_topics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m         )\n\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[0;32m    369\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m             \u001b[0muse_numpy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatcher\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 371\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunks_as_numpy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_numpy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    372\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minit_dir_prior\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\gensim\\models\\ldamulticore.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, corpus, chunks_as_numpy)\u001b[0m\n\u001b[0;32m    266\u001b[0m             \u001b[1;31m# wait for all outstanding jobs to finish\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mqueue_size\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m                 \u001b[0mprocess_result_queue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mforce\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mreallen\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlencorpus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\site-packages\\gensim\\models\\ldamulticore.py\u001b[0m in \u001b[0;36mprocess_result_queue\u001b[1;34m(force)\u001b[0m\n\u001b[0;32m    228\u001b[0m                 \"\"\"\n\u001b[0;32m    229\u001b[0m                 \u001b[0mmerged_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m                 \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mresult_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m                     \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m                     \u001b[0mqueue_size\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\multiprocessing\\queues.py\u001b[0m in \u001b[0;36mempty\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36mpoll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36m_poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    328\u001b[0m                         _winapi.PeekNamedPipe(self._handle)[0] != 0):\n\u001b[0;32m    329\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_get_more_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mov\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\U4-S1-NLP\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(object_list, timeout)\u001b[0m\n\u001b[0;32m    870\u001b[0m             \u001b[1;31m# request that overlapped reads stop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mov\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mov_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 872\u001b[1;33m                 \u001b[0mov\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcancel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    873\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    874\u001b[0m             \u001b[1;31m# wait for all overlapped reads to stop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "coherence_values = compute_coherence_values(dictionary=id2word, \n",
    "                                                        corpus=corpus, \n",
    "                                                        df_column=df.tokens, \n",
    "                                                        start=2, \n",
    "                                                        limit=40, \n",
    "                                                        step=6,\n",
    "                                                        passes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'coherence_values' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-fe714395de06>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtopic_coherence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoherence_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'coherence_values' is not defined"
     ]
    }
   ],
   "source": [
    "topic_coherence = pd.DataFrame.from_records(coherence_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'topic_coherence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-c247aba0bbea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlineplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"num_topics\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"coherence_score\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtopic_coherence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'topic_coherence' is not defined"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "ax = sns.lineplot(x=\"num_topics\", y=\"coherence_score\", data=topic_coherence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stretch Goals\n",
    "\n",
    "Complete one of more of these to push your score towards a three: \n",
    "* Incorporate named entity recognition into your analysis\n",
    "* Compare vectorization methods in the classification section\n",
    "* Analyze more (or all) of the yelp dataset - this one is v. hard. \n",
    "* Use a generator object on the reviews file - this would help you with the analyzing the whole dataset.\n",
    "* Incorporate any of the other yelp dataset entities in your analysis (business, users, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "u4-s1-nlp"
  },
  "kernelspec": {
   "display_name": "U4-S1-NLP (Python3)",
   "language": "python",
   "name": "u4-s1-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "nteract": {
   "version": "0.14.5"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
